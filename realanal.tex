\documentclass[12pt]{book}
\usepackage{realanal}

\hypersetup{
    %colorlinks,
    pdfborderstyle={/S/U/W 0.5},
    %citecolor=black,
    %filecolor=black,
    %linkcolor=black,
    %urlcolor=black,
    pdfkeywords={real analysis, Riemann integral, derivative, limit, sequence},
    pdfsubject={Real Analysis},
    pdftitle={Basic Analysis: Introduction to Real Analysis},
    pdfauthor={Jiri Lebl}
}

% Set up our index
\makeindex


\author{Ji\v{r}\'i Lebl}

\title{Basic Analysis: Introduction to Real Analysis}


\begin{document}

\ifpdf
  \pdfbookmark{Title Page}{title}
\fi
\newlength{\centeroffset}
\setlength{\centeroffset}{-0.5\oddsidemargin}
\addtolength{\centeroffset}{0.5\evensidemargin}
%\addtolength{\textwidth}{-\centeroffset}
\thispagestyle{empty}
\vspace*{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\Huge\bfseries \sffamily Basic Analysis }
\noindent\rule[-1ex]{\textwidth}{5pt}\\[2.5ex]
\hfill\emph{\Large \sffamily Introduction to Real Analysis}
\end{minipage}}

\vspace{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\bfseries 
by Ji{\v r}\'i Lebl\\[3ex]} 
\today
\\
(version 4.1 with modifications by Anton Petrunin)
% --- 4th edition, 0th update)
\end{minipage}}

%\addtolength{\textwidth}{\centeroffset}
\vspace{\stretch{2}}


\pagebreak

\vspace*{\fill}

%\begin{small} 
\noindent
Typeset in \LaTeX.


\bigskip

\noindent
Copyright \copyright 2009--2016 Ji{\v r}\'i Lebl

%\noindent
%ISBN XXXX

\bigskip

%\begin{floatingfigure}{1.4in}
%\vspace{-0.05in}
\noindent
\includegraphics[width=1.38in]{license}
%\end{floatingfigure}

\bigskip

\noindent
This work is licensed under the Creative Commons
Attribution-Non\-commercial-Share Alike 3.0 United States License. To view a copy of this license, visit
\url{http://creativecommons.org/licenses/by-nc-sa/3.0/us/} or send a letter to Creative Commons, 171 Second Street, Suite 300, San Francisco, California, 94105, USA.
%\end{small}

\bigskip

\noindent
You can use, print, duplicate, share these notes as much as you want.
You can base your own notes on these and reuse parts if you keep the license the
same.
 If you plan to use these commercially (sell them for more than just duplicating cost), then you need to contact me and we will work something out.
If you are printing a course pack for your students, then it is fine if the  duplication service is charging a fee for printing and selling the printed copy.
 I consider that duplicating cost.

\bigskip

\noindent
During the writing of these notes, 
the author was in part supported by NSF grants DMS-0900885 and
DMS-1362337.

\bigskip

\noindent
The date is the main identifier of version.
 The major version / edition number is raised only if there have been substantial changes.
 Edition number started at 4, that is, version 4.0, as it was not kept track of
before.  %The edition given with the ISBN number is the major version.

\bigskip

\noindent
See \url{http://www.jirka.org/ra/} for more information
(including contact information).


% For large print do this
%\large

\microtypesetup{protrusion=false}
\tableofcontents
\microtypesetup{protrusion=true}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{INTRODUCTION}{INTRODUCTION}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{About this book}

This book is a one semester course in basic analysis.
It started its life as my lecture notes for teaching Math 444 at the University of Illinois at Urbana-Champaign (UIUC) in Fall semester 2009.
Later I added the metric space chapter to teach Math 521 at University of Wisconsin--Madison (UW).
A prerequisite for this course is a basic proof course, using  for example~\cite{Hammack}, \cite{GIAM}, or \cite{DW}.

It should be possible to use the book for both a basic course for students who do not necessarily wish to go to graduate school (such as UIUC 444), but also as a more advanced one-semester course that also covers topics such as metric spaces (such as UW 521).
Here are my suggestions for what to cover in a semester course.
 For a slower course such as UIUC 444:
\begin{center}
\S0.3, \S1.1--\S1.4, \S2.1--\S2.5, \S3.1--\S3.4, \S4.1--\S4.2,
\S5.1--\S5.3, \S6.1--\S6.3
\end{center}
%\noindent
For a more rigorous course covering metric spaces that runs quite a bit faster (such as UW 521):
\begin{center}
\S0.3, \S1.1--\S1.4, \S2.1--\S2.5, \S3.1--\S3.4, \S4.1--\S4.2,
\S5.1--\S5.3, \S6.1--\S6.2, \S7.1--\S7.6
\end{center}
%\noindent
It should also be possible to run a faster course without metric spaces covering all sections of chapters 0 through 6.
 The approximate number of
lectures given in the section notes through chapter 6 are a very rough estimate and were designed for the slower course.
The first few chapters of the book can be used in an introductory proofs course as is for example done at Iowa State University Math 201, where  this book is used in conjunction with Hammack's Book of Proof~\cite{Hammack}.

The book normally used for the class at UIUC is Bartle and Sherbert,
\emph{Introduction to Real Analysis} third edition \cite{BS}.
The structure of the beginning of the book somewhat follows the standard syllabus of UIUC Math 444 and therefore has some similarities with \cite{BS}.
A major difference is that we define the Riemann integral using Darboux sums and not tagged partitions.
 The Darboux approach is far more appropriate for a course of this level.

Our approach allows us to fit a course such as UIUC 444 within a semester and still spend some extra time on the interchange of limits and end with Picard's theorem on the existence and uniqueness of solutions of ordinary differential equations.
This theorem is a wonderful example that uses many results proved in the book.
 For more advanced students,
material may be covered faster so that we arrive at metric spaces and prove Picard's theorem using the fixed point theorem as is usual.

Other excellent books exist.
 My favorite is  Rudin's excellent \emph{Principles of Mathematical Analysis}~\cite{Rudin:baby} or as it is commonly and lovingly called \emph{baby Rudin} (to distinguish it from his other great analysis textbook).  
 I took a lot of inspiration and ideas from Rudin.
 However, Rudin is a bit more advanced and ambitious than this present course.
For those that wish to continue mathematics, Rudin is a fine investment.
An inexpensive and somewhat simpler alternative to Rudin is Rosenlicht's \emph{Introduction to Analysis}~\cite{Rosenlicht}.
%Rosenlicht
%may not be as dry as Rudin for those just starting out in mathematics.
There is also the freely downloadable \emph{Introduction to Real Analysis} by William Trench~\cite{Trench}.
% for those that do not wish to
%invest much money.

\medskip

A note about the style of some of the proofs:  Many proofs traditionally done by contradiction, I prefer to do by a direct proof or by contrapositive.
While the book does include proofs by contradiction, 
I only do so when the contrapositive statement seemed too awkward, or when  contradiction follows rather quickly.
In my opinion, contradiction is more likely to get beginning students into trouble, as we are talking about objects that do not exist.
%In a
%direct proof or a contrapositive proof one can be guided by intuition, but in a contradiction
%proof, intuition usually leads us astray.

I try to avoid unnecessary formalism where it is unhelpful.
Furthermore, the proofs and the language get slightly less formal as we progress through the book, as more and more details are left out to avoid clutter.

As a general rule, I use $:=$ instead of $=$ to define an object rather than to simply show equality.
I use this symbol rather more liberally than is usual for emphasis.
I use it even when the context is ``local,''
that is, I may simply define a function $f(x) := x^2$ for a single exercise or example.

%\medskip
%
%It is possible to skip or skim some material in the book as it is not used
%later on.  Some material that can safely be skipped is marked as such
%in the notes that appear below every section title.
%Section
%\sectionref{sec:basicset} can usually be covered lightly, or left as
%reading, depending on the prerequisites of the course.
%The section on Taylor's theorem
%(\sectionref{sec:taylor}) can safely be skipped as it is never used later.
%Other parts are marked in the section notes for each section.
%Uncountability of $\R$ in \sectionref{sec:intandsizeR} can safely be skipped.
%The alternative proof of Bolzano--Weierstrass in \sectionref{sec:bw} can
%safely be skipped.
%And of course, the section on Picard's theorem can also
%be skipped if there is no time at the end of the course, though I have not
%marked the section optional.

%It is possible to fill a semester with a course that ends with Picards
%theorem in \sectionref{sec:picard} by skipping little.  On the other hand,
%if metric spaces are covered, one might have to go a little faster and skip
%some topics in the first six chapters.

%\medskip
%
%Here is an approximate correspondence of the sections to \cite{BS}.  Note
%that the material in this book and in \cite{BS} differs.  The correspondence
%to other books is more complicated.
%
%FIXME: correspondence of new sections
%
%{\small
%
%\begin{center}
%\begin{tabular}{l|l|}
%Section
%& Section in \cite{BS}
%\\
%\hline
%\sectionref{sec:basicset} & \S1.1--\S1.3
%\\
%\sectionref{sec:basicpropsrn} & \S2.1 and \S2.3
%\\
%\sectionref{sec:setofreals} & \S2.3 and \S2.4
%\\
%\sectionref{sec:absval} & \S2.2
%\\
%\sectionref{sec:intandsizeR} & \S2.5
%\\
%\sectionref{sec:seqsandlims} & parts of \S3.1, \S3.2, \S3.3, \S3.4
%\\
%\sectionref{sec:factslimsseqs} & \S3.2
%\\
%\sectionref{sec:bw} & \S3.3 and \S3.4
%\\
%\sectionref{sec:cauchy} & \S3.5
%\\
%\sectionref{sec:series} & \S3.7
%\\
%\sectionref{sec:limoffunc} & \S4.1--\S4.2
%\\
%\sectionref{sec:cont} & \S5.1 and \S5.2
%\end{tabular}
%\begin{tabular}{|l|l}
%Section
%& Section in \cite{BS}
%\\
%\hline
%\sectionref{sec:minmaxint} & \S5.3
%\\
%\sectionref{sec:unifcont} & \S5.4
%\\
%\sectionref{sec:der} & \S6.1
%\\
%\sectionref{sec:mvt} & \S6.2
%\\
%\sectionref{sec:taylor} & \S6.3
%\\
%\sectionref{sec:rint} & \S7.1, \S7.2
%\\
%\sectionref{sec:rintprop} & \S7.2
%\\
%\sectionref{sec:ftc} & \S7.3
%\\
%\sectionref{sec:puconv} & \S8.1
%\\
%\sectionref{sec:liminter} & \S8.2
%\\
%\sectionref{sec:picard} & Not in \cite{BS}
%\\
%\chapterref{ms:chapter} & partially \S11
%\end{tabular}
%\end{center}
%}

%\medskip
%\hrule
\medskip

Finally, I would like to acknowledge 
Jana Ma\v{r}\'ikov\'a,
Glen Pugh, Paul Vojta, 
Frank Beatrous, 
S\"{o}nmez \c{S}ahuto\u{g}lu,
Jim Brandt, 
Kenji Kozai, 
and Arthur Busch,
for teaching with the book and giving me lots of useful feedback.
Frank Beatrous wrote the University of Pittsburgh version extensions, which served as inspiration for many of the recent additions.
I would also like to thank
Dan Stoneham, 
Jeremy Sutter, 
Eliya Gwetta, 
Daniel Pimentel-Alarc\'on,
Steve Hoerning, 
Yi Zhang, 
Nicole Caviris,
Kristopher Lee, 
Baoyue Bi, 
Hannah Lund,
Trevor Mannella, 
Mitchel Meyer, 
Gregory Beauregard,
Chase Meadors, 
Andreas Giannopoulos,
an anonymous reader, and in general all the students in my classes for suggestions and
finding errors and typos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{About analysis} \label{sec:aboutra}

Analysis is the branch of mathematics that deals with inequalities and limits.  
The present course deals with the most basic  concepts in analysis.  
The goal of the course is to acquaint the reader with rigorous proofs in analysis and also to set a firm foundation for calculus of one variable.

Calculus has prepared you, the student, for using mathematics without telling you why what you learned is true.  
To use, or teach, mathematics effectively, you cannot simply know \emph{what} is true, you must know \emph{why} it is true.  
This course shows you \emph{why} calculus is true.  
It is here to give you a good understanding of the concept of a limit, the derivative, and the integral.

Let us use an analogy.
An auto mechanic that has learned to change the oil, fix broken headlights, and charge the battery, will only be able to do those simple tasks.  
He will be unable to work independently to diagnose and fix problems.
%A high school teacher that does not understand the definition of the Riemann integral or the derivative may not be able to properly answer all the students' questions.
%To this day I remember several nonsensical statements I heard from my calculus teacher in high school, who simply did not understand the concept of the limit, though he could ``do'' all problems in calculus.

\medskip

We start with a discussion of the real number system, most importantly its completeness property, which is the basis for all that comes after.
We then discuss the simplest form of a limit, the limit of a sequence.  
Afterwards, we study functions of one variable, continuity, and the derivative.
Next, we define the Riemann integral and prove the fundamental theorem of calculus.  
We discuss sequences of functions and the interchange of limits.  
Finally, we give an introduction to metric spaces.

Let us give the most important difference between analysis and algebra.  
In algebra, we prove equalities directly; we prove that an object, a number perhaps, is equal to another object.  
In analysis, we usually prove inequalities.  

To illustrate the point, consider the following statement.

\medskip

\emph{Let $x$ be a real number.
If $0 \leq x < \epsilon$ is true for all real numbers $\epsilon > 0$, then $x = 0$}.

\medskip

This statement is the general idea of what we do in analysis.
If we wish to show that $x = 0$, we show that $0 \leq x < \epsilon$ for all positive $\epsilon$.

\medskip

The term \emph{real analysis} is a little bit of a misnomer.
I prefer to use simply \emph{analysis}.
The other type of analysis, 
\emph{complex analysis}, really builds up on the present material, rather than being distinct.
Furthermore, a more advanced course on real analysis would talk about complex numbers often.
I suspect the nomenclature is historical baggage.


\medskip

Let us get on with the show\ldots


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Basic set theory} \label{sec:basicset}
\index{set theory}

\sectionnotes{1--3 lectures (some material can be skipped or covered lightly)}

Before we start talking about analysis we need to fix some language.
Modern\footnote{The term ``modern'' refers to late 19th century up to
the present.}
analysis uses the language of sets, and therefore that is where we start.
We talk about sets in a rather informal way, using the so-called
``\myindex{na\"ive set theory}.''  Do not worry, that is what majority of
mathematicians use, and it is hard to get into trouble.

We assume the reader has seen basic set theory
and has had a course in basic proof writing.
This section should be thought of as a refresher.

\subsection*{Sets}

A \emph{\myindex{set}} is a collection of objects called
\emph{elements}\index{element} or \emph{members}\index{member}.
 A set with
no objects is called the \emph{\myindex{empty set}} and is denoted by
$\emptyset$ (or sometimes by $\{ \}$).

\medskip

Think of a set as a club with a certain membership.
 For
example, the students who play chess are members of the chess club.  However,
do not take the analogy too far.
 A set is only defined by the members
that form the set; two sets that have the same members are the same set.

Most of the time we will consider sets
of numbers.
 For example, the set
\begin{equation*}
S := \{ 0, 1, 2 \}
\end{equation*}
is the set containing
the three elements 0, 1, and 2.
We write
\begin{equation*}
1 \in S
\end{equation*}
to denote that the number 1 belongs to the set $S$.
 That is, 1 is a member
of $S$.
 Similarly we write
\begin{equation*}
7 \notin S
\end{equation*}
to denote that the number 7 is not in $S$.
 That is, 7 is not a member of
$S$.
The elements of all sets under consideration come from some set we call the
\emph{\myindex{universe}}.
 For simplicity,
we often consider the universe to be the set that contains only the elements
we are interested in.
The universe is generally understood from context
and is not explicitly mentioned.
 In this course, our universe will
most often be the set of real numbers.

While the elements of a set are often numbers,
other objects, such as other sets, can be elements of a set.
A set may also contain some of the same elements as another set.
 For example,
\begin{equation*}
T := \{ 0, 2 \}
\end{equation*}
contains the numbers 0 and 2.
In this case each element of $T$ also belongs to $S$.  
We write $T \subset S$.  
More formally we make the following definition.

\begin{defn}
{\ }
\begin{enumerate}[(i)]
\item
A set $A$ is a \emph{\myindex{subset}}
of a set $B$ if $x \in A$ implies $x \in B$, and we write $A \subset B$.
That is, all members of $A$ are also members of $B$.
\item
Two sets $A$ and $B$ are \emph{\myindex{equal}} if $A \subset B$ and $B
\subset A$.
 We write $A = B$.
That is, $A$ and $B$ contain exactly the same elements.
If it is not true that $A$ and $B$ are equal, then 
we write $A \not= B$.
\item
A set $A$ is a \emph{\myindex{proper subset}} of $B$ if $A \subset B$
and $A \not= B$.
 We write $A \subsetneq B$.
\end{enumerate}
\end{defn}

For example, for $S$ and $T$ defined above $T \subset S$, but
$T \not= S$.
 So $T$ is a proper subset of $S$.
If $A = B$, then $A$ and $B$ are simply two names for the
same exact set.

Let us mention the
\emph{\myindex{set building notation}},
\begin{equation*}
\{ x \in A : P(x) \} .
\end{equation*}
This notation refers to a subset of the set $A$ containing all elements
of $A$ that satisfy the property $P(x)$. 
%???AN EXAMPLE IS NEEDED RIGHT HERE
The notation is sometimes
abbreviated, $A$ is not mentioned when understood from context.
Furthermore, $x \in A$ is sometimes replaced with a formula to make the notation
easier to read.  

\begin{example}
The following are sets including the standard notations.
\begin{enumerate}[(i)]
\item The set of \emph{\myindex{natural numbers}}, $\N := \{ 1, 2, 3, \ldots
\}$.
\item The set of \emph{\myindex{integers}}, $\Z := \{ 0, -1, 1, -2, 2, \ldots
\}$.
\item The set of \emph{\myindex{rational numbers}}, $\Q := \{ \frac{m}{n} :  m, n \in \Z
\text{ and } n \not= 0 \}$.
\item The set of even natural numbers, $\{  2m : m \in \N \}$.
\item The set of real numbers, $\R$.
\end{enumerate}

Note that $\N \subset \Z \subset \Q \subset \R$.
\end{example}

There are many operations we want to do with sets.

\begin{defn}
{\ }
\begin{enumerate}[(i)]
\item
A \emph{\myindex{union}} of two sets $A$ and $B$ is defined as
\begin{equation*}
A \cup B := \{ x : x \in A \text{ or } x \in B \} .
\end{equation*}
\item
An \emph{\myindex{intersection}} of two sets $A$ and $B$ is defined as
\begin{equation*}
A \cap B := \{ x : x \in A \text{ and } x \in B \} .
\end{equation*}
\item
We say sets $A$ and $B$ are \emph{\myindex{disjoint}} if $A \cap B =
\emptyset$.
\item
A \emph{complement of $B$ relative to $A$\index{complement relative to}}
(or \emph{\myindex{set-theoretic difference}} of $A$ and $B$) is defined as
\begin{equation*}
A \setminus B := \{ x : x \in A \text{ and } x \notin B \} .
\end{equation*}
\item
We say
\emph{\myindex{complement}} of $B$ and write $B^c$ instead of $A \setminus B$ if
the set $A$ is either the entire universe or is the obvious
set containing $B$, and is understood from context.
\end{enumerate}
\end{defn}

The notation $B^c$ may be a little vague at this point.
 If
the set $B$ is a subset of the real numbers $\R$, then $B^c$ means
$\R \setminus B$.
 If $B$ is naturally a subset of the natural numbers,
then $B^c$
is $\N \setminus B$.
 If ambiguity would ever arise, we will 
use the set difference notation $A \setminus B$.

\begin{figure}[h!t]
\begin{center}
\input figsetop.pdf_t
\caption{Venn diagrams of set operations.\label{figsetop}}
\end{center}
\end{figure}
We illustrate the operations on the
\emph{Venn diagrams}\index{Venn diagram} in \figureref{figsetop}.
Let us now establish one of most basic theorems about sets and logic.

\begin{thm}[DeMorgan]\index{DeMorgan's theorem}
Let $A, B, C$ be sets.
 Then
\begin{align*}
{(B \cup C)}^c &= B^c \cap C^c , \\
{(B \cap C)}^c &= B^c \cup C^c ,
\end{align*}
or, more generally,
\begin{align*}
A \setminus (B \cup C) &= (A \setminus B) \cap (A \setminus C) , \\
A \setminus (B \cap C) &= (A \setminus B) \cup (A \setminus C) .
\end{align*}
\end{thm}

\begin{proof}
The first statement is proved by the second statement if we
assume the set $A$ is our ``universe.''

Let us prove $A \setminus (B \cup C) = (A \setminus B) \cap (A \setminus C)$.
Remember the definition of equality of sets.
 First, we must show that
if $x \in A \setminus (B \cup C)$, then
$x \in (A \setminus B) \cap (A \setminus C)$.
 Second, we must also show that
if $x \in (A \setminus B) \cap (A \setminus C)$, then
$x \in A \setminus (B \cup C)$.

So let us assume $x \in A \setminus (B \cup C)$.
 Then $x$ is in 
$A$, but not in $B$ nor $C$.
 Hence $x$ is in $A$ and not in $B$, that is,
$x \in A \setminus B$.
 Similarly $x \in A \setminus C$.
 Thus
$x \in (A \setminus B) \cap (A \setminus C)$.

On the other hand suppose 
$x \in (A \setminus B) \cap (A \setminus C)$.
 In particular
$x \in (A \setminus B)$, so 
$x \in A$ and $x \notin B$.
 Also as $x \in (A \setminus C)$, then $x \notin C$.
Hence $x \in A \setminus (B \cup C)$.

The proof of the other equality is left as an exercise.
\end{proof}

We will also need to intersect or union several sets at once.
 If there are
only finitely many, then we simply apply the union or intersection operation
several times.
 However, suppose we have an infinite collection
of sets (a set of sets)
$\{ A_1, A_2, A_3, \ldots \}$.
 We define
\begin{align*}
& \bigcup_{n=1}^\infty A_n := \{ x : x \in A_n \text{ for some $n \in \N$}
\} , \\
& \bigcap_{n=1}^\infty A_n := \{ x : x \in A_n \text{ for all $n \in \N$}
\} .
\end{align*}

We can also have sets indexed by two integers.
 For example, we can have
the set of sets
$\{ A_{1,1}, A_{1,2}, A_{2,1}, A_{1,3}, A_{2,2}, A_{3,1}, \ldots \}$.
 Then
we write 
\begin{equation*}
\bigcup_{n=1}^\infty \bigcup_{m=1}^\infty A_{n,m}
=
\bigcup_{n=1}^\infty \left( \bigcup_{m=1}^\infty A_{n,m} \right) .
\end{equation*}
And similarly with intersections.

It is not hard to see that we can take the unions in any order.  However,
switching the order of unions and intersections is not generally permitted without proof.
For example:
\begin{equation*}
\bigcup_{n=1}^\infty
\bigcap_{m=1}^\infty
\{ k \in \N : mk < n \}
=
\bigcup_{n=1}^\infty \emptyset = \emptyset .
\end{equation*}
However,
\begin{equation*}
\bigcap_{m=1}^\infty
\bigcup_{n=1}^\infty
\{ k \in \N : mk < n \}
=
\bigcap_{m=1}^\infty
\N
=
\N.
\end{equation*}

Sometimes, the index set is not the natural numbers.
 In this case we need a
more general notation.
Suppose $I$ is some set and for each $\iota \in
I$, we have a set $A_\iota$.
Then we define
\begin{equation*}
\bigcup_{\iota \in I} A_\iota := \{ x : x \in A_\iota \text{ for some $\iota \in I$}
\} 
\qquad
\bigcap_{\iota \in I} A_\iota := \{ x : x \in A_\iota \text{ for all $\iota \in I$}
\} .
\end{equation*}

\subsection*{Induction}

When a statement includes an arbitrary natural number,
a common method of proof is the principle of induction.  
We start with the set of natural numbers $\N = \{ 1,2,3,\ldots \}$, and we
give them their natural ordering, 
that is, $1 < 2 < 3 < 4 < \cdots$.
By $S \subset \N$ having a \emph{\myindex{least element}}, we mean that
there exists an $x \in S$,
such that for every
$y \in S$, we have $x \leq y$.

%\pagebreak[2]

The natural numbers $\N$ ordered in the natural way
possess the so-called \emph{\myindex{well ordering property}}.
We take this property
as an axiom; we simply assume it is true.

\theoremstyle{plain}
\newtheorem*{wellordprop}{Well ordering property of $\N$}
\hypertarget{wop:link}{}%
\begin{wellordprop}
Every nonempty subset of $\N$ has a least (smallest) element.
\end{wellordprop}

The \emph{principle of \myindex{induction}}\index{principle of induction} is
the following theorem, which is equivalent to the well ordering property of
the natural numbers.

\begin{thm}[Principle of induction] \label{induction:thm}
Let $P(n)$ be a statement depending on a natural number $n$.  Suppose that
\begin{enumerate}[(i)]
\item \emph{(basis statement)} $P(1)$ is true,
\item \emph{(induction step)} if $P(n)$ is true, then $P(n+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for all $n \in \N$.
\end{thm}

\begin{proof}
Suppose $S$ is the set of natural numbers $m$ for which $P(m)$ is
not true.  

Suppose $S$ is nonempty.  
Then $S$ has a least element by the well ordering property.  
Let us call $m$ the least element of $S$. 

We know $1 \notin S$ by assumption.
Therefore $m > 1$ and $m-1$ is a natural number as well.

Since $m$ was the least element of $S$, we know that $P(m-1)$ is true.
But by the induction step we see that $P(m-1+1) = P(m)$ is true, 
contradicting the statement that $m \in S$.  

Therefore $S$ is empty and $P(n)$ is true for all $n \in \N$.
\end{proof}

Sometimes it is convenient to start at a different number than 1, but 
all that changes is the labeling.
The assumption that
$P(n)$ is true in ``if $P(n)$ is true,
then $P(n+1)$ is true''
is usually called the \emph{\myindex{induction hypothesis}}.

\begin{example}
Let us prove that for all $n \in \N$,
\begin{equation*}
2^{n-1} \leq n! := 1\cdot 2\cdots n.
\end{equation*}
We let $P(n)$ be the statement that
$2^{n-1} \leq n!$ is true.
By plugging in $n=1$, we see that $P(1)$
is true.

Suppose $P(n)$ is true.
That is, suppose 
$2^{n-1} \leq n!$ holds.
Multiply both sides by 2 to obtain
\begin{equation*}
2^n \leq 2(n!) .
\end{equation*}
As $2 \leq (n+1)$ when $n \in \N$, we have
$2(n!) \leq (n+1)(n!) = (n+1)!$.
That is,
\begin{equation*}
2^n \leq 2(n!) \leq  (n+1)!,
\end{equation*}
and hence $P(n+1)$ is true.
By the principle of induction, we see that
$P(n)$
is true for all $n$, and hence
$2^{n-1} \leq n!$ is true for all $n \in \N$.
\end{example}

\begin{example}
We claim that for all $c \not= 1$,
\begin{equation*}
1 + c + c^2 + \cdots + c^n = \frac{1-c^{n+1}}{1-c} .
\end{equation*}
\begin{proof} It is easy to check that the equation holds with $n=1$.  Suppose 
it is true for $n$.
Then
\begin{equation*}
\begin{split}
1 + c + c^2 + \cdots + c^n + c^{n+1} & =
( 1 + c + c^2 + \cdots + c^n ) + c^{n+1} \\
& = \frac{1-c^{n+1}}{1-c}  + c^{n+1} \\
& = \frac{1-c^{n+1}  + (1-c)c^{n+1}}{1-c} \\
& = \frac{1-c^{n+2}}{1-c} .
\end{split}
\end{equation*}
\end{proof}
\end{example}

Some times it is easier to prove inductive step all the statements $P(k)$ is true for all $k = 1,2,\ldots,n$.
This is called \emph{strong induction} and it is equivalent to the standard induction.  
The proof that of the equivalence is left as an exercise.

\begin{thm}[Principle of strong induction]
\index{principle of strong induction}
Let $P(n)$ be a statement depending on a natural number $n$.  Suppose that
\begin{enumerate}[(i)]
\item \emph{(basis statement)} $P(1)$ is true,
\item \emph{(induction step)} if $P(k)$ is true for all $k = 1,2,\ldots,n$, then $P(n+1)$ is true.
\end{enumerate}
Then $P(n)$ is true for all $n \in \N$.
\end{thm}

\subsection*{Functions}

Informally,
a \emph{\myindex{set-theoretic function}}\index{function}
$f$ taking a set $A$ to a set $B$
is a mapping that to each $x \in A$ assigns a unique $y \in B$.
We write
$f \colon A \to B$.  

For example, we may define a function $f \colon S \to T$ taking $S = \{ 0, 1, 2 \}$ to $T = \{ 0, 2 \}$
by assigning $f(0) := 2$, $f(1) := 2$, and $f(2) := 0$.  
That is, a function $f
\colon A \to B$ is
a black box, into which we stick an element of $A$ and the function
spits out an element of $B$.
Sometimes $f$ is called a \emph{\myindex{mapping}}
and we say $f$ \emph{maps $A$ to $B$}.


Often, functions are defined by some sort of
formula, however, you should really think of a function as just a very big
table of values.
The subtle issue here is that a single function can have several different
formulas, all giving the same function.
Also, for many functions, there is
no formula that expresses its values.

To define a function in a more rigorous way, first let us define the Cartesian product.

\begin{defn}
Let $A$ and $B$ be sets.
The \emph{\myindex{Cartesian product}}
is the set of tuples defined as
\begin{equation*}
A \times B :=
\{ (x,y) : x \in A, y \in B \} .
\end{equation*}
\end{defn}

For example, the set $[0,1] \times [0,1]$ is a set in the plane
bounded by a square with vertices $(0,0)$, $(0,1)$, $(1,0)$, and $(1,1)$.
When $A$ and $B$ are the same set we sometimes use a superscript 2 to denote
such a product.
For example $[0,1]^2 = 
[0,1] \times [0,1]$, or $\R^2 = \R \times \R$ (the Cartesian plane).

\begin{defn}
A \emph{function} $f \colon A \to B$ is a subset $f$ of $A \times B$
such that for each $x \in A$, there is a unique $(x,y) \in f$.
We then
write $f(x) = y$ or $f\colon x\mapsto y$.  

The set $A$ is called the \emph{\myindex{domain}} of $f$ (and
sometimes confusingly denoted $D(f)$).
The set
\begin{equation*}
R(f) := \{ y \in B : \text{there exists an $x$ such that
%$(x,y) \in f$
$f(x)=y$
} \}
\end{equation*}
is called the \emph{\myindex{range}} of $f$.
\end{defn}

Sometimes
the set $f$ is called the \emph{\myindex{graph}} of the function rather than
the function itself.

Note that $R(f)$ can possibly be a proper subset of $B$,
while the domain of $f$ is always equal to $A$.
We usually 
assume that the domain of $f$ is nonempty.

\begin{example}
From calculus, you are most familiar with functions taking real numbers to real
numbers.
However, you saw some other types of functions as well.
For
example, the derivative is a function mapping the set of
differentiable functions to the set of all functions.
Another example is the Laplace transform, which also
takes functions to functions.
Yet another example is the function that takes
a continuous function $g$ defined on the interval $[0,1]$ and returns the
number $\int_0^1 g(x) dx$.
\end{example}

\begin{defn}
Let $f \colon A \to B$ be a function, and $C \subset A$.
Define
the \emph{\myindex{image}} (or \emph{\myindex{direct
image}}) of $C$ as
\begin{equation*}
f(C) := \{ f(x) \in B : x \in C \} .
\end{equation*}
Let $D \subset B$.
Define the \emph{\myindex{inverse image}} as
\begin{equation*}
f^{-1}(D) := \{ x \in A : f(x) \in D \} .
\end{equation*}
\end{defn}

\begin{example}
Define the function $f \colon \R \to \R$ by
$f(x) := \sin(\pi x)$.
Then $f([0,\nicefrac{1}{2}]) = [0,1]$, 
$f^{-1}(\{0\}) = \Z$, etc\ldots.
\end{example}

\begin{prop} \label{st:propinv}
Let $f \colon A \to B$.
Let $C, D$ be subsets of $B$.
Then
\begin{align*}
& f^{-1}( C \cup D) = f^{-1} (C) \cup f^{-1} (D) , \\
& f^{-1}( C \cap D) = f^{-1} (C) \cap f^{-1} (D) , \\
& f^{-1}( C^c) = {\left( f^{-1} (C) \right)}^c .
\end{align*}
\end{prop}

Read the last line as
$f^{-1}( B \setminus C) = A \setminus f^{-1} (C)$.

\begin{proof}
Let us start with the union.
Suppose $x \in 
f^{-1}( C \cup D)$.
That means 
$x$ maps to $C$ or $D$.
Thus
$f^{-1}( C \cup D) \subset f^{-1} (C) \cup f^{-1} (D)$.
Conversely
if $x \in f^{-1}(C)$, then $x \in f^{-1}(C \cup D)$.
Similarly for
$x \in f^{-1}(D)$.
Hence
$f^{-1}( C \cup D) \supset f^{-1} (C) \cup f^{-1} (D)$, and we have
equality.

The rest of the proof is left as an exercise.
\end{proof}

For direct images we have the following weaker result.

\begin{prop} \label{st:propfor}
Let $f \colon A \to B$.
Let $C, D$ be subsets of $A$.
Then
\begin{align*}
& f( C \cup D) = f (C) \cup f (D) , \\
& f( C \cap D) \subset f (C) \cap f (D) .
\end{align*}
\end{prop}

The proof is left as an exercise.

\begin{defn}
Let $f \colon A \to B$ be a function.
The function $f$ is said to be
\emph{\myindex{injective}} or
\emph{\myindex{one-to-one}} if $f(x_1) = f(x_2)$ implies $x_1 = x_2$.
In
other words,
for all $y \in B$ the set
$f^{-1}(\{y\})$ is empty or consists of a single element.
We call such an $f$ an \emph{\myindex{injection}}.

The function $f$ is said to be
\emph{\myindex{surjective}} or
\emph{\myindex{onto}} if $f(A) = B$.
We call such an $f$ a \emph{\myindex{surjection}}.

A function $f$ that is both an injection and a surjection is
said to be \emph{\myindex{bijective}}, and we say $f$ is a
\emph{\myindex{bijection}}.
\end{defn}

When $f \colon A \to B$ is a bijection, then $f^{-1}(\{y\})$ is always
a unique element of $A$, and we can consider $f^{-1}$ as a function
$f^{-1} \colon B \to A$.
In this case, we call $f^{-1}$ the \emph{\myindex{inverse function}} of $f$.
For example, for the bijection $f \colon \R \to \R$ defined by $f(x) := x^3$ we have
$f^{-1}(x) = \sqrt[3]{x}$.
%???I WOULD CHANGE TO f(x)=2x SINCE SOMETIMES CUBIC ROOT IS DEFINED ONLY FOR POSITIVE NUMBERS.

A final piece of notation for functions that
we need is the \emph{\myindex{composition of functions}}.

\begin{defn}
Let $f \colon A \to B$, $g \colon B \to C$.
The function 
$g \circ f \colon A \to C$ is defined as
\begin{equation*}
(g \circ f)(x) := g\bigl(f(x)\bigr) .
\end{equation*}
\end{defn}

\subsection*{Equivalence relations}

A binary relation on a set $X$ can be formally defined as a subset of the Cartesian product $X\times X$.

As an example consider relation ``divides''. 
For a pair $(a,b)$ of natural numbers $\N$ we can ask if $a$ divides $b$, if yes we may denote it as $a\mid b$ which means that the pair $(a,b)$ belongs to the relation ``$\mid$''.

We will need a special type of relation called \emph{equivalence relation} which could be vaguely defined as relation which behaves as ``equality''.

More precisely, a relation ``$\sim$'' on a set $X$ is said to be an equivalence relation if it satisfies the following condition for all $a$, $b$ and $c$ in $X$:
\begin{enumerate}[(i)]
\item $a\sim a$. (reflexivity)
\item $a\sim b$ if and only if $b\sim a$. (symmetry)
\item if $a\sim b$ and $b\sim c$ then $a\sim c$. (transitivity)
\end{enumerate}

As an example one can consider the relation of congruence, usually denoted as ``$\cong$'' on the set of all triangles in the euclidean plane.
It is straightforward to check that the three conditions hold for  ``$\cong$''.


The equivalence class of $a\in X$ under the equivalence relation ``$\sim$'', denoted $[a]$ and defined as
$$[a] = \{\,x\in X \mid a\sim x\,\}.$$

The equivalence classes of elements in $X$ either coincide or disjoint. 
In other words, if two equivalence classes intersect then they coincide.

Indeed, if $x\in [a]\cap [b]$, then $a\sim x\sim b$ and by transitivity we have $a\sim b$.
By transitivity the latter implies that $a\sim y$ if and only if $b\sim y$; 
that is, $[a]=[b]$.

The set of all equivalence classes is denoted as $X/{\sim}$;
the map $x\mapsto[x]$ is the natural surjective map from $X$ to $X/{\sim}$

The equivalence relations often used to construct new objects in from the old ones.
For example, on a very formal level the rational numbers are constructed from the integers as equivalence classes of pairs $(m,n)\in \Z\times \N$ equipped with the equivalence relation ``$\sim$'' such that $(m,n)\sim (m',n')$ if and only if $mn'=m'n$. 
The fraction $\nicefrac{m}{n}$ can be thought just as a fancy way to denote the pair $(m,n)$.
After defining the product and sum of such pairs --- ta-da ---
we constructed the rational numbers from integers $\Q=(\Z\times\N)/{\sim}$.

\subsection*{Exercises}

\begin{exercise}
Show
$A \setminus (B \cap C) = (A \setminus B) \cup (A \setminus C)$.
\end{exercise}

\begin{exercise}
Prove that the principle of strong induction is equivalent to the standard
induction.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{st:propinv}.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
\item
Prove \propref{st:propfor}.
\item
Find an example for which equality of sets
in 
$f( C \cap D) \subset f (C) \cap f (D)$
fails.
That is, find an $f$, $A$, $B$, $C$, and $D$ such that
$f( C \cap D)$ is a proper subset of $f(C) \cap f(D)$.
\end{enumerate}
\end{exercise}

\begin{exercise}[Tricky]
Prove that if $A$ is finite, then there exists a unique number $n$ such
that there exists a bijection between $A$ and $\{ 1, 2, 3, \ldots, n \}$.
In other words, the notation $\abs{A} := n$ is justified.
Hint: Show that if $n > m$, then there is no injection from
$\{ 1, 2, 3, \ldots, n \}$ to
$\{ 1, 2, 3, \ldots, m \}$.
\end{exercise}


\begin{exercise}
Prove
\begin{enumerate}[a)]
\item $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
\item $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $A \Delta B$ denote the
\emph{\myindex{symmetric difference}}, that is, the set of all elements that
belong to either $A$ or $B$, but not to both $A$ and $B$.
\begin{enumerate}[a)]
\item
Draw a Venn diagram for
$A \Delta B$.
\item
Show $A \Delta B = (A \setminus B) \cup (B \setminus A)$.
\item
Show $A \Delta B = (A \cup B) \setminus ( A \cap B)$.
\end{enumerate}
\end{exercise}

\begin{exercise}
For each $n \in \N$, let $A_n := \{ (n+1)k : k \in \N \}$.
\begin{enumerate}[a)]
\item Find $A_1 \cap A_2$.
\item Find $\bigcup_{n=1}^\infty A_n$.
\item Find $\bigcap_{n=1}^\infty A_n$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $f \colon A \to B$ and $g \colon B \to C$ be functions.
\begin{enumerate}[a)]
\item
Prove that if $g \circ f$ is injective, then $f$ is injective.
\item
Prove that if $g \circ f$ is surjective, then $g$ is surjective.
\item
Find an explicit example where $g \circ f$ is bijective, but neither $f$
nor $g$ are bijective.
\end{enumerate}
\end{exercise}

\begin{exercise}
Use induction to prove that $n < 2^n$ for any $n\in\N$.
\end{exercise}

\begin{exercise}
Show that for a finite set $A$ of cardinality $n$, the cardinality
of $\sP(A)$ is $2^n$.
\end{exercise}

\begin{exercise}
Prove $\frac{1}{1\cdot 2} + 
\frac{1}{2\cdot 3} + \cdots + \frac{1}{n(n+1)} = \frac{n}{n+1}$
for all $n \in \N$.
\end{exercise}

\begin{exercise}
Prove $1^3 + 2^3 + \cdots + n^3 = {\left( \frac{n(n+1)}{2} \right)}^2$
for all $n \in \N$.
\end{exercise}

\begin{exercise}
Prove that $n^3 + 5n$ is divisible by $6$ for all $n \in \N$.
\end{exercise}

\begin{exercise}
Find the smallest $n \in \N$ such that $2{(n+5)}^2 < n^3$ and call it $n_0$.
Show that $2{(n+5)}^2 < n^3$ for all $n \geq n_0$.
\end{exercise}

\begin{exercise}
Find all $n \in \N$ such that $n^2 < 2^n$.
\end{exercise}

\begin{exercise}
Finish the proof that the \hyperref[induction:thm]{principle of induction} is equivalent to the
\hyperlink{wop:link}{well ordering property}  of $\N$.
That is, prove the well ordering property for $\N$ using the principle of induction.
\end{exercise}

\begin{exercise} Consider the set of pairs of integer and natural numbes $(m,n)\in \Z\times \N$ with the relation ``$\sim$'' defined as $(m,n)\sim (m',n')$ if and only if $mn'=m'n$. 
Show that ``$\sim$'' is an equivalence relation.
\end{exercise}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Definition of real numbers} \label{rn:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\sectionnotes{1.5 lectures}

In this section we review definitions from abstract algebra and define real numbers.

\subsection*{Ordered sets}

The main object we work with in analysis is the set of
\myindex{real numbers}.
As this set is so fundamental, often much time is
spent on formally constructing the set of real numbers.
However, we 
take an easier approach here and just assume that a set with the correct
properties exists.
We need to start with the definitions of those
properties.

\begin{defn}
An \emph{\myindex{ordered set}} is a set $S$, together with
a relation $<$ such that
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
%\begin{enumerate}[(i),nolistsep]
\begin{enumerate}[(i)]
\item For any $x, y \in S$, exactly one of
$x < y$, $x=y$, or $y < x$ holds.
\item If $x < y$ and $y < z$, then $x < z$.
\end{enumerate}
We write $x \leq y$ if $x < y$ or $x=y$.
We define
$>$ and $\geq$ in the obvious way.
\end{defn}


For example, the set of rational numbers $\Q$ is an ordered set by letting
$x < y$ if and only if $y-x$ is a positive rational number, that is
if $y-x = \nicefrac{p}{q}$ where $p,q \in \N$.
Similarly,
$\N$ and $\Z$ are also ordered sets.

There are other ordered sets than sets of numbers.
For example, the set of countries can be ordered by landmass, so for example India $>$ Lichtenstein.
Any time you sort a set in some way, you are making an ordered set.
A typical ordered set that you have used since primary school is the dictionary.
It is the ordered set of words where the order is the so-called lexicographic ordering.
Such ordered sets appear often for example in computer science.
In this class we will mostly be interested in ordered set of numbers however.

\begin{defn}\label{def:sup-inf}
Let $E \subset S$, where $S$ is an ordered set.
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
%\begin{enumerate}[(i),nolistsep]
\begin{enumerate}[(i)]
\item If there exists a $b \in S$ such that $x \leq b$ for all $x \in E$,
then we say $E$ is \emph{\myindex{bounded above}} and $b$
is an \emph{\myindex{upper bound}} of $E$.
\item If there exists a $b \in S$ such that $x \geq b$ for all $x \in E$,
then we say $E$ is \emph{\myindex{bounded below}} and $b$
is a \emph{\myindex{lower bound}} of $E$.
\item If there exists an upper bound $b_0$ of $E$ such that whenever
$b$ is any upper bound for $E$ we have $b_0 \leq b$, then $b_0$
is called the \emph{\myindex{least upper bound}} or
the \emph{\myindex{supremum}}
of $E$.
We write
\begin{equation*}
\sup\, E := b_0  .
\end{equation*}
\item Similarly, if there exists a lower bound $b_0$ of $E$ such that whenever
$b$ is any lower bound for $E$ we have $b_0 \geq b$, then $b_0$
is called the \emph{\myindex{greatest lower bound}} or
the \emph{\myindex{infimum}}
of $E$.
We write
\begin{equation*}
\inf\, E := b_0  .
\end{equation*}
\end{enumerate}
\end{defn}

When a set $E$ is both bounded above and bounded below, we say simply that
$E$ is \emph{bounded}\index{bounded set}.

\begin{exercise}
Let $S$ be an ordered set.
Let $A \subset S$ and suppose 
$b$ is an upper bound for $A$.
Suppose $b \in A$.
Show
that $b = \sup\, A$.
\end{exercise}

\begin{exercise}
Find a (nonstandard) ordering of the set of natural numbers $\N$
such that there exists a nonempty proper subset $A \subsetneq \N$
and such that $\sup\, A$ exists in $\N$, but $\sup\, A \notin A$.
\end{exercise}

\begin{exercise}
Let $D$ be the ordered set of all possible words (not just English words,
all strings of letters of arbitrary length)
using the Latin alphabet using only lower case letters.
The order is the
lexicographic order as in a dictionary (e.g.\ \emph{aa} $<$ \emph{aaa} $<$ \emph{dog} $<$ \emph{door}).
Let $A$ be the subset of $D$ containing the words whose
first letter is `a' (e.g.\ \emph{a} $\in A$, \emph{abcd} $\in A$).
Show that $A$ has a supremum and find what it is.
\end{exercise}

\begin{exercise}
Let $S$ be an ordered set.
Let $B \subset S$ be bounded (above and
below).
Let $A \subset B$ be a nonempty subset.
Suppose all the $\inf$'s and
$\sup$'s exist. Show that
\begin{equation*}
\inf\, B \leq \inf\, A \leq \sup\, A \leq \sup\, B .
\end{equation*}
\end{exercise}

\begin{exercise} \label{exercise:dominatingb}
Let $S$ be an ordered set and
$A$ is a nonempty subset such that $\sup \, A$ exists.
Suppose there
is a $B \subset A$ such that whenever $x \in A$ there is a $y \in B$
such that $x \leq y$.
Show that $\sup \, B$ exists and $\sup \, B = \sup \, A$.
\end{exercise}

A supremum or infimum for $E$ (even if they exist) need not be in $E$.
For example, the set $E := \{ x \in \Q : x < 1 \}$ has a least upper bound of 1, but 1 is not in the set $E$ itself.
On the other hand, if we take $G := \{ x \in \Q : x \leq 1 \}$, then the least upper bound of $G$ is clearly also 1, and in this case $1 \in G$.
On the other hand, the set $P := \{ x \in \Q : x \geq 0 \}$ has no upper bound (why?) and therefore it can have no least upper bound.
On the other hand 0 is the greatest lower bound of $P$.

\begin{exercise} \label{exercise:finitesethasminmax}
Let $S$ be an ordered set.

Let $A \subset S$ be a nonempty finite subset.
Then $A$ is bounded.  
Furthermore, $\inf\, A$ exists and is in $A$ and $\sup\, A$ exists and is in $A$.  
Hint: Use \hyperref[induction:thm]{induction}.
\end{exercise}

\begin{defn} \label{defn:lub}
An ordered set $S$ has the \emph{\myindex{least-upper-bound property}} if
every nonempty
subset $E \subset S$ that is bounded above has a least upper bound,
that is $\sup\, E$ exists in $S$.
\end{defn}

The \emph{least-upper-bound property}
is sometimes called the \emph{\myindex{completeness property}} or the
\emph{\myindex{Dedekind completeness property}}.


\subsection*{Fields}

As you will see in the next section, the set of rational numbers $\Q$ with standard order does not have the least-upper-bound property.
This is one of the most important reasons why we work with real numbers $\R$ in analysis.
The set $\Q$ is just fine for algebraists --- analysts require the least-upper-bound
property to do any work.
We also require our real numbers to have many algebraic properties.  
In particular, we require that they are a \emph{field} defined below.

%\enlargethispage{\baselineskip}
\begin{defn}
A set $F$ is called a \emph{\myindex{field}} if it has two operations
defined on it, addition $x+y$ and multiplication $xy$, and if it satisfies
the following axioms.
\begin{enumerate}[({A}1)]
\item If $x \in F$ and $y \in F$, then $x+y \in F$.
\item \emph{(commutativity of addition)}
$x+y = y+x$ for all $x,y \in F$.
\item \emph{(associativity of addition)}
$(x+y)+z = x+(y+z)$ for all $x,y,z \in F$.
\item There exists an element $0 \in F$ such that
$0+x = x$ for all $x \in F$.
\item For every element $x\in F$ there exists an element $-x \in F$
such that $x + (-x) = 0$.
\end{enumerate}
\begin{enumerate}[({M}1)]
\item If $x \in F$ and $y \in F$, then $xy \in F$.
\item \emph{(commutativity of multiplication)}
$xy = yx$ for all $x,y \in F$.
\item \emph{(associativity of multiplication)}
$(xy)z = x(yz)$ for all $x,y,z \in F$.
\item There exists an element $1 \in F$ (and $1 \not= 0$) such that
$1x = x$ for all $x \in F$.
\item For every $x\in F$ such that $x \not= 0$ there exists an element
$\nicefrac{1}{x} \in F$
such that $x(\nicefrac{1}{x}) = 1$.
%\end{enumerate}
%\begin{enumerate}
\item[(D)] \emph{(distributive law)} $x(y+z) = xy+xz$
for all $x,y,z \in F$.
\end{enumerate}
\end{defn}


For example the set $\Q$ of rational numbers is a field.
An other example is two element set $F=\{0,1\}$ with the multiplication and addition given by
$$0\cdot 0=0\cdot 1=1\cdot 0=1,\quad 1\cdot 1=1,\quad 0+0=1+1=0,\quad 0+1=1+0=1.$$

On the other hand the set $\Z$ of integers is not a field, as it does not contain multiplicative inverses.
For example, there is no $x \in \Z$ such that $2x = 1$, so (M5) is not satisfied.  
You can check that (M5) is the only property that fails\footnote{An algebraist would say that $\Z$ is an ordered ring, or perhaps more precisely a commutative ordered ring.}.

The associativity of addition and multiplication imply that the shortcuts $x+y+z=(x+y)+z=x+(y+z)$ and $xyz=(xy)z=x(yz)$ make sense since in any field. 
We will use the following shortcuts 
\[\nicefrac{x}{y}=x(\nicefrac{1}{y}),\  x^2=xx,\  x^3=xxx,\  2x=x+x,\  3x=x+x+x,\]  
and so on.

In the field one can do the usual tricks with the formulas;
they all follow from the axioms, but sometimes it is tricky.

\begin{example}
In any field we have $0 = 0x$. 
\begin{proof}
By (A4), (D), and (M2) we have
\begin{align*}
xx &= (0+x)x 
\\
&= 0x+xx.
\end{align*}
Using (A5) for $xx$, there is $-xx$.
Adding it to the both sides, and applying (A2), (A3), and (A4), we obtain $0 = 0x$.
\end{proof}
\end{example}

The following exercise includes the most common tricks which makes work in the field easier.

\begin{exercise}
Show that for any elements $x$, $y$ and $z$ in a fild the following conditions hold.
\begin{enumerate}[a)]
\item If $x+z=y+z$ then $x=y$.
\item If $z\ne 0$ and $xz=yz$ then $x=y$.
\item $(-x)(-y)=xy$
\item $-(-x)=x$ 
\item If $x\ne 0$ then $1/(\nicefrac1x)=x$ 
\end{enumerate}
\end{exercise}

\subsection*{Ordered fields}

Now let us marry ordered sets with fields.

\begin{defn}
A field $F$ is said to be an \emph{\myindex{ordered field}} if
$F$ is also an ordered set such that:
\begin{enumerate}[(i)]
\item \label{defn:ordfield:i} For $x,y,z \in F$,  $x < y$ implies $x+z <
y+z$.
\item \label{defn:ordfield:ii} For $x,y \in F$, $x > 0$ and $y > 0$
implies $xy > 0$.
\end{enumerate}
If $x > 0$, we say $x$ is \emph{\myindex{positive}}.
If $x < 0$, we say $x$ is \emph{\myindex{negative}}.
We also say $x$ is \emph{\myindex{nonnegative}} if $x \geq 0$,
and $x$ is \emph{\myindex{nonpositive}} if $x \leq 0$.
\end{defn}

\begin{prop} \label{prop:bordfield}
Let $F$ be an ordered field and $x,y,z \in F$.
Then:
\begin{enumerate}[(i)]
\item \label{prop:bordfield:i} If $x > 0$, then $-x < 0$ (and vice-versa).
\item \label{prop:bordfield:ii} If $x > 0$ and $y < z$, then $xy < xz$.
\item \label{prop:bordfield:iii} If $x < 0$ and $y < z$, then $xy > xz$.
\item \label{prop:bordfield:iv} If $x \not= 0$, then $x^2 > 0$. In particular $1>0$.
\item \label{prop:bordfield:v} If $0 < x < y$, then $0 < \nicefrac{1}{y} <
\nicefrac{1}{x}$.
\end{enumerate}
\end{prop}


\begin{proof}
Let us prove \ref{prop:bordfield:i}.
The inequality $x > 0$ implies by item
\ref{defn:ordfield:i} of definition of ordered field that
$x + (-x) > 0 + (-x)$.
Now apply the algebraic properties of fields to
obtain $0 > -x$.
The ``vice-versa'' follows by similar calculation.

For \ref{prop:bordfield:ii}, first notice that $y < z$ implies
$0 < z - y$ by applying 
item \ref{defn:ordfield:i} of the definition of ordered fields.  
Now apply item 
\ref{defn:ordfield:ii} of the definition of ordered fields to obtain
$0 < x(z-y)$.
By algebraic properties we get $0 < xz - xy$,
and again applying item
\ref{defn:ordfield:i} of the definition we obtain $xy < xz$.

\begin{exercise}
Prove part \ref{prop:bordfield:iii}.
\end{exercise}

To prove part \ref{prop:bordfield:iv} first suppose $x > 0$.
Then
by item 
\ref{defn:ordfield:ii} of the definition of ordered fields we obtain
that $x^2 > 0$ (use $y=x$).
If $x < 0$, we use 
part \ref{prop:bordfield:iii} of this proposition.
Plug in $y=x$ and
$z=0$.

Finally to prove part \ref{prop:bordfield:v}, notice that
$\nicefrac{1}{x}$ cannot be equal to zero (why?).
Suppose $\nicefrac{1}{x} < 0$,
then $\nicefrac{-1}{x} > 0$ by \ref{prop:bordfield:i}.
Then apply
part \ref{prop:bordfield:ii} (as $x > 0$) to obtain
$x(\nicefrac{-1}{x}) > 0x$ or $-1 > 0$, which contradicts $1 > 0$ by using part
\ref{prop:bordfield:i} again.
Hence $\nicefrac{1}{x} > 0$.
Similarly $\nicefrac{1}{y} > 0$.
Thus $(\nicefrac{1}{x})(\nicefrac{1}{y}) > 0$
by definition of ordered field and by part \ref{prop:bordfield:ii}
\begin{equation*}
(\nicefrac{1}{x})(\nicefrac{1}{y})x < (\nicefrac{1}{x})(\nicefrac{1}{y})y .
\end{equation*}
By algebraic properties we get $\nicefrac{1}{y} < \nicefrac{1}{x}$.
\end{proof}

\begin{exercise}
Let $x, y \in F$, where $F$ is an ordered field.
Suppose 
$0 < x < y$.
Show that $x^2 < y^2$.
\end{exercise}

\begin{exercise} Show that $\Q$ admits unique ordering. 
Hint: use Proposition \ref{prop:bordfield}\ref{prop:bordfield:iv}
\end{exercise}


Product of two positive numbers (elements of an ordered field) is positive.
However, it is not true that if the product is positive, then each of the two
factors must be positive.

\begin{prop}
Let $x,y \in F$ where $F$ is an ordered field.
Suppose 
$xy > 0$.
Then either both $x$ and $y$ are positive, or both are negative.
\end{prop}

\begin{proof}
Clearly both of the conclusions can happen.
If either
$x$ and $y$ are zero, then $xy$ is zero and hence not positive.
Hence we assume that $x$ and $y$ are nonzero,
and we simply need to show that if they have opposite signs, then
$xy < 0$.
Without loss of
generality suppose $x > 0$ and $y < 0$.

Multiply $y < 0$ by $x$ to get
$xy < x0 = 0$.

The result follows by contrapositive.
\end{proof}

\begin{exercise}
Let $F = \{ 0, 1, 2 \}$ ba a field and $0$ and $1$ have their usual meaning of (A4) and (M4).
\begin{enumerate}[a)]
\item Show that  
\[0\cdot0=0\cdot 1=0\cdot 2=0,\quad 1\cdot 1=2\cdot 2=1,\quad 1\cdot 2=2,\]
\[0+0=1+2=0,\quad 0+ 1=2+ 2=1,\quad 0+2=1+1=2.\]
\item Show that $F$ is not an ordered field.
\item Show that arbitrary finite field does not admit an order.
\end{enumerate}
\end{exercise}

It can be checked that the rational numbers $\Q$ with the
standard ordering is an ordered field.
Moreover, by the following proposition, we can (and will) assume that ordered contains $\Q$.

\begin{prop}\label{prop:QinF}
Any ordered field $F$ contains a \emph{copy} of $\Q$.

More precisely, there is a injection $i\colon\Q\to F$ which respects multiplication and addition;
that is, $i(x\cdot y)=i(x)\cdot i(y)$ and $i(x+ y)=i(x)+i(y)$ for any $x,y\in\Q$.
\end{prop}

We will present an informal proof --- a formal proof will take us too far into the field of abstract algebra.

\begin{proof}[Informal proof]
Given a natural number $n \in \N$, we can consider the corresponding element in $F$
\[n:=\underbrace{1+\dots +1}_{\text{$n$ times}}.\]

Since $F$ is an ordered field,  Proposition~\ref{prop:bordfield}(\ref{prop:bordfield:iv}) implies that $1 > 0$.
By \hyperref[induction:thm]{induction} (exercise) we can prove that $n> 0$ for all $n \in \N$ and all these numbers are different in $F$.
In other words $\N\subset F$ --- the natural numbers can be considered as a subset the real numbers.

Since $F$ is a field we can take difference and ratios.
By taking differences of all natural numbers we get all integers $\Z$. 
Futher by taking ratios of integers by natural numbers we get all the rational numbers $\Q$.
\end{proof}


The reader may also know about the \emph{\myindex{complex numbers}},
usually denoted by
$\C$.
That is, $\C$ is the set of numbers of
the form $x + iy$, where $x$ and $y$ are real numbers, and $i$ is the
imaginary number, a number such that $i^2 = -1$.
The reader may
remember from algebra that $\C$ is also a field, however, it is not an
ordered field.
While one can make $\C$ into an ordered set in some way, the following exercise shows that $\C$ is not an ordered field.

\begin{exercise}
Assume that a field $F$ contains an element $i$ such that $i^2+1=0$.
Show that $F$ does not admit an order.
\end{exercise}


\subsection*{Real numbers} \label{sec:setofreals}

\sectionnotes{2 lectures, the extended real numbers are optional}
 

\begin{defn}
An ordered field with the \hyperref[defn:lub]{least-upper-bound property} is called the \emph{field of real numbers}. 
It will be further denoted as $\R$.
\end{defn}

Recall that by Proposition \ref{prop:QinF}, we can (and will) assume that $\Q\subset \R$;
that is, the field of rational numbers forms a subset in the field of reals.

Let us prove one of the most basic but useful results about the real numbers.
The following proposition is essentially how an analyst proves that a number
is zero.

\begin{prop}
If $x \in \R$ is such that $x \geq 0$ and $x \leq \epsilon$ for all
$\epsilon \in \R$ where
$\epsilon > 0$, then $x = 0$.
\end{prop}

\begin{proof}
%If $x > 0$, then taking
%$\epsilon = x$ we get the contradiction $x < x$.  Therefore $x=0$.
If $x > 0$, then $0 < \nicefrac{x}{2} < x$ (why?).  
Taking $\epsilon = \nicefrac{x}{2}$ obtains a contradiction.  
Thus $x=0$.
\end{proof}

The proof above for any ordered field, but the most useful property of $\R$ for analysts is \hyperref[defn:lub]{least-upper-bound property}.  
Essentially, in analysts we want $\Q$, but we also want to take suprema (and infima) willy-nilly.  
So what we do is to throw in enough numbers to obtain $\R$.
In the next section you will see the first application of the \hyperref[defn:lub]{least-upper-bound property}

\subsection*{On square root of two}

Here we will show that reals contain $\sqrt{2}$, but rationals do not.

The proof requires quite a bit of work.
However, using theorems the intermediate value theorem (see \ref{IVT:thm} below)
we could write this proof in a few lines.
We hurry up since we wanted to show that not all reals are rational right after the definition of reals.

\begin{example} \label{example:sqrt2}
There exists a unique positive real number $r$ such that $r^2 = 2$.  
We denote $r$ by $\sqrt{2}$.

\medskip

In the following, it may seem we are pulling certain expressions out of a hat.  
When writing a proof such as this we would, of course, come up with the expressions only after playing around with what we wish to prove.  
The order in which we write the proof is not necessarily the order in which we come up with the proof.

\begin{proof}
Consider the set 
$$A := \{ x \in \R : x^2 < 2 \}.$$  

Assume $x> 2$.
Since $x$ and $2$ are positive, \propref{prop:bordfield}(\ref{prop:bordfield:ii}) implies 
$x^2> 2x$ and $2x> 4$. 
Hence $x^2 > 4$.

Therefore any number $x$ such that $x > 2$ is not in $A$.  
That is $2$ is an upper bound of $A$;
in particular $A$ is bounded above.
Further note that $1 \in A$, so $A$ is nonempty.

Define $r := \sup\, A$.  
We will show that $r^2 = 2$ by showing two inequalities $r^2 \geq 2$ and $r^2 \leq 2$.  
(This is the way analysts show equality.)
Note that we already know that $2\geq r \geq 1 > 0$.

Take a positive number $s$ such that $s^2 < 2$.  
We wish to find an $h$ such that $0<h<1$ and $(s+h)^2 < 2$.
Once it is done we see that $s$ is not an upper bound for $A$.
Therefore $s\ne r$ and since $s$ is arbitrary such that $s^2 < 2$, we will get $r^2\geq 2$.

Since $s^2<2$, the value $h=\tfrac{2-s^2}5$ is positive.
From above, $0<s<2$ and therefore $0<h<1$.
Hence
\begin{align*}
(s+h)^2&= s^2+(2s+h)h<
\\
&< s^2+5(\tfrac{2-s^2}5)=
\\
&=2.
\end{align*}

Similarly, for a positive number $s$ such that $s^2 > 2$,  
we wish to find an $h > 0$ such that ${(s-h)}^2 > 2$.
Once it is done we see that $s$ is not the least upper bound for $A$.
Therefore $s\ne r$ and since $s$ is arbitrary number such that $s^2 > 2$, we will get $r^2\leq 2$.

Take $h=\tfrac{s^2-2}{2s}$; since $s^2>2$, we have $h>0$.
Therefore
\begin{align*}
(s-h)^2&= s^2-2sh+h^2>
\\
&> s^2-2s(\tfrac{s^2-2}{2s})=
\\
&= 2.
\end{align*}


\medskip

Together, $r^2 \geq 2$ and $r^2 \leq 2$ imply $r^2 = 2$.  
The existence part is finished.

To prove uniqueness.  
Suppose $s \in \R$ such that $s^2 = 2$ and $s > 0$.
Thus $s^2 = r^2$.  
If $0 < s < r$, then $s^2 < r^2$.  
Similarly $0 < r < s$ implies $r^2 < s^2$.  
Hence $s = r$.
\end{proof}
\end{example}

Now let us show that $\sqrt{2}$ is not rational.

\begin{prop} 
There is no positive rational number $r$ sucht that $r^2=2$;
in other words $\sqrt{2}\notin\Q$.
\end{prop}


\begin{proof}
Suppose $r \in \Q$ such that $r^2 = 2$.
Write $r=\nicefrac{m}{n}$ in lowest terms.  
So ${(\nicefrac{m}{n})}^2 = 2$ or $m^2 = 2n^2$.  
Hence $m^2$ is divisible by 2 and so $m$ is divisible by 2.  
Write $m = 2k$ and so ${(2k)}^2 = 2n^2$.  
Divide by 2 and note that $2k^2 = n^2$, and hence $n$ is divisible by 2.
But that is a contradiction as $\nicefrac{m}{n}$ was in lowest terms.
\end{proof}

\begin{exercise}
Mimic the proof above to show that $\sqrt[3]{3}$ is irrational;
that is, there is no rational number $q$ such that $q^3=3$.
\end{exercise}

\begin{exercise}
Consider the set $S$ of real numbers of the form $a+b\sqrt{2}$, where $a,b\in\Q$.
Given $x=a+b\sqrt{2}$ for $a,b\in\Q$, set $\bar x=a-b\sqrt{2}$.
\begin{enumerate}[a)]
\item Show that $S$ is a field and any element of $S$ admits a unique representation as $a+b\sqrt{2}$, where $a,b\in\Q$. Conclude that the map $x\mapsto \bar x$ is well defined.
\item\label{ex:Q(sqrt2):b} Show that the map $x\mapsto \bar x$ respects muliplication and addition; that is 
 \[\overline{x\cdot y}=\bar x\cdot \bar y,\quad \overline{x+y}=\bar x+\bar y\]
for any $x,y\in S$. 
\item Use \ref{ex:Q(sqrt2):b} to show that the field $S$ admits distinct orderings.
\end{enumerate}


\end{exercise}


The complement $\R \setminus \Q$ is called the set of \emph{\myindex{irrational}} numbers.  
We just saw that $\sqrt{2} \notin \Q$;
in particular $\R \setminus \Q$ is nonempty.
In particular it implies that $\Q$ does not have an upper bound property.


In Chapter~\ref{rn:counable} we will see that is the set of irrational numbers is very large --- most of real numbers are irrational.


\subsection*{Archimedean property}

The following is one of the fundamental facts about the real numbers.

\begin{thm}\label{thm:arch:i}
The set of natural numbers $\N$ is unbounded in $\R$.

In particular, if $x, y \in \R$ and
$x > 0$, then there exists an $n \in \N$ such that $nx>y$.
\end{thm}

The second statement in the theorem is called \myindex{Archimedean property}.%
\footnote{It appears as Axiom V of Archimedes' ``On the Sphere and Cylinder'' 225 BC.}


\begin{proof}
Arguing by contradiction, suppose that $\N$ is bounded above.

Let $b := \sup \N$.
The number $b-1$ cannot possibly be an upper bound for $\N$ as it is strictly less than $b$ (the supremum).

Thus there exists an $m \in \N$ such that $m > b-1$.
We add one to obtain $m+1 > b$, which contradicts $b$ being an upper bound.

In particular there is a natural number $n$ such that $n>\nicefrac{y}{x}$.
Since $x>0$, we get $nx>y$.
\end{proof}

\begin{exercise}
Prove that
if $t \geq 0$ ($t \in \R$), then there exists an $n \in \N$ such that $n-1 \leq t < n$.
\end{exercise}

Let us state and prove a simple but useful corollary of the
\hyperref[thm:arch:i]{Archimedean property}.

\begin{cor}
$\inf \{ \nicefrac{1}{n} : n \in \N \} = 0$.
\end{cor}

\begin{proof}
Let $A := \{ \nicefrac{1}{n} : n \in \N \}$.  
Obviously $A$ is not empty.
Furthermore, $\nicefrac{1}{n} > 0$ and so 0 is a lower bound, and $b := \inf\, A$ exists.
As 0 is a lower bound, then $b \geq 0$.
Now take an arbitrary $a > 0$.  
By the \hyperref[thm:arch:i]{Archimedean property} there exists an $n$ such that
$na > 1$, or in other words $a > \nicefrac{1}{n} \in A$.
Therefore
$a$ cannot be a lower bound for $A$.
Hence $b=0$.
\end{proof}

\begin{exercise}
Assume $t$ is nonnegative real number such that $t\le \tfrac{1+n^2}{1+n^4}$ for any $n\in \N$.
Show that $t=0$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Properties of real numbers} \label{rn:properties}

In this chapter we introduce the standard constructions in the field of real numbers.
Most of them work in any ordered field.

\subsection*{Extended real numbers}

To make using suprema and infima even easier, we may want to
write $\sup\, A$ and $\inf\, A$ without worrying about $A$ being
bounded and nonempty.
We make the following natural definitions.

\begin{defn}
Let $A \subset \R$ be a set.
\begin{enumerate}[(i)]
\item If $A$ is empty, then $\sup\, A := -\infty$.
\item If $A$ is not bounded above, then $\sup\, A := \infty$.
\item If $A$ is empty, then $\inf\, A := \infty$.
\item If $A$ is not bounded below, then $\inf\, A := -\infty$.
\end{enumerate}
\end{defn}

For convenience,  $\infty$ and $-\infty$ are sometimes treated as if they were
numbers, except we do not allow arbitrary arithmetic with them.
We make $\R^* := \R \cup \{ -\infty , \infty\}$ into an ordered set
by letting
\begin{equation*}
-\infty < \infty \quad \text{and} \quad
-\infty < x \quad \text{and} \quad
x < \infty \quad \text{for all $x \in \R$}.
\end{equation*}

The set $\R^*$ is called the set of \emph{\myindex{extended real numbers}}.
It is possible to define some arithmetic on $\R^*$.  
Most operations
are extended in an obvious way, but we must leave
$\infty-\infty$, $0 \cdot (\pm\infty)$, and $\frac{\pm\infty}{\pm\infty}$
undefined.
We refrain from using this arithmetic, it leads to easy mistakes as $\R^*$ is not a field.

Now we can take suprema and infima without fear of emptiness or
unboundedness.  
In this book we mostly avoid using $\R^*$ outside of exercises, and leave such generalizations to the interested reader.

\subsection*{Intervals}

You surely saw the notation for intervals\index{interval}
before, but let us give a formal
definition here.
For $a,  b \in \R$ such that $a < b$ we define
\begin{align*}
& [a,b] := \{ x \in \R : a \leq x \leq b \}, \\
& (a,b) := \{ x \in \R : a < x < b \}, \\
& (a,b] := \{ x \in \R : a < x \leq b \}, \\
& [a,b) := \{ x \in \R : a \leq x < b \} .
\end{align*}
The interval $[a,b]$ is called a \emph{\myindex{closed interval}}
and $(a,b)$ is called an \emph{\myindex{open interval}}.
The intervals
of the form $(a,b]$ and $[a,b)$ are called
\emph{half-open intervals}\index{half-open interval}.

The above intervals were all \emph{bounded intervals}\index{bounded
interval}, since both $a$ and $b$ were real numbers.
We 
define \emph{unbounded intervals}\index{unbounded interval},
\begin{align*}
& [a,\infty) := \{ x \in \R : a \leq x \}, \\
& (a,\infty) := \{ x \in \R : a < x \}, \\
& (-\infty,b] := \{ x \in \R : x \leq b \}, \\
& (-\infty,b) := \{ x \in \R : x < b \} .
\end{align*}
For completeness we define $(-\infty,\infty) := \R$.


\begin{exercise} \label{exercise:intervaldef}
Suppose $I \subset \R$ is a subset with at least 2 elements
such that if $a < b < c$ and $a, c \in I$, then $b\in I$.
Show that $I$ is one of the nine types of intervals explicitly given in this section.
Furthermore, prove that the intervals given in this section
all satisfy this property.
\end{exercise}

\begin{exercise}
Show that an intersection
of a possibly infinite family of closed intervals is either empty, a single point,
or a closed interval.
\end{exercise}


\subsection*{Supremum and infimum}

The following fact about supremum and infimum is often useful.
It follow directly from the definitions of least upper bound and greatest lower bound \ref{def:sup-inf}.

\begin{prop} \label{prop:existsxepsfromsup}
If $S \subset \R$ is a nonempty set and $b$ is its upper bound.
Then $b=\sup\, S$ if and only if for every $\epsilon > 0$ there exists $x \in S$ such
that $x>b - \epsilon$.

Analogously, if $S \subset \R$ is a nonempty set and $a$ is its lower bound.
Then $a=\inf\, S$ if and only if for every $\epsilon > 0$ there exists $x \in S$ such
that $x<a + \epsilon$.
\end{prop}
 
\begin{example}\label{ex:sup(x+A)}
Let $A \subset \R$ be nonempty bounded above and $x \in \R$.
Define
\begin{align*}
x + A & := \{ x+y \in \R : y \in A \}. 
\end{align*}

Assume $B=A+x$. Show that 
$$\sup B = x + \sup\, A.$$
In particular supremum of $B$ exists, which means that the set $B$ is nonempty and bounded from above.

\begin{proof}
Note that $y \leq sup\, A$ for all $y \in A$.
Then $x+y \leq x+sup\, A$ for all $y \in A$, and so $x+sup\, A$ is an upper
bound for $B$.
In particular, 
\begin{equation*}
\sup\, B \leq  x+ \sup\, A.
\end{equation*}

Note that $A=(-x)+B$.
Therefore the same argument as above implies 
\begin{equation*}
\sup\, A \leq  (-x)+ \sup\, B.
\end{equation*}
Hence 
\begin{equation*}
\sup\, B =  x+ \sup\, A.
\end{equation*}
\end{proof}
\end{example}

\begin{exercise}
\label{prop:supinfalg}
Set $xA  := \{ xy \in \R : y \in A \}$.
Assume $A \subset \R$ be nonempty. 
Prove the following:
\begin{enumerate}[(i)]
\item If $x \in \R$ and $A$ is bounded below, then $\inf (x+A) = x + \inf\, A$.
\item If $x > 0$ and $A$ is bounded above, then $\sup (xA) = x ( \sup\, A )$.
\item If $x > 0$ and $A$ is bounded below, then $\inf (xA) = x ( \inf\, A )$.
\item If $x < 0$ and $A$ is bounded below, then $\sup (xA) = x ( \inf\, A )$.
\item If $x < 0$ and $A$ is bounded above, then $\inf (xA) = x ( \sup\, A )$.
\end{enumerate}
Do note that multiplying a set by a negative number switches supremum for an infimum and vice-versa.  
\end{exercise}

Sometimes we need to apply supremum or infimum twice. 
Here is an example.

\begin{prop} \label{infsupineq:prop}
Let $A, B \subset \R$ be nonempty sets such that $x \leq y$ whenever $x \in A$ and
$y \in B$.
Then $A$ is bounded above, $B$ is bounded below, and $\sup\, A \leq \inf\, B$.
\end{prop}

\begin{proof}
Any $x \in A$ is a lower bound for $B$.  
Therefore $x \leq \inf\, B$ for all $x \in A$, so $\inf\, B$ is an upper bound for
$A$.
Hence, $\sup\, A \leq \inf\, B$.
\end{proof}

We must be careful about strict inequalities and taking suprema and
infima.  
Note that $x < y$ whenever $x \in A$ and $y \in B$ still only implies $\sup\, A \leq \inf\, B$, and not a strict inequality.  
This is an important subtle point that comes up often.
For example, take $A := (-\infty,0)$ and take $B := (0,\infty)$.
Then $x < y$ for all $x \in A$ and $y \in B$.  
However, $\sup\, A = 0$ and $\inf\, B = 0$.


\begin{exercise}
Let $A,B\subset \R$ be bounded nonempty sets.
Assume for any $a\in A$ there is $b\in B$ such that $a\le b$.
Show that $\sup A\le \sup B$.
\end{exercise}


\begin{exercise} \label{exercise:supofsum}
Let $A$ and $B$ be two nonempty bounded sets of real numbers.
Let
$C := \{ a+b : a \in A, b \in B \}$.
Show that $C$ is a bounded set and that
\begin{equation*}
\sup\,C = \sup\,A + \sup\,B 
\qquad \text{and} \qquad
\inf\,C = \inf\,A + \inf\,B .
\end{equation*}
Hint: Use Proposition~\ref{prop:existsxepsfromsup}.
\end{exercise}

\begin{exercise}
Let $A$ and $B$ be two nonempty bounded sets of nonnegative real numbers.
Define the set
$C := \{ ab : a \in A, b \in B \}$.
Show that $C$ is a bounded set and that
\begin{equation*}
\sup\,C = (\sup\,A )( \sup\,B) 
\qquad \text{and} \qquad
\inf\,C = (\inf\,A )( \inf\,B).
\end{equation*}
Hint: Use Proposition~\ref{prop:existsxepsfromsup}.
\end{exercise}

\subsection*{Maximum and minimum}

By \exerciseref{exercise:finitesethasminmax} we know 
a finite set of numbers always has a supremum or an infimum that is contained
in the set itself.
In this case we usually do not use the words
supremum or infimum.

When a set $A$ of real numbers is bounded above,
such that 
$\sup\, A \in A$, then we can use the word
\emph{\myindex{maximum}} and the notation $\max A$ to denote the supremum.
Similarly for infimum; when a set $A$ is bounded below
and $\inf\, A \in A$, then we can use the
word \emph{\myindex{minimum}} and the notation $\min\, A$.
For example,
\begin{align*}
& \max \{ 1,2.4,\pi,100 \} = 100 , \\
& \min \{ 1,2.4,\pi,100 \} = 1 .
\end{align*}
While writing $\sup$ and $\inf$ may be technically
correct in this situation, $\max$ and
$\min$ are generally used to emphasize that the supremum or infimum
is in the set itself.

Notice that any set of integers bounded below has well defined  minimum.
The same way any set of integers bounded above has maximum.
These properties maybe considered as a reformulation of \hyperref[wop]{well ordering property of $\N$}. 

\subsection*{Absolute value} \label{sec:absval}

\sectionnotes{0.5--1 lecture}

A concept we will encounter over and over is the concept of
\emph{\myindex{absolute value}}.
You want to think of the absolute value as the ``size'' of a real number.
Let us give a formal definition.
\begin{equation*}
\abs{x} :=
\begin{cases}
x & \text{ if $x \geq 0$}, \\
-x & \text{ if $x < 0$} .
\end{cases}
\end{equation*}

\begin{exercise}
Show that
\begin{enumerate}[a)]
\item
$\max \{x,y\} = \frac{x+y+\abs{x-y}}{2}$
\item
$\min \{x,y\} = \frac{x+y-\abs{x-y}}{2}$
\end{enumerate}
\end{exercise}

Let us give the main features of the absolute
value as a proposition.

\begin{prop} \label{prop:absbas}
{\ }
\begin{enumerate}[(i)]
\item \label{prop:absbas:i} $\abs{x} \geq 0$, and $\abs{x}=0$ if and only if $x = 0$.
\item \label{prop:absbas:ii} $\abs{-x} = \abs{x}$ for all $x \in \R$.
\item \label{prop:absbas:iii} $\abs{xy} = \abs{x}\abs{y}$ for all $x,y \in \R$.
\item \label{prop:absbas:iv} $\abs{x}^2 = x^2$ for all $x \in \R$.
\item \label{prop:absbas:v} $\abs{x} \leq y$ if and only if $-y \leq x \leq y$.
\item \label{prop:absbas:vi} $-\abs{x} \leq x \leq \abs{x}$ for all $x \in \R$.
\end{enumerate}
\end{prop}

\begin{proof}
\ref{prop:absbas:i}: This statement is not difficult to see from the definition.

\medskip

\ref{prop:absbas:ii}: Suppose $x > 0$, then $\abs{-x} = -(-x) = x =
\abs{x}$.
Similarly when $x < 0$, or $x = 0$.

\medskip

\ref{prop:absbas:iii}:
If $x$ or $y$ is zero, then the result is obvious.
When $x$ and $y$ are both positive, then $\abs{x}\abs{y} = xy$.  $xy$ is also positive and hence $xy = \abs{xy}$.
If $x$ and $y$ are both negative
then $xy$ is still positive and $xy = \abs{xy}$, and
$\abs{x}\abs{y} = (-x)(-y) = xy$.
Next assume
$x > 0$ and $y < 0$.
Then $\abs{x}\abs{y} = x(-y) = -(xy)$.
Now
$xy$ is negative and hence $\abs{xy} = -(xy)$.
Similarly if
$x < 0$ and $y > 0$.

\medskip

\ref{prop:absbas:iv}:
Obvious if $x \geq 0$.
If $x < 0$, then $\abs{x}^2 = {(-x)}^2 =
x^2$.

\medskip

\ref{prop:absbas:v}:  Suppose $\abs{x} \leq y$.
If $x \geq 0$,
then $x \leq y$.
Obviously $y \geq 0$ and hence $-y \leq 0 \leq x$ so $-y \leq x \leq y$
holds.
If $x < 0$, then $\abs{x} \leq y$ means $-x \leq y$.
Negating both
sides we get $x \geq -y$.
Again $y \geq 0$ and so $y \geq 0 > x$.
Hence, $-y \leq x \leq y$.

On the other hand, suppose 
$-y \leq x \leq y$ is true.
If $x \geq 0$, then $x \leq y$ is equivalent
to $\abs{x} \leq y$.
If $x < 0$, then $-y \leq x$ implies
$(-x) \leq y$, which is equivalent to $\abs{x} \leq y$.

\medskip

\ref{prop:absbas:vi}:  Apply \ref{prop:absbas:v} with $y = \abs{x}$.
\end{proof}

\begin{exercise}
Show that
$\abs{x-y} < \epsilon$ if and only if $y\in (x-\epsilon,x+\epsilon)$.
\end{exercise}

A property used frequently enough to give it a name is the so-called
\emph{\myindex{triangle inequality}}.

\begin{prop}[Triangle Inequality]
$\abs{x+y} \leq \abs{x}+\abs{y}$
for all $x, y \in \R$.
\end{prop}

\begin{proof}
From \propref{prop:absbas} we
have $- \abs{x} \leq x \leq \abs{x}$ and
$- \abs{y} \leq y \leq \abs{y}$.
We add these two inequalities to obtain
\begin{equation*}
- (\abs{x}+\abs{y}) \leq x+y \leq \abs{x}+ \abs{y} .
\end{equation*}
Again by \propref{prop:absbas} we have 
$\abs{x+y} \leq \abs{x}+\abs{y}$.
\end{proof}

There are other often applied versions of the triangle inequality.

\begin{cor}
Let $x,y \in \R$
\begin{enumerate}[(i)]
\item \emph{(\myindex{reverse triangle inequality})}
~
$\bigl\lvert (\abs{x}-\abs{y}) \bigr\rvert \leq \abs{x-y}$.
\item $\abs{x-y} \leq \abs{x}+\abs{y}$.
\end{enumerate}
\end{cor}

\begin{proof}
Let us plug in $x=a-b$ and $y=b$ into the standard
triangle inequality to obtain
\begin{equation*}
\abs{a} = \abs{a-b+b} \leq \abs{a-b} + \abs{b} ,
\end{equation*}
or $\abs{a}-\abs{b} \leq \abs{a-b}$.
Switching the roles of $a$ and $b$
we obtain 
or $\abs{b}-\abs{a} \leq \abs{b-a} = \abs{a-b}$.
Now applying
\propref{prop:absbas} again we obtain the reverse triangle
inequality.

The second version of the triangle inequality is obtained from the standard
one by just replacing $y$ with $-y$ and noting again that $\abs{-y} =
\abs{y}$.
\end{proof}

\begin{cor}
Let $x_1, x_2, \ldots, x_n \in \R$.
Then
\begin{equation*}
\abs{x_1 + x_2 + \cdots + x_n} \leq 
\abs{x_1} + \abs{x_2} + \cdots + \abs{x_n} .
\end{equation*}
\end{cor}

\begin{proof}
We proceed by \hyperref[induction:thm]{induction}.
The conclusion holds trivially for $n=1$, and
for $n=2$ it is the standard triangle inequality.
Suppose the corollary
holds for $n$.
Take $n+1$ numbers $x_1,x_2,\ldots,x_{n+1}$ and 
first use the standard triangle inequality, then the induction
hypothesis
\begin{equation*}
\begin{split}
\sabs{x_1 + x_2 + \cdots + x_n + x_{n+1}} & \leq 
\sabs{x_1 + x_2 + \cdots + x_n} + \sabs{x_{n+1}} \\
& \leq 
\sabs{x_1} + \sabs{x_2} + \cdots + \sabs{x_n} + \sabs{x_{n+1}} .  \qedhere
\end{split}
\end{equation*}
\end{proof}

Let us see an example of the use of the triangle inequality.

\begin{example}
Find a number $M$ such that $\sabs{x^2-9x+1} \leq M$ for all $-1 \leq x \leq
5$.

Using the triangle inequality, write
\begin{equation*}
\sabs{x^2-9x+1} \leq \sabs{x^2}+\sabs{9x}+\sabs{1}
=
\sabs{x}^2+9\sabs{x}+1 .
\end{equation*}
It is obvious that 
$\sabs{x}^2+9\sabs{x}+1$ is largest when $\abs{x}$ is largest.
In the interval
provided, $\abs{x}$ is largest when $x=5$ and so $\abs{x}=5$.
One
possibility for $M$ is
\begin{equation*}
M = 5^2+9(5)+1 = 71 .
\end{equation*}
There are, of course, other $M$ that work.
The bound of 71
is much higher than it
need be, but we didn't ask for the best possible $M$, just one that works.
\end{example}

\begin{exercise}
Find a number $M$ such that $\sabs{x^3-x^2+8x} \leq M$ for all $-2 \leq x \leq
10$.
\end{exercise}

\begin{exercise}
Find $\epsilon>0$ such that $x^2-x^4>\epsilon$ for all  $\nicefrac13<x<\nicefrac23$.
\end{exercise}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Sequences and Series} \label{seq:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sequences}
\label{sec:seqsandlims}

\sectionnotes{2.5 lectures}

Analysis is essentially about taking limits.
The most basic type of a limit
is a limit of a sequence of real numbers.
We have already seen sequences used informally.
Let us give the formal
definition.

\begin{defn}
A \emph{\myindex{sequence}} (of real numbers) is a function $x \colon \N \to \R$.
\end{defn}


Instead of $x(n)$ we 
usually denote the $n$th element in the sequence by $x_n$.
We 
use the notation $\{ x_n \}$, or more precisely
\begin{equation*}
\{ x_n \}_{n=1}^\infty,
\end{equation*}
to denote a sequence.

When we need
to give a concrete sequence we often give each term as a formula in
terms of $n$.
For example, $\{ \nicefrac{1}{n} \}_{n=1}^\infty$, or simply $\{
\nicefrac{1}{n} \}$, stands for
the sequence $1, \nicefrac{1}{2}, \nicefrac{1}{3}, \nicefrac{1}{4},
\nicefrac{1}{5}, \ldots$.
The sequence $\{ \nicefrac{1}{n} \}$
is a bounded sequence ($B=1$ will
suffice).
On the other hand the sequence $\{ n \}$ stands for
$1,2,3,4,\ldots$, and this sequence is not bounded (why?).

While the notation for a sequence
is similar\footnote{\cite{BS} use the notation $(x_n)$ to denote
a sequence instead of $\{ x_n \}$, which is what \cite{Rudin:baby} uses.
Both are common.}
to that of a set, the notions are
distinct.
For example, the sequence $\{ {(-1)}^n \}$ is the sequence
$-1,1,-1,1,-1,1,\ldots$, whereas the set of values, the
\emph{range of the sequence}\index{range of a sequence},
is just the set $\{ -1, 1 \}$.
We can write this set
as $\{ {(-1)}^n : n \in \N \}$.
When ambiguity can arise, we use the words \emph{sequence} or \emph{set} to distinguish the two concepts.

Another example of a sequence is the so-called \emph{\myindex{constant sequence}}.
That is a sequence $\{ c \} = c,c,c,c,\ldots$ consisting of a single
constant $c \in \R$ repeating indefinitely.

\begin{defn}
A sequence $\{ x_n \}$ is \emph{bounded}\index{bounded sequence} if
there exists a $B \in \R$ such that
\begin{equation*}
\abs{x_n} \leq B \qquad \text{for all $n \in \N$.}
\end{equation*}
In other words, the sequence $\{x_n\}$ is bounded whenever
the set $\{ x_n : n \in \N \}$
is bounded.
\end{defn}

\begin{exercise}
Which of the following sequences are bounded?  
Prove or disprove.
\begin{enumerate}[a)]
 \item $\{ 3n \}$
 \item $\{ (-\nicefrac12)^n \}$
 \item $\{ (-2)^n \}$
 \item $\{ (-1)^n \}$
\end{enumerate}

\end{exercise}



We now get to the idea of a \emph{\myindex{limit of a sequence}}.  We will
see in \propref{prop:limisunique}
that the notation below is well defined.
That is, if a limit exists, then
it is unique.
So it makes sense to talk about \emph{the} limit of a sequence.

\begin{defn}
A sequence $\{ x_n \}$ is said to \emph{\myindex{converge}} to a number
$x \in \R$, if for every $\epsilon > 0$, there exists an $M \in \N$ such
that $\abs{x_n - x} < \epsilon$ for all $n \geq M$.
In this case, the number $x$ is said to be the \emph{limit} of $\{ x_n \}$.
We write
\begin{equation*}
\lim_{n\to \infty} x_n := x .
\end{equation*}

A sequence that converges is said to be \emph{convergent}\index{convergent sequence}.
Otherwise, the sequence is said to be \emph{divergent}\index{divergent sequence}.
\end{defn}

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input sequence-convergence.eepic

\vspace*{12pt}

\hspace{13pt} \input sequence-convergence-2.eepic
\caption{Illustration of convergence.
On top, the first ten points of the sequence as a graph
with $M$ and the interval around the limit $x$ marked.
On bottom, the points of the same sequence marked on the
number line.\label{figsequenceconvergence}}
\end{center}
\end{figure}

\begin{exercise}
Let $\{ x_n \}$ be a sequence and $x \in \R$.
Suppose for any $\epsilon > 0$, there is an $M$ such that for
all $n \geq M$, $\abs{x_n-x} \leq \epsilon$.
Show that $\lim\, x_n = x$.
\end{exercise}

Intuitively it means that eventually every number in the sequence is close to the number $x$ --- we get arbitrarily close to the limit, provided we go far enough in the sequence.
It does not mean we ever reach the limit; it is possible,
and quite common, that $x_n\ne x$ for any $n$.

We illustrate the concept in \figureref{figsequenceconvergence}.
In the
figure we first think of the sequence as a graph, as it is a function of
$\N$.
 Secondly we also plot it as a sequence of labeled points on the real
line.

When we write $\lim\, x_n = x$ for some real number $x$, we are saying two things:
first, that $\{ x_n \}$ is convergent, so the limit is defined, 
and second that the limit is $x$.

The above definition is one of the most important definitions in analysis, and it is necessary to understand it perfectly.
The key point in the definition is that given \emph{any} $\epsilon > 0$, we can find an $M$.
The $M$ can depend on $\epsilon$, so we only pick an $M$ once we know $\epsilon$.

\begin{example}
The constant sequence $1,1,1,1,\ldots$ is convergent and the limit is 1.
For
every $\epsilon > 0$, we pick $M = 1$.
\end{example}

\begin{example}
Claim: The sequence $\{ \nicefrac{1}{n} \}$ is convergent and
\begin{equation*}
\lim_{n\to \infty} \frac{1}{n} = 0 .
\end{equation*}
\begin{proof} Given an $\epsilon > 0$, we find an $M \in \N$ such that
$0 < \nicefrac{1}{M} < \epsilon$
(\hyperref[thm:arch:i]{Archimedean property} at work).
Then for all $n \geq M$ we have that
\begin{equation*}
\abs{x_n - 0} = \abs{\frac{1}{n}} = \frac{1}{n} \leq \frac{1}{M} < \epsilon .
\end{equation*}
\end{proof}
\end{example}

\begin{example}
The sequence $\{ {(-1)}^n \}$ is divergent.
\begin{proof} If there
were a limit $x$, then for $\epsilon = \frac{1}{2}$ we expect an $M$ that
satisfies the definition.
Suppose
such an $M$ exists, then for an even $n \geq M$ we compute
\begin{equation*}
\nicefrac{1}{2} > \abs{x_n - x}  = \abs{1 - x}
\qquad \text{and} \qquad
\nicefrac{1}{2} > \abs{x_{n+1} - x}  = \abs{-1 - x} .
\end{equation*}
But
\begin{equation*}
2 = \abs{1 - x - (-1 -x)} \leq
\abs{1 - x} + \abs{-1 -x} < \nicefrac{1}{2} + \nicefrac{1}{2} = 1 ,
\end{equation*}
and that is a contradiction.
\end{proof}
\end{example}

\begin{example}
Let us show the sequence $\left\{ \frac{n^2+1}{n^2+n} \right\}$ converges and
\begin{equation*}
\lim_{n\to\infty} \frac{n^2+1}{n^2+n} = 1 .
\end{equation*}

Given $\epsilon > 0$,
find $M \in \N$ such that $\frac{1}{M+1} < \epsilon$.
Then for any $n \geq
M$ we have
\begin{equation*}
\begin{split}
%\abs{\frac{n^2+1}{n^2+n} - 1} & =
%\abs{\frac{n^2+1 - (n^2+n)}{n^2+n}} \\
\abs{\frac{n^2+1}{n^2+n} - 1}  =
\abs{\frac{n^2+1 - (n^2+n)}{n^2+n}}
& =
\abs{\frac{1 - n}{n^2+n}} \\
& =
\frac{n-1}{n^2+n} \\
& \leq 
\frac{n}{n^2+n} 
 =
\frac{1}{n+1}  \\
& \leq \frac{1}{M+1} < \epsilon .
\end{split}
\end{equation*}
Therefore,
$\lim \frac{n^2+1}{n^2+n} = 1$.
\end{example}


\begin{prop} \label{prop:limisunique}
A convergent sequence has a unique limit.
\end{prop}

The proof of this proposition exhibits a useful technique in
analysis.
Many proofs follow the same general scheme.
We want to show a certain quantity is zero.
We write the quantity using the triangle inequality as two quantities, and we estimate each one
by arbitrarily small numbers.

\begin{proof}
Suppose $x$ and $y$ are limits of the sequence $\{ x_n \}$.

Take an arbitrary $\epsilon > 0$.
From the definition find an $M_1$ such that for all $n \geq M_1$,
$\abs{x_n-x} < \nicefrac{\epsilon}{2}$.
Similarly find an $M_2$
such that for all $n \geq M_2$ we have
$\abs{x_n-y} < \nicefrac{\epsilon}{2}$.

Take $M := \max \{M_1, M_2\}$.
Fix $n \geq M$ (so that both $n \geq M_1$ and $n \geq M_2$).
Then by the triangle inequality, we have
\begin{equation*}
\begin{split}
\abs{y-x}
& =
\abs{x_n-x - (x_n -y)} \\
& \leq
\abs{x_n-x} + \abs{x_n -y} \\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
As $\abs{y-x} < \epsilon$ for all $\epsilon > 0$, then $\abs{y-x} = 0$
and $y=x$.
Hence the limit (if it exists) is unique.
\end{proof}

\begin{prop}
A convergent sequence $\{ x_n \}$ is bounded.
\end{prop}

The sequence $\{ {(-1)}^n \}$ shows that the converse does not hold ---
a bounded sequence is not necessarily convergent.

\begin{proof}
Suppose $\{ x_n \}$ converges to $x$.
Thus there exists an $M \in \N$
such that for all $n \geq M$ we have
$\abs{x_n - x} < 1$.
Let $B_1 := \abs{x}+1$; 
note that for $n \geq M$ we have
\begin{equation*}
\begin{split}
\abs{x_n} & = \abs{x_n - x + x}
\\
& \leq \abs{x_n - x} + \abs{x}
\\
& < 1 + \abs{x} 
\\
&= B_1 .
\end{split}
\end{equation*}
The set $\{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}} \}$
is a finite set and hence let
\begin{equation*}
B_2 := \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}} \} .
\end{equation*}
Let $B := \max \{ B_1, B_2 \}$.
Then for all $n \in \N$ we have
\begin{equation*}
\abs{x_n} \leq B. \qedhere
\end{equation*}
\end{proof}


\begin{exercise}[Easy]
Let $\{ x_n \}$ be a sequence and $x \in \R$ such that
there exists a $k \in \N$ such that for all $n \geq k$,
$x_n = x$.
Prove that $\{ x_n \}$ converges to $x$.
\end{exercise}

\begin{exercise}
Is the following sequence convergent?  
If so, what is the limit?

\begin{enumerate}[a)]
\item $\{ n \}$
\item $\left\{ \dfrac{{(-1)}^n}{2n} \right\}$
\item $\{ 2^{-n} \}$
\item $\left\{ \dfrac{n}{n+1} \right\}$
\item $\left\{ \dfrac{n}{n^2+1} \right\}$
\item $\left\{ \dfrac{2^n}{n!} \right\}$
\end{enumerate}

\end{exercise}



\begin{exercise} \label{exercise:absconv}
Let $\{ x_n \}$ be a sequence.
\begin{enumerate}[a)]
\item Show that $\lim\, x_n = 0$ (that is, the limit exists and is zero)
if and only if $\lim \abs{x_n} = 0$.
\item Find an example such that $\{ \abs{x_n} \}$ converges and $\{ x_n \}$
diverges.
\end{enumerate}
\end{exercise}

\sectionnewpage
\section{Subsequences}
\subsection*{Tail of a sequence}

\begin{defn}
For a sequence $\{ x_n \}$,
the \emph{$K$-tail} (where $K \in \N$)
or just the
\emph{tail}\index{tail of a sequence} of
the sequence is the sequence starting at $K+1$, usually written as
\begin{equation*}
\{ x_{n+K} \}_{n=1}^\infty
\qquad \text{or} \qquad \{ x_n \}_{n=K+1}^\infty .
\end{equation*}
\end{defn}

The main result about the tail of a sequence is the following proposition.

\begin{prop}
Let $\{ x_n \}_{n=1}^\infty$ be a sequence.
Then the following
statements are equivalent:
\begin{enumerate}[(i)]
\item \label{prop:ktail:i}
The sequence $\{ x_n \}_{n=1}^\infty$ converges.
\item \label{prop:ktail:ii}
The $K$-tail $\{ x_{n+K} \}_{n=1}^\infty$ converges for all $K \in \N$.
\item \label{prop:ktail:iii}
The $K$-tail $\{ x_{n+K} \}_{n=1}^\infty$ converges for some $K \in \N$.
\end{enumerate}
Furthermore, if any (and hence all) of the limits exist, then for any $K \in \N$
\begin{equation*}
\lim_{n\to \infty} x_n = \lim_{n \to \infty} x_{n+K} .
\end{equation*}
\end{prop}

\begin{proof}
It is clear that
\ref{prop:ktail:ii} implies \ref{prop:ktail:iii}.
We will therefore show first that
\ref{prop:ktail:i}
implies
\ref{prop:ktail:ii},
and then we will show that
\ref{prop:ktail:iii}
implies
\ref{prop:ktail:i}.
In the process we will also show that the limits are equal.

Let us start with \ref{prop:ktail:i} implies \ref{prop:ktail:ii}.
Suppose $\{x_n \}$ converges to some $x \in \R$.
Let $K \in \N$ be arbitrary.
Define $y_n := x_{n+K}$, we wish to show that $\{ y_n \}$ converges
to $x$.
That is, given an $\epsilon > 0$, there exists an $M \in \N$ such that
$\abs{x-x_n} < \epsilon$ for all $n \geq M$.
Note that $n \geq M$ implies $n+K \geq M$.
Therefore, it is true that for
all $n \geq M$ we have that 
\begin{equation*}
\abs{x-y_n} = \abs{x-x_{n+K}} < \epsilon .
\end{equation*}
Therefore $\{ y_n \}$ converges to $x$.

Let us move to \ref{prop:ktail:iii} implies \ref{prop:ktail:i}.
Let $K \in \N$ be given, define
$y_n := x_{n+K}$, and suppose that $\{ y_n \}$ converges $x \in \R$.
That is, given an $\epsilon > 0$, there exists an $M' \in \N$ such that
$\abs{x-y_n} < \epsilon$ for all $n \geq M'$.
Let $M := M'+K$.
Then $n \geq M$ implies $n-K \geq M'$.
Thus, whenever $n \geq M$ we have
\begin{equation*}
\abs{x-x_n} = \abs{x-y_{n-K}} < \epsilon.
\end{equation*}
Therefore $\{ x_n \}$ converges to $x$.
\end{proof}

Essentially, the limit does not care about how the sequence begins, it only
cares about the tail of the sequence.
That is, the beginning of the sequence
may be arbitrary.

For example, the sequence defined by $x_n := \frac{n}{n^2+16}$ is decreasing
if we start at $n=4$ (it is increasing before).
That is,
$\{ x_n \} =
\nicefrac{1}{17},
\nicefrac{1}{10},
\nicefrac{3}{25},
\nicefrac{1}{8},
\nicefrac{5}{41},
\nicefrac{3}{26},
\nicefrac{7}{65},
\nicefrac{1}{10},
\nicefrac{9}{97},
\nicefrac{5}{58},\ldots$, and 
\begin{equation*}
\nicefrac{1}{17} <
\nicefrac{1}{10} <
\nicefrac{3}{25} <
\nicefrac{1}{8} >
\nicefrac{5}{41} >
\nicefrac{3}{26} >
\nicefrac{7}{65} >
\nicefrac{1}{10} >
\nicefrac{9}{97} >
\nicefrac{5}{58} > \ldots .
\end{equation*}
That is if we throw away the first 3 terms
and look at the 3 tail it is decreasing.
The proof is left as an exercise.
Since the 3-tail
is monotone and bounded below by zero, it is convergent, and therefore the sequence is convergent.

\subsection*{Subsequences}

A very useful concept related to sequences is that of a subsequence.
A subsequence of $\{ x_n \}$ is a sequence that contains
only some of the numbers from $\{ x_n \}$ in the same order.

\begin{defn}
Let $\{ x_n \}$ be a sequence.
Let $\{ n_i \}$ be a strictly increasing sequence of natural
numbers (that is $n_1 < n_2 < n_3 < \cdots$).  
The sequence
\begin{equation*}
\{ x_{n_i} \}_{i=1}^\infty
\end{equation*}
is called 
a \emph{\myindex{subsequence}} of $\{ x_n \}$.
\end{defn}

For example, take the sequence $\{ \nicefrac{1}{n} \}$.
The sequence
$\{ \nicefrac{1}{3n} \}$ is a subsequence.
To see how these two
sequences fit in the definition, take $n_i := 3i$.  
The numbers in the
subsequence must come from the original sequence, so $1,0,\nicefrac{1}{3},0,
\nicefrac{1}{5},\ldots$
is not a subsequence of $\{ \nicefrac{1}{n} \}$.
Similarly order
must be preserved, so
the sequence $1,\nicefrac{1}{3},\nicefrac{1}{2},\nicefrac{1}{5},\ldots$
is not a subsequence of $\{ \nicefrac{1}{n} \}$.

A tail of a sequence is one special type of a subsequence.
For an arbitrary
subsequence, we have the following proposition about convergence.

\begin{prop} \label{prop:seqtosubseq}
If $\{ x_n \}$ is a convergent sequence,
then any subsequence $\{ x_{n_i} \}$ is also convergent and
\begin{equation*}
\lim_{n\to \infty} x_n = 
\lim_{i\to \infty} x_{n_i} .
\end{equation*}
\end{prop}

\begin{proof}
Suppose $\lim_{n\to \infty} x_n = x$.
That means that for every
$\epsilon > 0$ we have an $M \in \N$ such that for all $n \geq M$
\begin{equation*}
\abs{x_n - x} < \epsilon .
\end{equation*}
It is not hard to prove (do it!) by \hyperref[induction:thm]{induction} that
$n_i \geq i$.
Hence $i \geq M$ implies $n_i \geq M$.
Thus,
for all $i \geq M$ we have
\begin{equation*}
\abs{x_{n_i} - x} < \epsilon ,
\end{equation*}
and we are done.
\end{proof}

\begin{example}
Existence of a convergent subsequence does not imply
convergence of the sequence itself.
Take the sequence $0,1,0,1,0,1,\ldots$.
That is,
$x_n = 0$ if $n$ is odd, and $x_n = 1$ if $n$ is even.
The sequence
$\{ x_n \}$ is divergent, however, the subsequence
$\{ x_{2n} \}$ converges to 1 and the subsequence
$\{ x_{2n+1} \}$ converges to 0.
Compare \thmref{seqconvsubseqconv:thm}.
\end{example}

\begin{exercise}
Find a convergent subsequence of the sequence
$\{ {(-1)}^n \}$.
\end{exercise}

\begin{exercise}
Let $\{x_n\}$ be a sequence defined by
\begin{equation*}
x_n := 
\begin{cases}
n & \text{if $n$ is odd} , \\
\nicefrac{1}{n} & \text{if $n$ is even} .
\end{cases}
\end{equation*}
\begin{enumerate}[a)]
\item Is the sequence bounded? (prove or disprove)
\item Is there a convergent subsequence?
  If so, find it.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence and
define a sequence $\{ y_n \}$ by
$y_{2k} := x_{k^2}$ and $y_{2k-1} = x_k$ for all $k \in \N$.
Prove that $\{ x_n \}$ converges if and only if $\{ y_n \}$ converges.
Furthermore, prove that if they converge, then
$\lim\, x_n = \lim\, y_n$.
\end{exercise}

\begin{exercise}
Suppose that $\{ x_n \}$ is a sequence such that
the subsequences $\{ x_{2n} \}$, $\{ x_{2n-1} \}$, and
$\{ x_{3n} \}$ all converge.
Show that $\{ x_n \}$ is convergent.
\end{exercise}

\begin{exercise}
Find a sequence $\{ x_n \}$ such that for any $y \in \R$, there exists a
subsequence $\{ x_{n_i} \}$ converging to $y$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Properties of limit}
\label{sec:factslimsseqs}

\sectionnotes{2--2.5 lectures, recursively defined sequences can safely be
skipped}

In this section we go over some basic results about the limits of
sequences.
We start by looking at how sequences interact with inequalities.

\subsection*{Limits and inequalities}

A basic lemma about limits and inequalities is the so-called squeeze lemma.
It allows us to show convergence of sequences in difficult cases
if we find two other simpler convergent sequences that 
``squeeze'' the original sequence.

\begin{lemma}[Squeeze lemma]\index{squeeze lemma} \label{squeeze:lemma}
Let $\{ a_n \}$, 
$\{ b_n \}$, and 
$\{ x_n \}$ be sequences such that
\begin{equation*}
a_n \leq x_n \leq b_n \quad \text{ for all $n \in \N$} .
\end{equation*}
Suppose $\{ a_n \}$ and $\{ b_n \}$ converge and
\begin{equation*}
\lim_{n\to \infty} a_n
=
\lim_{n\to \infty} b_n .
\end{equation*}
Then $\{ x_n \}$ converges and
\begin{equation*}
\lim_{n\to \infty} x_n
=
\lim_{n\to \infty} a_n
=
\lim_{n\to \infty} b_n .
\end{equation*}
\end{lemma}

The intuitive idea of the proof is illustrated in
\figureref{figsqueeze}.
If $x$ is the limit of $a_n$ and $b_n$, then if they are both within
$\nicefrac{\epsilon}{3}$ of $x$, then the distance between $a_n$ and $b_n$
is at most $\nicefrac{2\epsilon}{3}$.
As $x_n$ is between $a_n$ and $b_n$
it is at most $\nicefrac{2\epsilon}{3}$ from $a_n$.
Since $a_n$ is
at most $\nicefrac{\epsilon}{3}$ away from $x$, then $x_n$ must be at
most $\epsilon$ away from $x$.
Let us follow through on this intuition
rigorously.
\begin{figure}[h!t]
\begin{center}
\newcommand{\ltepsilon}{< \nicefrac{2\epsilon}{3} + \nicefrac{\epsilon}{3} =
\epsilon}
\input figsqueeze.pdf_t
\caption{Squeeze lemma proof in picture.\label{figsqueeze}}
\end{center}
\end{figure}

\begin{proof}
Let $x := \lim\, a_n = \lim\, b_n$.
Let $\epsilon > 0$ be given.

Find an $M_1$ such that for all $n \geq M_1$ we have
that $\abs{a_n-x} < \nicefrac{\epsilon}{3}$, and an $M_2$
such that for all $n \geq M_2$
we have $\abs{b_n-x} < \nicefrac{\epsilon}{3}$.
Set $M := \max \{M_1, M_2 \}$.
Suppose $n \geq M$.
We compute
\begin{equation*}
\begin{split}
\abs{x_n - a_n} = x_n-a_n & \leq b_n-a_n \\
& = \abs{b_n - x + x - a_n} \\
& \leq \abs{b_n - x} + \abs{x - a_n} \\
& < \frac{\epsilon}{3} + \frac{\epsilon}{3} = \frac{2\epsilon}{3} .
\end{split}
\end{equation*}
Armed with this information we estimate
\begin{equation*}
\begin{split}
\abs{x_n - x}
&= \abs{x_n - x + a_n - a_n}
\\
&\leq \abs{x_n - a_n} + \abs{a_n - x}
\\
& < \frac{2\epsilon}{3} +  \frac{\epsilon}{3} = \epsilon .
\end{split}
\end{equation*}
And we are done.
\end{proof}

\begin{example}
One application of
the \hyperref[squeeze:lemma]{squeeze lemma} is to compute limits of 
sequences using limits that are already known.
For example, suppose 
we have the sequence $\{ \frac{1}{n\sqrt{n}} \}$.
Since $\sqrt{n} \geq 1$ for all $n \in \N$, we have
\begin{equation*}
0 \leq \frac{1}{n\sqrt{n}} \leq \frac{1}{n}
\end{equation*}
for all $n \in \N$.
We already know $\lim \nicefrac{1}{n} = 0$. 
Hence, using
the constant sequence $\{ 0 \}$ and the sequence $\{ \nicefrac{1}{n} \}$ in the
squeeze lemma, we conclude
\begin{equation*}
\lim_{n\to\infty} \frac{1}{n\sqrt{n}} = 0 .
\end{equation*}
\end{example}

Limits also preserve inequalities.

\begin{lemma} \label{limandineq:lemma}
Let $\{ x_n \}$ and $\{ y_n \}$ be
convergent sequences and
\begin{equation*}
x_n \leq y_n ,
\end{equation*}
for all $n \in \N$.
Then
\begin{equation*}
\lim_{n\to\infty} x_n \leq
\lim_{n\to\infty} y_n .
\end{equation*}
\end{lemma}

\begin{proof}
Let $x := \lim\, x_n$ and $y := \lim\, y_n$. 
Let 
$\epsilon > 0$ be given.
Find an $M_1$ such that for all $n \geq M_1$
we have $\abs{x_n-x} < \nicefrac{\epsilon}{2}$.
Find an $M_2$ such that
for all $n \geq M_2$ we have
$\abs{y_n-y} < \nicefrac{\epsilon}{2}$.
In particular,
for some $n \geq \max\{ M_1, M_2 \}$ we have
$x-x_n < \nicefrac{\epsilon}{2}$ and
$y_n-y < \nicefrac{\epsilon}{2}$.
We add these inequalities to
obtain
\begin{equation*}
y_n-x_n+x-y < \epsilon, \qquad \text{or} \qquad
y_n-x_n < y-x+ \epsilon .
\end{equation*}
Since $x_n \leq y_n$ we have
$0 \leq y_n-x_n$ and hence $0 < y-x+ \epsilon$.
In other words
\begin{equation*}
x-y < \epsilon .
\end{equation*}
Because $\epsilon > 0$ was arbitrary we obtain
$x-y \leq 0$, as
we have seen that a nonnegative
number less than any positive $\epsilon$ is zero.
Therefore $x \leq y$.
\end{proof}

An easy corollary is
proved
using constant sequences in
\lemmaref{limandineq:lemma}.
The proof is left as an exercise.

\begin{cor} \label{limandineq:cor}
{\ }
\begin{enumerate}[(i)]
\item Let $\{ x_n \}$ be a convergent sequence such that $x_n \geq 0$,
then
\begin{equation*}
\lim_{n\to\infty} x_n \geq 0.
\end{equation*}
\item
Let $a,b \in \R$ and
let $\{ x_n \}$ be a convergent sequence such that
\begin{equation*}
a \leq x_n \leq b ,
\end{equation*}
for all $n \in \N$.
Then
\begin{equation*}
a \leq \lim_{n\to\infty} x_n \leq b.
\end{equation*}
\end{enumerate}
\end{cor}

In \lemmaref{limandineq:lemma} and \corref{limandineq:cor} we cannot simply replace
all the non-strict inequalities with
strict inequalities.
For example,
let $x_n := \nicefrac{-1}{n}$ and $y_n := \nicefrac{1}{n}$.
Then $x_n < y_n$, $x_n < 0$,
and $y_n > 0$ for all $n$.
However, these inequalities are
not preserved by the limit operation as we have
$\lim\, x_n = \lim\, y_n = 0$.
The moral of this example is that strict inequalities may become non-strict
inequalities when limits are applied; if we know
$x_n < y_n$ for all $n$,
we may only conclude 
\begin{equation*}
\lim_{n \to \infty} x_n \leq
\lim_{n \to \infty} y_n .
\end{equation*}
This issue is a common source of errors.

\subsection*{Continuity of algebraic operations}

Limits interact nicely with algebraic operations.

\begin{prop} \label{prop:contalg}
Let $\{ x_n \}$ and $\{ y_n \}$ be convergent sequences.
\begin{enumerate}[(i)]
\item \label{prop:contalg:i}
The sequence $\{ z_n \}$, where $z_n := x_n + y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n + y_n) = 
\lim_{n \to \infty} z_n = 
\lim_{n \to \infty} x_n + 
\lim_{n \to \infty} y_n .
\end{equation*}
\item \label{prop:contalg:ii}
The sequence $\{ z_n \}$, where $z_n := x_n - y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n - y_n) = 
\lim_{n \to \infty} z_n = 
\lim_{n \to \infty} x_n - 
\lim_{n \to \infty} y_n .
\end{equation*}
\item \label{prop:contalg:iii}
The sequence $\{ z_n \}$, where $z_n := x_n y_n$, converges and
\begin{equation*}
\lim_{n \to \infty} (x_n y_n) = 
\lim_{n \to \infty} z_n = 
\left( \lim_{n \to \infty} x_n \right)
\left( \lim_{n \to \infty} y_n \right) .
\end{equation*}
\item \label{prop:contalg:iv}
If $\lim\, y_n \not= 0$ and $y_n \not= 0$ for all $n \in \N$, then
the sequence $\{ z_n \}$, where $z_n := \dfrac{x_n}{y_n}$, converges and
\begin{equation*}
\lim_{n \to \infty} \frac{x_n}{y_n} = 
\lim_{n \to \infty} z_n = 
%\frac{\lim_{n \to \infty} x_n}{\lim_{n \to \infty} y_n} .
\frac{\lim\, x_n}{\lim\, y_n} .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with \ref{prop:contalg:i}.
Suppose $\{ x_n \}$ and $\{ y_n \}$ are convergent sequences and
write $z_n := x_n + y_n$.
Let $x := \lim\, x_n$,
$y := \lim\, y_n$, and $z := x+y$.

Let $\epsilon > 0$ be given.
Find an $M_1$ such that for all $n \geq M_1$
we have
$\abs{x_n - x} < \nicefrac{\epsilon}{2}$.  
Find an $M_2$ such that for all $n \geq M_2$
we have
$\abs{y_n - y} < \nicefrac{\epsilon}{2}$.
Take $M := \max \{ M_1, M_2 \}$.
For all $n \geq M$ we have
\begin{equation*}
\begin{split}
\abs{z_n - z} &=
\abs{(x_n+y_n) - (x+y)} =
\abs{x_n-x + y_n-y} \\
& \leq
\abs{x_n-x} + \abs{y_n-y} \\
& <
\frac{\epsilon}{2} +
\frac{\epsilon}{2}
= \epsilon.
\end{split}
\end{equation*}
Therefore \ref{prop:contalg:i} is proved.
Proof of \ref{prop:contalg:ii} is almost identical and is left as an
exercise.

Let us tackle 
\ref{prop:contalg:iii}.
Suppose again that $\{ x_n \}$ and $\{ y_n \}$ are convergent sequences and
write $z_n := x_n y_n$.
Let $x := \lim\, x_n$,
$y := \lim\, y_n$, and $z := xy$.

Let $\epsilon > 0$ be given.
As $\{ x_n \}$ is convergent, it is bounded.
Therefore, find
a $B >0$ such that $\abs{x_n} \leq B$ for all $n \in \N$.
Find an $M_1$ such that for all $n \geq M_1$
we have
$\abs{x_n - x} < \frac{\epsilon}{2(\abs{y}+1)}$.  
Find an $M_2$ such that for all $n \geq M_2$
we have
$\abs{y_n - y} < \frac{\epsilon}{2B}$.
Take $M := \max \{ M_1, M_2 \}$.
For all $n \geq M$ we have
\begin{equation*}
\begin{split}
\abs{z_n - z} &=
\abs{(x_ny_n) - (xy)} \\
& =
\abs{x_ny_n - (x+x_n-x_n)y} \\
& =
\abs{x_n(y_n -y) + (x_n - x)y} \\
& \leq
\abs{x_n(y_n -y)} + \abs{(x_n - x)y} \\
& =
\abs{x_n}\abs{y_n -y} + \abs{x_n - x}\abs{y} \\
& \leq
B\abs{y_n -y} + \abs{x_n - x}\abs{y} \\
& <
B\frac{\epsilon}{2B} + \frac{\epsilon}{2(\abs{y}+1)}\abs{y}
\\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}

Finally let us tackle
\ref{prop:contalg:iv}.
Instead of proving 
\ref{prop:contalg:iv} directly, we prove the following simpler claim:

\emph{Claim: If $\{ y_n \}$ is a convergent sequence such that
$\lim\, y_n \not= 0$ and $y_n \not= 0$ for all $n \in \N$, then}
\begin{equation*}
\lim_{n\to\infty} \frac{1}{y_n} = \frac{1}{\lim\, y_n}  .
\end{equation*}

Once the claim is proved, we take the sequence $\{ \nicefrac{1}{y_n} \}$,
multiply it by the sequence $\{ x_n \}$ and apply item
\ref{prop:contalg:iii}.

Proof of claim:  Let $\epsilon > 0$ be given.
Let $y := \lim\, y_n$.
Find an $M$ such that for all $n \geq M$
we have
\begin{equation*}
\abs{y_n - y} < \min \left\{ \abs{y}^2\frac{\epsilon}{2}, \, \frac{\abs{y}}{2}
\right\} .
\end{equation*}
Notice that we can make this claim as the right hand side is positive
because $\abs{y} \not= 0$.
Therefore for all $n \geq M$ we have
$\abs{y - y_n} < \frac{\abs{y}}{2}$, and so
\begin{equation*}
\abs{y} = 
\abs{y - y_n + y_n } \leq
\abs{y - y_n} + \abs{ y_n } < \frac{\abs{y}}{2} + \abs{y_n}.
\end{equation*}
Subtracting $\nicefrac{\abs{y}}{2}$ from both sides we obtain
$\nicefrac{\abs{y}}{2} < \abs{y_n}$, or in other words,
\begin{equation*}
\frac{1}{\abs{y_n}} < \frac{2}{\abs{y}} .
\end{equation*}
%or in other words $\abs{y_n} \geq \abs{y} - \abs{y - y_n}$.
%Now 
%$\abs{y_n - y} < \frac{\abs{y}}{2}$ implies 
%\begin{equation*}
%\abs{y} - \abs{y_n - y} > \frac{\abs{y}}{2} .
%\end{equation*}
%Therefore
%\begin{equation*}
%\abs{y_n} \geq \abs{y} - \abs{y - y_n} > \frac{\abs{y}}{2} ,
%\end{equation*}
%and consequently
%\begin{equation*}
%\frac{1}{\abs{y_n}} < \frac{2}{\abs{y}} .
%\end{equation*}
Now we finish the proof of the claim:
\begin{equation*}
\begin{split}
\abs{\frac{1}{y_n} - \frac{1}{y}} &=
\abs{\frac{y - y_n}{y y_n}} \\
& =
\frac{\abs{y - y_n}}{\abs{y} \abs{y_n}} \\
& <
\frac{\abs{y - y_n}}{\abs{y}} \, \frac{2}{\abs{y}} \\
& <
\frac{\abs{y}^2 \frac{\epsilon}{2}}{\abs{y}} \, \frac{2}{\abs{y}}
= \epsilon .
\end{split}
\end{equation*}
And we are done.
\end{proof}

By plugging in constant sequences, we get several easy corollaries.
If $c \in \R$ and $\{ x_n \}$ is a convergent sequence, then
for example
\begin{equation*}
\lim_{n \to \infty} c x_n = 
c \left( \lim_{n \to \infty} x_n \right) \qquad
\text{and}
\qquad
\lim_{n \to \infty} (c + x_n) = 
c + \lim_{n \to \infty} x_n .
\end{equation*}
Similarly with constant subtraction and division.

As we can take limits past multiplication we can show (exercise)
that $\lim\, x_n^k = {(\lim\, x_n)}^k$ for all $k \in \N$.
That is, we can take limits
past powers.
Let us see if we can do the same with roots.

%???$\sqrt{x}$ is not defined???

\begin{prop}
Let $\{ x_n \}$ be a convergent sequence such
that $x_n \geq 0$.
Then
\begin{equation*}
\lim_{n\to\infty} \sqrt{x_n} =
\sqrt{ \lim_{n\to\infty} x_n } .
\end{equation*}
\end{prop}

Of course to even make this statement, we need to apply
\corref{limandineq:cor} to show
that
$\lim\, x_n \geq 0$, so that we can take the square root without
worry.

\begin{proof}
Let $\{ x_n \}$ be a convergent sequence and let $x := \lim\, x_n$.

First suppose $x=0$.
Let $\epsilon > 0$ be given.
Then there is an $M$ such that for all $n \geq M$ we have
$x_n = \abs{x_n} < \epsilon^2$, or in other words $\sqrt{x_n} < \epsilon$.
Hence
\begin{equation*}
\abs{\sqrt{x_n} - \sqrt{x}} =
\sqrt{x_n} < \epsilon.
\end{equation*}

Now suppose $x > 0$ (and hence $\sqrt{x} > 0$).
\begin{equation*}
\begin{split}
\abs{\sqrt{x_n}-\sqrt{x}} &= 
\abs{\frac{x_n-x}{\sqrt{x_n}+\sqrt{x}}} \\
&=
\frac{1}{\sqrt{x_n}+\sqrt{x}}
\abs{x_n-x} \\
& \leq
\frac{1}{\sqrt{x}}
\abs{x_n-x} .
\end{split}
\end{equation*}
We leave the rest of the proof to the reader.
\end{proof}

A similar proof works for the $k$th root.
That is, we also
obtain
$\lim\, x_n^{1/k} = {( \lim\, x_n )}^{1/k}$.
We leave this to the reader
as a challenging exercise.

We may also want to take the limit past the absolute value sign.
The converse of this proposition is not true, see
\exerciseref{exercise:absconv} part b).

\begin{prop}
If $\{ x_n \}$ is a convergent sequence, then $\{ \abs{x_n} \}$
is convergent and
\begin{equation*}
\lim_{n\to\infty} \abs{x_n} = 
\abs{\lim_{n\to\infty} x_n} .
\end{equation*}
\end{prop}

\begin{proof}
We simply note the reverse triangle inequality
\begin{equation*}
\big\lvert \abs{x_n} - \abs{x} \big\rvert \leq \abs{x_n-x} .
\end{equation*}
Hence if $\abs{x_n -x}$ can be made arbitrarily small, so can
$\big\lvert \abs{x_n} - \abs{x} \big\rvert$.
Details are left to the reader.
\end{proof}

Let us see an example putting the above propositions together.  Since
we know that $\lim \nicefrac{1}{n} = 0$, then
\begin{equation*}
\lim_{n\to \infty}
\abs{\sqrt{1 + \nicefrac{1}{n}} - \nicefrac{100}{n^2}} =  
\abs{\sqrt{1 + (\lim \nicefrac{1}{n})} - 100 (\lim \nicefrac{1}{n})(\lim
\nicefrac{1}{n})} = 1.
\end{equation*}
That is, the limit on the left hand side exists because the right hand
side exists.
You really should read the above equality from right to left.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sectionnewpage
\section{Upper and lower limits}

In this section we study bounded sequences and their subsequences.
In particular we define the so-called upper limit and lower limit
of a bounded sequence and talk about limits of subsequences.
Furthermore, we prove the
Bolzano--Weierstrass theorem%
\footnote{%
Named after the Czech mathematician
\href{http://en.wikipedia.org/wiki/Bernard_Bolzano}{Bernhard Placidus Johann Nepomuk Bolzano}
(1781 -- 1848), and the German mathematician
\href{http://en.wikipedia.org/wiki/Karl_Weierstrass}{Karl Theodor Wilhelm Weierstrass}
(1815 -- 1897).}, which is an
indispensable tool in analysis.

We have seen that every convergent sequence is bounded,
although there exist many bounded divergent sequences.
For example,
the sequence $\{ {(-1)}^n \}$ is bounded,
but it is divergent.
All is not lost however and we can
still compute certain limits with a bounded divergent sequence.

\subsection*{Monotone sequences}


The simplest type of a sequence is a monotone sequence.
Checking that
a monotone sequence converges is as easy as checking that it is bounded.
It is also easy to find
the limit for a convergent
monotone sequence, provided we can find the supremum or infimum
of a countable set of numbers.

\begin{defn}
A sequence $\{ x_n \}$ is \emph{monotone increasing}\index{monotone
increasing sequence} if $x_n \leq x_{n+1}$ for all $n \in \N$.  
%
A sequence $\{ x_n \}$ is \emph{monotone decreasing}\index{monotone
decreasing sequence} if $x_n \geq x_{n+1}$ for all $n \in \N$.  
%
If a sequence is either monotone increasing or monotone decreasing, we
can simply say the sequence is \emph{monotone}\index{monotone sequence}.
Some
authors also use the word \emph{monotonic}\index{monotonic sequence}.
\end{defn}

For example, $\{ \nicefrac{1}{n} \}$ is monotone decreasing,
the constant sequence $\{ 1 \}$ is both monotone increasing and monotone
decreasing, and $\{ {(-1)}^n \}$ is not monotone.
First few terms of a sample monotone increasing sequence
are shown in 
\figureref{figsequenceincreasing}.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input sequence-increasing.eepic
\caption{First few terms of a monotone increasing sequence as a
graph.\label{figsequenceincreasing}}
\end{center}
\end{figure}

\begin{thm} \label{thm:monotoneconv}
A monotone sequence $\{ x_n \}$ is bounded if and only if it is convergent.

Furthermore, if $\{ x_n \}$ is monotone increasing and bounded, then
\begin{equation*}
\lim_{n\to \infty} x_n = \sup \{ x_n : n \in \N \} .
\end{equation*}
If $\{ x_n \}$ is monotone decreasing and bounded, then
\begin{equation*}
\lim_{n\to \infty} x_n = \inf \{ x_n : n \in \N \} .
\end{equation*}
\end{thm}

\begin{proof}
Let us suppose the sequence is monotone increasing.
Suppose 
the sequence is bounded, so there exists a $B$
such that $x_n \leq B$ for all $n$, that is the set
$\{ x_n : n \in  \N \}$ is bounded from above.
Let
\begin{equation*}
x := \sup \{ x_n : n \in \N \} .
\end{equation*}
Let $\epsilon > 0$ be arbitrary.
As $x$ is the supremum, then
there must be at least one $M \in \N$ such that $x_{M} > x-\epsilon$
(because $x$ is the supremum).
As $\{ x_n \}$ is monotone increasing,
then it is easy to see (by \hyperref[induction:thm]{induction}) that
$x_n \geq x_{M}$ for all $n \geq M$.
Hence
\begin{equation*}
\abs{x_n-x} = x-x_n \leq x-x_{M} < \epsilon  .
\end{equation*}
Therefore the sequence converges to $x$.
We already know that a convergent sequence is bounded, which completes the
other direction of the implication.

\begin{exercise}
Prove the theorem for monotone decreasing sequences.
\end{exercise}
\end{proof}

%???formally speaking we did not defined \sqrt{x}???

\begin{example}
Take the sequence $\{ \frac{1}{\sqrt{n}} \}$.

First $\frac{1}{\sqrt{n}} > 0$ for all $n \in \N$, and hence the sequence is
bounded from below.
Let us show that it is monotone decreasing.
We start with
$\sqrt{n+1} \geq \sqrt{n}$ (why is that true?).
From this inequality
we obtain
\begin{equation*}
\frac{1}{\sqrt{n+1}} \leq \frac{1}{\sqrt{n}} .
\end{equation*}
So the sequence is monotone decreasing and bounded from below (hence
bounded).
We apply the theorem to note that the sequence is
convergent and in fact
\begin{equation*}
\lim_{n\to \infty} \frac{1}{\sqrt{n}}
=
\inf \left\{ \frac{1}{\sqrt{n}} : n \in \N \right\} .
\end{equation*}
We already know that the infimum is greater than or equal to 0, as
0 is a lower bound.
Take a number $b \geq 0$ such
that $b \leq \frac{1}{\sqrt{n}}$ for all $n$.
We square both sides to
obtain
\begin{equation*}
b^2 \leq \frac{1}{n} \qquad \text{for all $n \in \N$}.
\end{equation*}
We have seen before that this implies that $b^2 \leq 0$ (a consequence
of the \hyperref[thm:arch:i]{Archimedean property}).
As we also have $b^2 \geq 0$, then $b^2 = 0$
and so $b = 0$.
Hence $b=0$ is the greatest lower bound, and $\lim \frac{1}{\sqrt{n}} = 0$.
\end{example}

\begin{example}
A word of caution:  
We must show that a monotone sequence is bounded in order to use \thmref{thm:monotoneconv}.
For example, the sequence $x_n=1 + \nicefrac{1}{2} + \cdots + \nicefrac{1}{n}$ is a monotone increasing sequence that grows very slowly.
We will see latter in \exampleref{example:harmonicseries}, that this sequence has no upper bound and so does not converge.
It is not at all obvious that this sequence has no upper bound.
\end{example}

\begin{exercise}
Show that the sequence
$\left\{ \dfrac{1}{\sqrt[3]{n}} \right\}$ is monotone, bounded, and use \thmref{thm:monotoneconv} to find the limit.
%???\sqrt[3]{n} is not yet defined
\end{exercise}

\begin{exercise}
Show that the sequence
$\left\{ \dfrac{n+1}{n} \right\}$
is monotone, bounded, and use
\thmref{thm:monotoneconv} to find the limit.
\end{exercise}

Here is a common example of where monotone sequences arise.

\begin{prop} \label{prop:supinfseq}
Let $S \subset \R$ be a nonempty bounded set.
Then there exist monotone sequences
$\{ x_n \}$ and $\{ y_n \}$ such that $x_n, y_n \in S$ and
\begin{equation*}
\sup\,S = \lim_{n\to \infty} x_n \qquad \text{and} \qquad \inf\,S =
\lim_{n\to\infty} y_n .
\end{equation*}
\end{prop}

\begin{exercise}
Prove the proposition above.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a convergent monotone sequence.
Suppose 
\begin{equation*}
\lim_{n\to \infty} x_n = x_k 
\end{equation*}
for some $k\in\N$.
Show that $x_n = x_k$ for all $n \geq k$.
\end{exercise}


\subsection*{Upper and lower limits}\label{sec:bw}

There are ways of creating monotone sequences out of any sequence, and
in this fashion we
get the so-called \emph{upper} and \emph{lower limits}, 
also known as \emph{\myindex{limit superior}} and
\emph{\myindex{limit inferior}}.
These limits always exist for bounded
sequences.

If a sequence $\{ x_n \}$ is bounded, then 
the set $\{ x_k : k \in \N \}$ is bounded.
Then for every $n$
the set $\{ x_k : k \geq n \}$ is also bounded (as it is a subset).

\begin{defn} \label{liminflimsup:def}
Let $\{ x_n \}$ be a bounded sequence.
Let
$a_n := \sup \{ x_k : k \geq n \}$ and
$b_n := \inf \{ x_k : k \geq n \}$.  
%The
%sequence $\{ a_n \}$ is bounded monotone decreasing
%and $\{ b_n \}$ is bounded monotone increasing (more on this point below).
Define
\begin{align*}
\limsup_{n \to \infty} \, x_n & := \lim_{n \to \infty} a_n ,
\\
\liminf_{n \to \infty} \, x_n & := \lim_{n \to \infty} b_n .
\end{align*}
\end{defn}

For a bounded sequence, liminf and limsup always exist (see below).  It is possible
to define liminf and limsup for unbounded sequences if we allow $\infty$
and $-\infty$.
It is not hard to generalize the following results to
include unbounded sequences, however, we first restrict our attention to
bounded ones.

\begin{prop}\label{prop:limsup}
Let $\{ x_n \}$ be a bounded sequence.
Let $a_n$ and $b_n$ be as in
the definition above.
\begin{enumerate}[(i)]
\item
The
sequence $\{ a_n \}$ is bounded monotone decreasing
and $\{ b_n \}$ is bounded monotone increasing.
In particular,
$\liminf x_n$ and $\limsup x_n$ exist.
\item
$\displaystyle \limsup_{n \to \infty} \, x_n = \inf \{ a_n : n \in \N \}$
and
$\displaystyle \liminf_{n \to \infty} \, x_n = \sup \{ b_n : n \in \N \}$.
\item
$\displaystyle \liminf_{n \to \infty} \, x_n \leq
\limsup_{n \to \infty} \, x_n$.
\end{enumerate}
\end{prop}

\begin{proof}
Sine $a_n$ is the least upper
bound for $\{ x_k : k \geq n \}$, it is also
an upper bound for the subset $\{ x_k : k \geq (n+1) \}$.
Therefore
$a_{n+1}$, the least upper bound for
$\{ x_k : k \geq (n+1) \}$, has to be less than or equal to $a_n$,
that is,
$a_n \geq a_{n+1}$ for any $n$.
That is,  $\{ a_n \}$ is a decreasing sequence.

\begin{exercise} 
Show that $b_n$ is an increasing sequence.
\end{exercise}

\begin{exercise}
Show that if $x_n$ is bounded, then $a_n$ and $b_n$ must be bounded.
\end{exercise}

The second item in the proposition follows as the sequences
$\{ a_n \}$ and $\{ b_n \}$ are monotone.

For the third item, we note that $b_n \leq a_n$, as the $\inf$ of a nonempty set
is less than or equal to its $\sup$.
We know that $\{ a_n \}$ and $\{ b_n \}$
converge to the limsup and the liminf (respectively).
We apply \lemmaref{limandineq:lemma} to obtain
\begin{equation*}
\lim_{n\to \infty} b_n \leq \lim_{n\to \infty} a_n.  \qedhere
\end{equation*}
%As $a_n$ is decreasing, then
%for every $k \leq n$ we have
%\begin{equation*}
%b_n \leq a_n \leq a_k.
%\end{equation*}
%Similarly as $b_n$ is increasing, then for every $k \geq n$ we have
%\begin{equation*}
%b_k \leq b_n \leq a_n .
%\end{equation*}
%In other words, $b_k \leq a_n$ for any pair of natural numbers $k$ and $n$.
%We have seen before that we can thus conclude that
%\begin{equation*}
%\sup \{ b_n : n \in \N \} \leq \sup \{ a_n : n \in \N \} .
%\end{equation*}
%By the first item of the proposition, we get
%\begin{equation*}
%\liminf_{n \to \infty} \, x_n \leq
%\limsup_{n \to \infty} \, x_n .
%\end{equation*}
\end{proof}

\begin{example}
Let $\{ x_n \}$ be defined by
\begin{equation*}
x_n :=
\begin{cases}
\frac{n+1}{n} & \text{ if $n$ is odd,} \\
0 & \text{ if $n$ is even.}
\end{cases}
\end{equation*}
Let us compute the $\liminf$ and $\limsup$ of this sequence.
First  the
lower limit:
\begin{equation*}
\liminf_{n\to\infty} \, x_n = 
\lim_{n\to\infty}
\left(
\inf \{ x_k : k \geq n \}
\right)
=
\lim_{n\to\infty} 0 = 0 .
\end{equation*}
For the upper limit we write
\begin{equation*}
\limsup_{n\to\infty} \, x_n = 
\lim_{n\to\infty}
\left(
\sup \{ x_k : k \geq n \}
\right) .
\end{equation*}
It is not hard to see that
\begin{equation*}
\sup \{ x_k : k \geq n \} =
\begin{cases}
\frac{n+1}{n} & \text{ if $n$ is odd,} \\
\frac{n+2}{n+1} & \text{ if $n$ is even.}
\end{cases}
\end{equation*}
We leave it to the reader to show that the limit is 1.
That is,
\begin{equation*}
\limsup_{n\to\infty} \, x_n = 1 .
\end{equation*}
Do note that the sequence $\{ x_n \}$ is not a convergent sequence.
\end{example}

\begin{exercise}
\begin{enumerate}[a)]
\item
Let $x_n := \tfrac{{(-1)}^n}{n}$, find $\limsup \, x_n$ and $\liminf \, x_n$.
\item
Let $x_n := \tfrac{(n-1){(-1)}^n}{n}$, find $\limsup \, x_n$ and $\liminf \, x_n$.
\end{enumerate}
\end{exercise}

We associate with $\limsup$ and $\liminf$ certain subsequences.

\begin{thm} \label{subseqlimsupinf:thm}
If $\{ x_n \}$ is a bounded sequence, then there exists a subsequence
$\{ x_{n_k} \}$ such that
\begin{equation*}
\lim_{k\to \infty} x_{n_k} = \limsup_{n \to \infty} \, x_n .
\end{equation*}
Similarly, there exists a (perhaps different) subsequence
$\{ x_{m_k} \}$ such that
\begin{equation*}
\lim_{k\to \infty} x_{m_k} = \liminf_{n \to \infty} \, x_n .
\end{equation*}
\end{thm}

\begin{proof}
Define $a_n := \sup \{ x_k : k \geq n \}$.
By \propref{prop:limsup}, $\limsup \, x_n$ is defined and we can write 
$x := \limsup \, x_n = \lim\, a_n$.

Define the subsequence as follows.
Pick $n_1 := 1$ and work inductively.
Suppose we have
defined $x_{n_1},\dots,x_{n_{k-1}}$.
Now pick some $m > n_{k-1}$
such that
\begin{equation*}
a_{(n_{k-1}+1)} - x_m < \nicefrac1k .
\end{equation*}
We can do this as $a_{(n_{k-1}+1)}$ is a supremum of the
set $\{ x_n : n \geq n_{k-1} + 1 \}$ and hence there are elements
of the sequence arbitrarily close (or even possibly equal) to the supremum; see \propref{prop:existsxepsfromsup}.

Set $n_k :=  m$.
The subsequence $\{ x_{n_k} \}$ is defined.

Next we need to prove that it converges and has the right limit.

Note that $n_{k-1}+1\le n_k$ for any $k$.
Since $\{a_n\}$ is monotonically decreasing, we get
\[a_{n_k}-\nicefrac1k\le a_{(n_{k-1}+1)}-\nicefrac1k\le x_{n_k}\le a_{(n_{k-1}+1)} \le a_{n_k}.\]
for any $k$.
In particular 
\[a_{n_k}-\nicefrac1k\le x_{n_k}\le a_{n_k}\]
for any $k$.

Since  $a_n\to x$ as $n\to \infty$, 
its subsequence $\{a_{n_k}\}$ also convereges to $x$.
Further, since $\nicefrac1k\to0$ as $k\to\infty$ we get that $(a_{n_k}-\nicefrac1k)\to x$ as $k\to\infty$.
By the squeeze lemma \ref{squeeze:lemma}, $x_{n_k}\to x$ as $k\to \infty$.

The second statement left as an exercise below.
\end{proof}

\begin{exercise}
Prove the statement for $\liminf$.
\end{exercise}

\subsection*{Properties of upper and lower limits}

The advantage of $\liminf$ and $\limsup$ is that we can always write them
down for any (bounded) sequence.
If we could somehow compute them, we could also compute the limit of the
sequence if it exists, or show that the sequence diverges.
Working with $\liminf$ and $\limsup$ is a
little bit like working with limits, although there are subtle differences.

\begin{thm} \label{liminfsupconv:thm}
Let $\{ x_n \}$ be a bounded sequence.
Then $\{ x_n \}$ converges
if and only if
\begin{equation*}
\liminf_{n\to \infty} \, x_n = 
\limsup_{n\to \infty} \, x_n.
\end{equation*}
Furthermore, if $\{ x_n \}$ converges, then
\begin{equation*}
\lim_{n\to \infty} x_n = 
\liminf_{n\to \infty} \, x_n = 
\limsup_{n\to \infty} \, x_n.
\end{equation*}
\end{thm}

\begin{proof}
Define $a_n$ and $b_n$ as in \defnref{liminflimsup:def}.
Note that
\begin{equation*}
b_n \leq x_n \leq a_n .
\end{equation*}
If 
$\liminf \, x_n = \limsup \, x_n$, then we know that $\{ a_n \}$ and $\{ b_n \}$
have limits and that these two limits are the same.
By the squeeze lemma
(\lemmaref{squeeze:lemma}), $\{ x_n \}$ converges and
\begin{equation*}
\lim_{n\to \infty} b_n
=
\lim_{n\to \infty} x_n
=
\lim_{n\to \infty} a_n .
\end{equation*}

Now suppose $\{ x_n \}$ converges to $x$.
We know by
\thmref{subseqlimsupinf:thm}
that there exists a subsequence $\{ x_{n_k} \}$
that converges to $\limsup \, x_n$.
As $\{ x_n \}$ converges to $x$,
every subsequence converges to $x$ and
therefore $\limsup \, x_n = \lim\, x_{n_k} = x$.
Similarly $\liminf \, x_n = x$.
\end{proof}

Upper limit and lower limit behave nicely
with subsequences.

\begin{prop} \label{prop:subseqslimsupinf}
Suppose $\{ x_n \}$ is a bounded sequence and
$\{ x_{n_k} \}$ is a subsequence.
Then
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\liminf_{k\to\infty} \, x_{n_k} \leq
\limsup_{k\to\infty} \, x_{n_k} \leq
\limsup_{n\to\infty} \, x_n .
\end{equation*}
\end{prop}

\begin{proof}
The middle inequality has been proved already.
We will prove the third
inequality, and leave the first inequality as an exercise.

We want to prove that
$\limsup \, x_{n_k} \leq \limsup \, x_n$.
Define
$a_j := \sup \{ x_k : k \geq j \}$ 
as usual.
Also define
$c_j := \sup \{ x_{n_k} : k \geq j \}$.
It is not true that $c_j$ is necessarily a subsequence of $a_j$.  However,
as $n_k \geq k$ for all $k$, we have that
$\{ x_{n_k} : k \geq j \} \subset \{ x_k : k \geq j \}$.
A supremum of a subset is less than or equal to the supremum of the
set and therefore
\begin{equation*}
c_j \leq a_j .
\end{equation*}
We apply \lemmaref{limandineq:lemma} to conclude 
\begin{equation*}
\lim_{j\to\infty} c_j \leq \lim_{j\to\infty} a_j ,
\end{equation*}
which is the desired conclusion.
\end{proof}

\begin{exercise}
Finish the proof of \propref{prop:subseqslimsupinf}.
That is,
suppose $\{ x_n \}$ is a bounded sequence and
$\{ x_{n_k} \}$ is a subsequence.
Prove
$\displaystyle \liminf_{n\to\infty}\, x_n \leq
\liminf_{k\to\infty}\, x_{n_k}$.
\end{exercise}

Upper limit and lower limit are the largest and smallest subsequential limits.
If the subsequence in the previous proposition is convergent, then we have that
\[\liminf_{k\to\infty} \, x_{n_k} = \lim_{k\to\infty}\, x_{n_k} = \limsup_{k\to\infty} \, x_{n_k}.\]  
Therefore,
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\lim_{k\to\infty} x_{n_k} \leq
\limsup_{n\to\infty} \, x_n .
\end{equation*}

Similarly we get the following useful test for convergence
of a bounded sequence.
We leave the proof as an exercise.

\begin{thm} \label{seqconvsubseqconv:thm}
A bounded sequence $\{ x_n \}$ is convergent and converges to $x$
if and only if
every convergent subsequence
$\{ x_{n_k} \}$ converges to $x$.
\end{thm}

\begin{exercise}
Prove \thmref{seqconvsubseqconv:thm}.
\end{exercise}

\subsection*{Bolzano--Weierstrass theorem}

While it is not true that a bounded sequence is convergent, the
Bolzano--Weierstrass theorem tells us that we can at least find a convergent
subsequence.
The version of Bolzano--Weierstrass 
that we present in this section is the Bolzano--Weierstrass for
sequences.

\begin{thm}[Bolzano--Weierstrass]\index{Bolzano--Weierstrass theorem}\label{thm:bwseq}
Suppose a sequence $\{ x_n \}$ of real numbers is bounded.
Then there exists a convergent subsequence $\{ x_{n_i} \}$.
\end{thm}

\begin{proof}
We use \thmref{subseqlimsupinf:thm}.
It says that there exists
a subsequence whose limit is $\limsup \, x_n$.
\end{proof}

The reader might complain right now that 
\thmref{subseqlimsupinf:thm} is strictly stronger than the
Bolzano--Weierstrass theorem as presented above.
That is true.
However, 
\thmref{subseqlimsupinf:thm} only applies to the real line, but
Bolzano--Weierstrass applies in more general contexts (that is, in $\R^n$)
with pretty much the exact same statement.

As the theorem is so important to analysis, we present an explicit
proof.
The following proof generalizes more easily to different contexts.

\begin{proof}[Alternate proof of Bolzano--Weierstrass]
As the sequence is bounded, then there exist two numbers $a_1 < b_1$
such that $a_1 \leq x_n \leq b_1$ for all $n \in \N$.

We will define a subsequence $\{ x_{n_i} \}$ and two
sequences $\{ a_i \}$ and $\{ b_i \}$, such that
$\{ a_i \}$ is monotone increasing, $\{ b_i \}$ is monotone decreasing,
$a_i \leq x_{n_i} \leq b_i$ and such that $\lim\, a_i = \lim\, b_i$.   
That $x_{n_i}$ converges follows by the \hyperref[squeeze:lemma]{squeeze lemma}.

We define the sequences inductively.
We will always have that $a_i < b_i$,
and that $x_n \in [a_i,b_i]$ for infinitely many
$n \in \N$.
We have already defined $a_1$ and $b_1$.
We take $n_1 := 1$, that is
$x_{n_1} = x_1$.

Now suppose that up to some $k \in \N$
we have defined the subsequence $x_{n_1}, x_{n_2}, \ldots,
x_{n_k}$, and the sequences $a_1,a_2,\ldots,a_k$
and $b_1,b_2,\ldots,b_k$.
Let $y := \frac{a_k+b_k}{2}$.
Clearly
$a_k < y < b_k$.
If there exist infinitely many $j \in \N$
such that $x_j \in [a_k,y]$, then set $a_{k+1} := a_k$, $b_{k+1} := y$,
and pick $n_{k+1} > n_{k}$
such that $x_{n_{k+1}} \in [a_k,y]$.
If there are not infinitely many 
$j$ such that 
$x_j \in [a_k,y]$, then it must be true that there are infinitely many $j \in
\N$ such that 
$x_j \in [y,b_k]$.
In this case pick $a_{k+1} := y$, $b_{k+1} := b_k$,
and pick $n_{k+1} > n_{k}$
such that $x_{n_{k+1}} \in [y,b_k]$.

Now we have the sequences defined.
What is left to prove is that
$\lim\, a_i = \lim\, b_i$.
Obviously the limits exist as the sequences
are monotone.
From the construction, it is obvious that
$b_i - a_i$ is cut in half in each step.
Therefore
$b_{i+1} - a_{i+1} = \frac{b_i-a_i}{2}$.
By
\hyperref[induction:thm]{induction}, we obtain that
\begin{equation*}
b_i - a_i = \frac{b_1-a_1}{2^{i-1}} .
\end{equation*}

Let $x := \lim\, a_i$.
As $\{ a_i \}$ is monotone we have that
\begin{equation*}
x = \sup \{ a_i : i \in \N \}
\end{equation*}
Now let $y := \lim\, b_i = \inf \{ b_i : i \in \N \}$.
Obviously
$y \leq x$ as $a_i < b_i$ for all $i$.
As the sequences are monotone, then
for any $i$ we have (why?)
\begin{equation*}
y-x \leq b_i-a_i = \frac{b_1-a_1}{2^{i-1}} .
\end{equation*}
As $\frac{b_1-a_1}{2^{i-1}}$ is arbitrarily small and $y-x \geq 0$,
we have that $y-x = 0$.
We finish by the \hyperref[squeeze:lemma]{squeeze lemma}.
\end{proof}

Yet another proof of the Bolzano--Weierstrass theorem is to show the
following claim,
which is left as a challenging exercise.
\emph{Claim: Every sequence has a monotone subsequence}.

\subsection*{Infinite limits}

If we allow $\liminf$ and $\limsup$ to take on
the values $\infty$ and $-\infty$, we can apply $\liminf$ and $\limsup$
to all sequences, not just
bounded ones.
For any sequence, we 
write
\begin{equation*}
\limsup \, x_n := \inf \{ a_n : n \in \N \}, \qquad \text{and} \qquad
\liminf \, x_n := \sup \{ b_n : n \in \N \},
\end{equation*}
where 
$a_n := \sup \{ x_k : k \geq n \}$ and
$b_n := \inf \{ x_k : k \geq n \}$ as before.

We also often define infinite limits for certain divergent sequences.

\begin{defn}
We say
$\{ x_n \}$ \emph{\myindex{diverges to infinity}}%
\footnote{Sometimes it is said that $\{ x_n \}$ \emph{converges to infinity}.}
if for every $M \in
\R$, there exists an $N \in \N$ such that for all $n \geq N$ we have $x_n >
M$.
In this case we write $\lim \, x_n := \infty$.
Similarly
if for every $M \in \R$ there exists an $N \in \N$ such that
for all $n \geq N$ we have $x_n < M$, we say $\{ x_n \}$
\emph{\myindex{diverges to minus infinity}} and we write
$\lim \, x_n := -\infty$.
\end{defn}

This definition behaves as expected with
$\limsup$ and $\liminf$, see Exercise \ref{exercise:infseqlimlims}.

\begin{example}
If $x_n := 0$ for odd $n$ and $x_n := n$ for even $n$
then
\begin{equation*}
\lim_{n\to \infty} n = \infty,
\qquad 
\lim_{n\to \infty} x_n \quad \text{does not exist},
\qquad 
\limsup_{n\to \infty} x_n = \infty.
\end{equation*}
\end{example}

\begin{exercise} \label{exercise:infseqlimlims}
Given a sequence $\{ x_n \}$.  
\begin{enumerate}[a)]
 \item Show that
$\lim \, x_n = \infty$ if and only if $\liminf \, x_n = \infty$.
\item Then show that $\lim \, x_n = - \infty$ if and only if $\limsup \, x_n = -\infty$.
\item If $\{ x_n \}$ is monotone increasing, show that either
$\lim \, x_n$ exists and is finite or $\lim \, x_n = \infty$.
\end{enumerate}
\end{exercise}

\subsection*{Exercises}


\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences such that
$x_n \leq y_n$ for all $n$.
Then show that
\begin{equation*}
\limsup_{n\to\infty} \, x_n \leq
\limsup_{n\to\infty} \, y_n
\end{equation*}
and
\begin{equation*}
\liminf_{n\to\infty} \, x_n \leq
\liminf_{n\to\infty} \, y_n .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences.
\begin{enumerate}[a)]
\item
Show that $\{ x_n + y_n \}$ is bounded.
\item
Show that
\begin{equation*}
(\liminf_{n\to \infty}\, x_n)
+
(\liminf_{n\to \infty}\, y_n)
\leq
\liminf_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: Find a subsequence $\{ x_{n_i}+y_{n_i} \}$ of $\{ x_n + y_n \}$
that converges.
Then find a subsequence $\{ x_{n_{m_i}} \}$ of $\{ x_{n_i} \}$ that converges.
Then apply what you know about limits.
\item
Find an explicit $\{ x_n \}$ and $\{ y_n \}$ such that
\begin{equation*}
(\liminf_{n\to \infty}\, x_n)
+
(\liminf_{n\to \infty}\, y_n)
<
\liminf_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: Look for examples that do not have a limit.
\end{enumerate}
\end{exercise}

\begin{samepage}
\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be bounded sequences (from the previous
exercise we know that $\{ x_n + y_n \}$ is bounded).
\begin{enumerate}[a)]
\item
Show that
\begin{equation*}
(\limsup_{n\to \infty}\, x_n)
+
(\limsup_{n\to \infty}\, y_n)
\geq
\limsup_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: See previous exercise.
\item
Find an explicit $\{ x_n \}$ and $\{ y_n \}$ such that
\begin{equation*}
(\limsup_{n\to \infty}\, x_n)
+
(\limsup_{n\to \infty}\, y_n)
>
\limsup_{n\to \infty}\, (x_n+y_n) .
\end{equation*}
Hint: See previous exercise.
\end{enumerate}
\end{exercise}
\end{samepage}

\begin{exercise}
If $S \subset \R$ is a set, then $x \in \R$ is a \emph{\myindex{cluster
point}}
if for every $\epsilon > 0$, the set $(x-\epsilon,x+\epsilon) \cap S
\setminus \{ x \}$ is not empty.
That is, if there are points of $S$
arbitrarily close to $x$.
For example, $S := \{ \nicefrac{1}{n} : n \in \N \}$ has a unique (only
one) cluster point $0$, but $0 \notin S$.
Prove the following version of the Bolzano--Weierstrass theorem:

\medskip

\noindent
\emph{\textbf{Theorem.} Let $S \subset \R$ be a bounded infinite set,
then there exists at least one cluster point of $S$}.

\medskip

Hint: If $S$ is infinite, then $S$ contains a countably infinite subset.
That is, there is a sequence $\{ x_n \}$ of distinct numbers in $S$.
\end{exercise}

\begin{exercise}[Challenging]
\begin{enumerate}[a)]
 \item Prove that any sequence contains a monotone subsequence.
Hint: Call $n \in \N$ a \emph{peak} if $a_m \leq a_n$ for all $m \geq n$.

There are two possibilities: either the sequence has at most finitely many
peaks,
or it has infinitely many peaks.
\item Conclude the Bolzano--Weierstrass theorem.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let us prove a stronger version of \thmref{seqconvsubseqconv:thm}.
Suppose $\{ x_n \}$ is a sequence such that every subsequence $\{
x_{n_i} \}$ has a subsequence
$\{ x_{n_{m_i}} \}$ that converges to $x$. 
\begin{enumerate}[a)]
\item First show that $\{ x_n \}$ is bounded.
\item Now show that $\{ x_n \}$ converges to $x$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\{x_n\}$ be a bounded sequence.
Consider the set $S$ of all numbers $s$ such that for any $r > s$ there exists 
an $M \in \N$ such that for all $n \geq M$ we have
$x_n < r$.

\begin{enumerate}[a)]
 \item Show that $S$ is nonempty and bounded below.
 \item Prove that \[\limsup_{n\to\infty} \, x_n =\inf \, S.\]
\end{enumerate}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Cauchy sequences}
\label{sec:cauchy}

The following definition becomes useful if we need to check for convergence without knowing the limit.

\begin{defn}
A sequence $\{ x_n \}$ is a \emph{\myindex{Cauchy sequence}}%
\footnote{%
Also known as \emph{fundamental sequence} or \emph{sequence converging in it self}. 
Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Cauchy}{Augustin-Louis Cauchy} (1789--1857).} if
for every $\epsilon > 0$ there exists an $M \in \N$ such that
for all $n \geq M$ and all $k \geq M$ we have
\begin{equation*}
\abs{x_n - x_k} < \epsilon .
\end{equation*}
\end{defn}

Intuitively what it means is that the terms of the sequence are eventually
arbitrarily close to each other.
We would expect such a sequence to be
convergent.
It turns out that is true because $\R$ has the
\hyperref[defn:lub]{least-upper-bound property}.
First, let us look at some examples.

\begin{example}
The sequence $\{ \nicefrac{1}{n} \}$ is a Cauchy sequence.

\begin{proof}  Given $\epsilon > 0$, find $M$ such that
$M > \nicefrac{2}{\epsilon}$.
Then for $n,k \geq M$
we have that $\nicefrac{1}{n} < \nicefrac{\epsilon}{2}$
and
$\nicefrac{1}{k} < \nicefrac{\epsilon}{2}$.
Therefore for $n, k \geq M$
we have
\begin{equation*}
\abs{\frac{1}{n} - \frac{1}{k}}
\leq
\abs{\frac{1}{n}} + \abs{\frac{1}{k}}
< \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
\end{equation*}
\end{proof}
\end{example}

\begin{exercise}
Prove that $\{ \frac{n^2-1}{n^2} \}$ is Cauchy using directly the definition
of Cauchy sequences.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence such that
there exists a $0 < C < 1$ such that
\begin{equation*}
\abs{x_{n+1} - x_n} \leq C \abs{x_{n}-x_{n-1}} .
\end{equation*}
Prove that $\{ x_n \}$ is Cauchy.
Hint:  You can freely use the formula (for $C \not= 1$)
\begin{equation*}
1+ C+ C^2 + \cdots + C^n = \frac{1-C^{n+1}}{1-C}.
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $\abs{x_n-x_k} \leq \nicefrac{n}{k^2}$ for all $n$ and $k$.
Show that $\{ x_n \}$ is Cauchy.
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ and $\{ y_n \}$ be sequences such
that $\lim\, y_n =0$.
Suppose that for all $k \in \N$
and
for all $m \geq k$ we have
\begin{equation*}
\abs{x_m-x_k} \leq y_k .
\end{equation*}
Show that $\{ x_n \}$ is Cauchy.
\end{exercise}

\begin{prop}
A Cauchy sequence is bounded.
\end{prop}

\begin{proof}
Suppose $\{ x_n \}$ is Cauchy.
Pick $M$ such that for all
$n,k \geq M$ we have $\abs{x_n-x_k} < 1$.
In particular, we have
that for all $n \geq M$
\begin{equation*}
\abs{x_n - x_M} < 1 .
\end{equation*}
Or by the reverse triangle inequality,
$\abs{x_n} - \abs{x_M} \leq \abs{x_n - x_M} < 1$.
Hence for $n \geq M$
we have
\begin{equation*}
\abs{x_n} < 1 + \abs{x_M}.
\end{equation*}
Let
\begin{equation*}
B := \max \{ \abs{x_1}, \abs{x_2}, \ldots, \abs{x_{M-1}}, 1+ \abs{x_M} \} .
\end{equation*}
Then $\abs{x_n} \leq B$ for all $n \in \N$.
\end{proof}

\begin{thm}
A sequence of real numbers is Cauchy if and only if it converges.
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given and
suppose $\{ x_n \}$ converges to $x$.
Then there 
exists an $M$ such that for $n \geq M$ we have
\begin{equation*}
\abs{x_n - x} < \frac{\epsilon}{2} .
\end{equation*}
Hence for $n \geq M$ and $k \geq M$ we have
\begin{equation*}
\abs{x_n - x_k} = 
\abs{x_n - x + x - x_k}
\leq \abs{x_n-x} + \abs{x-x_k} < \frac{\epsilon}{2} + \frac{\epsilon}{2} =
\epsilon .
\end{equation*}

Alright, that direction was easy.
Now suppose $\{ x_n \}$ is Cauchy.
We have shown that $\{ x_n \}$ is bounded.
If we show that
\begin{equation*}
\liminf_{n\to \infty} \, x_n = \limsup_{n\to\infty} \, x_n ,
\end{equation*}
then $\{ x_n \}$ must be convergent by \thmref{liminfsupconv:thm}.
Assuming that liminf and limsup exist is where we use the
\hyperref[defn:lub]{least-upper-bound property}.


Define $a := \limsup \, x_n$ and
$b := \liminf \, x_n$.
By \thmref{seqconvsubseqconv:thm}, there exist subsequences
$\{ x_{n_i} \}$ and
$\{ x_{m_i} \}$, such that
\begin{equation*}
\lim_{i\to\infty} x_{n_i} = a
\qquad \text{and} \qquad
\lim_{i\to\infty} x_{m_i} = b.
\end{equation*}
Given an $\epsilon > 0$,
there exists an $M_1$ such that for all $i \geq M_1$
we have $\abs{x_{n_i} - a} < \nicefrac{\epsilon}{3}$ and
an $M_2$ such that for all $i \geq M_2$ we have
$\abs{x_{m_i} - b} < \nicefrac{\epsilon}{3}$.
There also exists an $M_3$
such that for all $n,k \geq M_3$ we have
$\abs{x_n-x_k} < \nicefrac{\epsilon}{3}$.
Let $M := \max \{ M_1, M_2, M_3 \}$.
Note that if $i \geq M$, then $n_i \geq M$ and $m_i \geq M$.
Hence
\begin{equation*}
\begin{split}
\abs{a-b} & =
\abs{a-x_{n_i}+x_{n_i}
-x_{m_i}+x_{m_i}
-b} \\
& \leq
\abs{a-x_{n_i}}
+ \abs{x_{n_i} -x_{m_i}}
+ \abs{x_{m_i} -b} \\
& <
\frac{\epsilon}{3}
+
\frac{\epsilon}{3}
+
\frac{\epsilon}{3}
= \epsilon .
\end{split}
\end{equation*}
As $\abs{a-b} < \epsilon$ for all $\epsilon > 0$, then $a=b$ and 
the sequence converges.
\end{proof}

\begin{remark}
The statement of this proposition is sometimes used to define the
completeness property of the real numbers.
We say a set is
\emph{\myindex{Cauchy-complete}} (or sometimes just \emph{\myindex{complete}})
if every Cauchy sequence converges.
Above we proved that
as $\R$ has the \hyperref[defn:lub]{least-upper-bound property}, then $\R$ is 
Cauchy-complete.

In fact one can construct $\R$ by ``completing'' $\Q$;
that is by ``throwing in'' just enough points to make all
Cauchy sequences converge (we omit the details).
The resulting field has the
least-upper-bound property.
The advantage of using Cauchy
sequences to define completeness is that this idea generalizes to
more abstract settings.
\end{remark}

It should be noted that the Cauchy criterion is stronger than just
$\abs{x_{n+1}-x_n}\to0$ as $n\to\infty$.
In fact, when we get to the partial sums of the harmonic series
(see \exampleref{example:harmonicseries} in the next section), we will have
a sequence such that $x_{n+1}-x_n = \nicefrac{1}{n}$, yet $\{ x_n \}$ is
divergent.
In fact, for that sequence it is true
that $\lim_{n\to\infty} \abs{x_{n+j}-x_n} = 0$ for
any $j \in \N$ (confer \exerciseref{exercise:badnocauchy}).
The key point in the definition of Cauchy is that $n$ and $k$
vary independently and can be arbitrarily far apart.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Series}
\label{sec:series}

\sectionnotes{2 lectures}

A fundamental object in mathematics is that of a series.
In fact, when
foundations of analysis were being developed, the motivation was to
understand series.
Understanding series is very important in applications
of analysis.
For example, solving differential equations often includes
series, and differential equations are the basis for understanding
almost all of modern science.

\subsection*{Definition}

\begin{defn}
A sequence $\{ x_n \}$,
considered together with an other sequence $\{ s_k \}$ defined by
\begin{equation*}
s_k := \sum_{n=1}^k x_n = x_1 + x_2 + \cdots + x_k.
\end{equation*}
is called series;
it will be denoted as $\sum x_n$ to emphasize that we are working with series, not a sequence.
The numbers $s_k$ are called
\emph{\myindex{partial sums}}.
If $x := \lim\, s_k$, we write
\begin{equation*}
\sum_{n=1}^\infty x_n =  x .
\end{equation*}
In this case, we cheat a little and treat
$\sum_{n=1}^\infty x_n$ as a number.

On the other hand, if the sequence $\{ s_k \}$ diverges,
we say the series $\sum x_n$ is \emph{divergent}\index{divergent series}.
\end{defn}

In other words, the notation 
\[\sum_{n=1}^\infty x_n\]
can be used for the series $\{x_n\}$ and as a shortcut for 
\begin{equation*}
\lim_{k\to\infty} 
\sum_{n=1}^k x_n .
\end{equation*}
We should be careful to only use this equality if the limit on
the right actually exists.
That is, the right-hand side does not make
sense (the limit does not exist) if the series does not converge.

\begin{remark}
Before going further, let us remark that it is sometimes convenient to start
the series at an index different from 1.
That is, for example we can write
\begin{equation*}
\sum_{n=0}^\infty r^n = \sum_{n=1}^\infty r^{n-1} .
\end{equation*}
The left-hand side is more convenient to write.
The idea is the same as
the notation for the tail of a sequence.
\end{remark}

\begin{remark}
It is common to write the series $\sum x_n$ as
\begin{equation*}
x_1 + x_2 + x_3 + \cdots
\end{equation*}
with the understanding that the ellipsis indicates a series and
not a simple sum.
We do not use this notation as it often leads to 
mistakes in proofs.
\end{remark}

\begin{example}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2^n}
\end{equation*}
converges and the limit is 1.
That is,
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2^n} = 
\lim_{k\to\infty} \sum_{n=1}^k \frac{1}{2^n} = 
1 .
\end{equation*}

\begin{proof} First we prove the following equality
\begin{equation*}
\left( \sum_{n=1}^k \frac{1}{2^n} \right)
+ \frac{1}{2^k}
= 1 .
\end{equation*}
The equality is easy to see when $k=1$.
The proof for general $k$
follows by \hyperref[induction:thm]{induction}, which we leave to the
reader.
Let $s_k$ be the partial sum.
We write
\begin{equation*}
\abs{
1 - s_k 
}
=
\abs{
1 - 
\sum_{n=1}^k \frac{1}{2^n}
}
=
\abs{\frac{1}{2^k}} = 
\frac{1}{2^k} .
\end{equation*}
The sequence $\{ \frac{1}{2^k} \}$ and therefore $\{ \abs{1-s_k} \}$
converges to zero.
So, $\{ s_k \}$ converges to 1.
\end{proof}
\end{example}

For $-1 < r < 1$, the \emph{\myindex{geometric series}}
\begin{equation*}
\sum_{n=0}^\infty r^n
\end{equation*}
converges.
In fact,
$\sum_{n=0}^\infty r^n = \frac{1}{1-r}$.
The proof is left as an exercise
to the reader.
The proof consists of showing 
\begin{equation*}
\sum_{n=0}^{k-1} r^n = \frac{1-r^k}{1-r} ,
\end{equation*}
and then taking the limit as $k$ goes to $\infty$.

A fact we often use is the following analogue of looking at the tail of
a sequence.

\begin{prop}
Let $\sum x_n$ be a series.
Let $M \in \N$.
Then
\begin{equation*}
\sum_{n=1}^\infty x_n \quad \text{converges if and only if} \quad
\sum_{n=M}^\infty x_n \quad \text{converges.}
\end{equation*}
\end{prop}

\begin{proof}
We look at partial sums of the two series (for $k \geq M$)
\begin{equation*}
\sum_{n=1}^{k} x_n
=
\left(
\sum_{n=1}^{M-1} x_n
\right)
+
\sum_{n=M}^{k} x_n .
\end{equation*}
Note that 
$\sum_{n=1}^{M-1} x_n$ is a fixed number.
Now use
\propref{prop:contalg} to finish the proof.
\end{proof}

\subsection*{Cauchy series}

\begin{defn}
A series $\sum x_n$ is said to be \emph{Cauchy} or a
\emph{Cauchy series}\index{Cauchy series},
if the sequence of partial sums $\{ s_n \}$ is a Cauchy sequence.
\end{defn}

A sequence of real numbers converges if and only if it is
Cauchy.
Therefore a series is convergent if and only if it is Cauchy.

The series $\sum x_n$ is Cauchy if for every $\epsilon > 0$,
there exists an $M \in \N$, such that for every $n \geq M$
and $k \geq M$ we have
\begin{equation*}
\abs{ \left( \sum_{j=1}^k x_j \right) - \left( \sum_{j=1}^n x_j \right) }
< \epsilon .
\end{equation*}
Without loss of generality we assume $n < k$.
Then we write
\begin{equation*}
\abs{ \left( \sum_{j=1}^k x_j \right) - \left( \sum_{j=1}^n x_j \right) }
=
\abs{ \sum_{j={n+1}}^k x_j }
< \epsilon .
\end{equation*}
We have proved the following simple proposition.

\begin{prop} \label{prop:cachyser}
The series $\sum x_n$ is Cauchy if for every $\epsilon > 0$, 
there exists an $M \in \N$ such that for every $n \geq M$
and every $k > n$ we have
\begin{equation*}
\abs{ \sum_{j={n+1}}^k x_j }
< \epsilon .
\end{equation*}
\end{prop}

\subsection*{Basic properties}

%A sequence is convergent if and only if it is Cauchy, and therefore
%the same statement is true for series.
%Proposition~\ref{prop:cachyser} has the following simple consequence.

\begin{prop}
Let $\sum x_n$ be a convergent series.
Then
the sequence $\{ x_n \}$ is convergent and
\begin{equation*}
\lim_{n\to\infty} x_n = 0.
\end{equation*}
\end{prop}

\begin{proof}
Let $\epsilon > 0$ be given.
As $\sum x_n$ is convergent, it is Cauchy.
Thus we find an $M$ such that for every $n \geq M$ we have
\begin{equation*}
\epsilon > 
\abs{ \sum_{j={n+1}}^{n+1} x_j }
=
\abs{ x_{n+1} } .
\end{equation*}
Hence for every $n \geq M+1$ we have $\abs{x_{n}} < \epsilon$.
\end{proof}

So if a series converges, the terms of the series go to zero.
The implication, however, goes only one way.
Let us give an example.

\begin{example} \label{example:harmonicseries}
The series $\sum \frac{1}{n}$ diverges (despite the fact that $\lim
\frac{1}{n} = 0$).
This is the famous \emph{\myindex{harmonic series}}%
\footnote{The divergence of the harmonic series was known 
before the theory of series was made rigorous.
In fact the proof we
give is the earliest proof and was given by
\href{http://en.wikipedia.org/wiki/Oresme}{Nicole Oresme}
(1323?--1382).}.

\begin{proof} We will show that the sequence of partial sums is unbounded, and hence
cannot converge.
Write the partial sums $s_n$ for $n = 2^k$ as:
\begin{align*}
 s_1 & = 1 , \\
 s_2 & = \left( 1 \right) + \left( \frac{1}{2} \right) , \\
 s_4 & = \left( 1 \right) + \left( \frac{1}{2} \right) +
        \left( \frac{1}{3} + \frac{1}{4} \right) , \\
 s_8 & = \left( 1 \right) + \left( \frac{1}{2} \right) +
        \left( \frac{1}{3} + \frac{1}{4} \right) +
        \left( \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} \right) , \\
& ~~ \vdots \\
 s_{2^k} & = 
1 + 
\sum_{j=1}^k
\left(
\sum_{m=2^{j-1}+1}^{2^j} \frac{1}{m}
\right) .
\end{align*}
We note that $\nicefrac{1}{3} + \nicefrac{1}{4} \geq \nicefrac{1}{4} + \nicefrac{1}{4} =
\nicefrac{1}{2}$ and
$\nicefrac{1}{5} + \nicefrac{1}{6} + \nicefrac{1}{7} + \nicefrac{1}{8}
\geq \nicefrac{1}{8} + \nicefrac{1}{8} + \nicefrac{1}{8} + \nicefrac{1}{8} =
\nicefrac{1}{2}$.
More generally
\begin{equation*}
\sum_{m=2^{k-1}+1}^{2^k} \frac{1}{m}
\geq
\sum_{m=2^{k-1}+1}^{2^k} \frac{1}{2^k}
=
(2^{k-1}) \frac{1}{2^k} = \frac{1}{2} .
\end{equation*}
Therefore
\begin{equation*}
s_{2^k} = 
1 + 
\sum_{j=1}^k
\left(
\sum_{m=2^{k-1}+1}^{2^k} \frac{1}{m}
\right) 
\geq
1 + \sum_{j=1}^k \frac{1}{2} = 1 + \frac{k}{2} .
\end{equation*}
As $\{ \frac{k}{2} \}$ is unbounded by the
\hyperref[thm:arch:i]{Archimedean property}, that means that
$\{ s_{2^k} \}$ is unbounded, and therefore $\{ s_n \}$ is unbounded.
Hence $\{ s_n \}$ diverges, and consequently $\sum \frac{1}{n}$ diverges.
\end{proof}
\end{example}

Convergent series are linear.
That is, we can multiply them by constants
and add them and these operations are done term by term.

\begin{prop}[Linearity of series]\index{linearity of series}
Let $\alpha \in \R$ and $\sum x_n$ and $\sum y_n$ be
convergent series.
Then
\begin{enumerate}[(i)]
\item
$\sum \alpha x_n$ is a convergent series and
\begin{equation*}
\sum_{n=1}^\infty \alpha x_n
=
\alpha \sum_{n=1}^\infty x_n .
\end{equation*}
\item
$\sum ( x_n + y_n )$ is a convergent series and
\begin{equation*}
\sum_{n=1}^\infty ( x_n + y_n ) 
=
\left( \sum_{n=1}^\infty x_n \right)
+
\left( \sum_{n=1}^\infty y_n \right) .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
For the first item,
we simply write the $k$th partial sum
\begin{equation*}
\sum_{n=1}^k \alpha x_n
=
\alpha \left( \sum_{n=1}^k x_n \right) .
\end{equation*}
We look at the right-hand side and note that the constant multiple of
a convergent sequence
is convergent.
Hence, we simply take the limit of both sides to obtain
the result.

For the second item we also look at the
$k$th partial sum
\begin{equation*}
\sum_{n=1}^k ( x_n + y_n ) 
=
\left( \sum_{n=1}^k x_n \right)
+
\left( \sum_{n=1}^k y_n \right) .
\end{equation*}
We look at the right-hand side and note that the sum of convergent sequences
is convergent.
Hence, we simply take the limit of both sides to obtain
the proposition.
\end{proof}

Note that multiplying series is not as simple as adding, see the next
section.
It is not true, of course, that we can multiply
term by term, since that strategy does not work even for finite sums.
For example, $(a+b)(c+d) \not= ac+bd$.

\subsection*{Absolute convergence}

Since monotone sequences are easier to work with than arbitrary sequences, it
is generally easier to work with series $\sum x_n$ where $x_n \geq 0$ for
all $n$.
Then the sequence of partial sums is monotone increasing
and converges if it is bounded from above.
Let us formalize this statement as a proposition.

\begin{prop}
If $x_n \geq 0$ for all $n$, then $\sum x_n$ converges if and only if
the sequence of partial sums is bounded from above.
\end{prop}

As the limit of a monotone increasing sequence is the supremum, have the
inequality
\begin{equation*}
\sum_{n=1}^k x_n \leq
\sum_{n=1}^\infty x_n .
\end{equation*}

The following criterion often gives a convenient way to test for convergence
of a series.

\begin{defn}
A series $\sum x_n$
\emph{\myindex{converges absolutely}}\index{absolute convergence} if
the series $\sum \abs{x_n}$ converges.
If a series converges, but does not converge absolutely, we say
it is \emph{\myindex{conditionally convergent}}.
\end{defn}

\begin{prop}
If the series $\sum x_n$ converges absolutely, then it converges.
\end{prop}

\begin{proof}
A series is convergent if and only if it is Cauchy.
Hence
suppose $\sum \abs{x_n}$ is Cauchy.
That is, for every $\epsilon > 0$,
there exists an $M$ such that for all $k \geq M$ and $n > k$ we have 
\begin{equation*}
\sum_{j=k+1}^n \abs{x_j} 
=
\abs{ \sum_{j=k+1}^n \abs{x_j} }
<
\epsilon .
\end{equation*}
We apply the triangle inequality for a finite sum to obtain
\begin{equation*}
\abs{ \sum_{j=k+1}^n x_j }
\leq
\sum_{j=k+1}^n \abs{x_j}
<
\epsilon .
\end{equation*}
Hence $\sum x_n$ is Cauchy and therefore it converges.
\end{proof}

Of course, if $\sum x_n$ converges absolutely, the limits of
$\sum x_n$ and $\sum \abs{x_n}$ are different.
Computing one
does not help us compute the other.

Absolutely convergent series have many wonderful properties.
For example, absolutely convergent
series can be rearranged arbitrarily, or we can multiply such
series together easily.
Conditionally convergent series on the other hand
do not often behave as one would expect.
See the next section.

We leave as an exercise to show that
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^n}{n}
\end{equation*}
converges, although the reader should finish this section before trying.
On the other hand we proved
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n}
\end{equation*}
diverges.
Therefore 
$\sum \frac{{(-1)}^n}{n}$ is a conditionally convergent subsequence.

\subsection*{Comparison test and the \texorpdfstring{$p$}{p}-series}

We have noted above that for a series to converge
the terms not only have to go to zero, but they have to go to zero ``fast
enough.''  If we know about convergence of a certain series
we can use the following comparison test to see if the terms of another
series go to zero ``fast enough.''

\begin{prop}[Comparison test]\index{comparison test for series}
Let $\sum x_n$ and $\sum y_n$ be series such that $0 \leq x_n \leq y_n$
for all $n \in \N$.
\begin{enumerate}[(i)]
\item If $\sum y_n$ converges, then so does $\sum x_n$.
\item If $\sum x_n$ diverges, then so does $\sum y_n$.
\end{enumerate}
\end{prop}

\begin{proof}
Since the terms of the series are all nonnegative, the sequences of
partial sums are both monotone increasing.
Since $x_n \leq y_n$ for all $n$, the partial sums
satisfy for all $k$
\begin{equation} \label{comptest:eq}
\sum_{n=1}^k x_n \leq \sum_{n=1}^k y_n .
\end{equation}
If the series $\sum y_n$ converges the partial sums for the series
are bounded.
Therefore the right-hand side of \eqref{comptest:eq}
is bounded for all $k$.
Hence the partial sums for $\sum x_n$
are also bounded.
Since the partial sums are a monotone increasing sequence
they are convergent.
The first item is thus proved.

On the other hand if $\sum x_n$ diverges, the sequence of partial sums
must be unbounded since it is monotone increasing.
That is, the partial
sums for $\sum x_n$ are eventually bigger than any real number.
Putting this
together with \eqref{comptest:eq} we see that for any $B \in
\R$, there is a $k$ such that 
\begin{equation*}
B \leq \sum_{n=1}^k x_n \leq \sum_{n=1}^k y_n .
\end{equation*}
Hence the partial sums for $\sum y_n$ are also unbounded, and $\sum
y_n$ also diverges.
\end{proof}

A useful series to use with the comparison test is the
$p$-series.

\begin{prop}[$p$-series or the $p$-test]\index{p-series}\index{p-test}
For $p \in \R$, 
the series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n^p}
\end{equation*}
converges if and only if $p > 1$.
\end{prop}

\begin{proof}
First suppose $p \leq 1$.
As $n \geq 1$, we have
$\frac{1}{n^p} \geq \frac{1}{n}$.
Since
$\sum \frac{1}{n}$ diverges, we see that the 
$\sum \frac{1}{n^p}$ must diverge for all $p \leq 1$ by the comparison test.

Now suppose $p > 1$.
We proceed in a similar fashion as we did in the case of the
harmonic series, but instead of showing that the sequence
of partial sums is unbounded we show that it is bounded.
Since the terms of the series are positive, the sequence of partial sums
is monotone increasing and will converge if we show that it is bounded
above.
Let $s_n$ denote the $n$th partial sum.
\begin{align*}
 s_1 & = 1 , \\
 s_3 & = \left( 1 \right) + \left( \frac{1}{2^p} + \frac{1}{3^p} \right) , \\
 s_7 & = \left( 1 \right) + \left( \frac{1}{2^p} + \frac{1}{3^p} \right) +
        \left( \frac{1}{4^p} + \frac{1}{5^p} + \frac{1}{6^p} + \frac{1}{7^p} \right) , \\
& ~~ \vdots \\
 s_{2^k - 1} &= 
1 + 
\sum_{j=1}^{k-1}
\left(
\sum_{m=2^j}^{2^{j+1}-1} \frac{1}{m^p}
\right) .
\end{align*}
Instead of estimating from below, we estimate from above.
In particular,
as $p$ is positive, then $2^p < 3^p$, and hence
$\frac{1}{2^p} + \frac{1}{3^p} <
\frac{1}{2^p} + \frac{1}{2^p}$.
Similarly
$\frac{1}{4^p} + \frac{1}{5^p} +
\frac{1}{6^p} + \frac{1}{7^p} <
\frac{1}{4^p} + \frac{1}{4^p} +
\frac{1}{4^p} + \frac{1}{4^p}$.
Therefore
\begin{equation*}
\begin{split}
s_{2^k-1}
& =
1+
\sum_{j=1}^k
\left(
\sum_{m=2^{j}}^{2^{j+1}-1} \frac{1}{m^p}
\right) 
\\
& <
1+
\sum_{j=1}^k
\left(
\sum_{m=2^{j}}^{2^{j+1}-1} \frac{1}{{(2^j)}^p}
\right) 
\\
& =
1+
\sum_{j=1}^k
\left(
\frac{2^j}{{(2^j)}^p}
\right) 
\\
& =
1+
\sum_{j=1}^k
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{split}
\end{equation*}
As $p > 1$, then $\frac{1}{2^{p-1}} < 1$.
Then by using the result of
\exerciseref{geometric:exr}, we note that
\begin{equation*}
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j
\end{equation*}
converges.
Therefore
\begin{equation*}
s_{2^k-1} < 
1+
\sum_{j=1}^k
{\left(
\frac{1}{2^{p-1}}
\right)}^j 
\leq 
1+
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{equation*}
As $\{ s_n \}$ is a monotone sequence, then all $s_n \leq s_{2^k-1}$
for all $n \leq 2^k-1$.
Thus for all $n$,
\begin{equation*}
s_n < 
1+
\sum_{j=1}^\infty
{\left(
\frac{1}{2^{p-1}}
\right)}^j .
\end{equation*}
The sequence of partial sums is bounded and hence converges.
\end{proof}

Note that neither the $p$-series test nor the comparison test 
tell us what the sum converges to.
They only tell us that a limit
of the partial sums exists.
For example, while we know that
$\sum \nicefrac{1}{n^2}$ converges it is far harder to
find\footnote{Demonstration of this fact is
what made the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Paul Euler} (1707 -- 1783)
famous.}
that the limit is $\nicefrac{\pi^2}{6}$.
If we treat $\sum \nicefrac{1}{n^p}$ as a function of $p$,
we get the so-called Riemann $\zeta$ function.
Understanding the
behavior of this function contains
one of the most famous unsolved problems in mathematics today and has applications
in seemingly unrelated areas such as modern cryptography.

\begin{example}
The series $\sum \frac{1}{n^2+1}$ converges.

\begin{proof}  First note that $\frac{1}{n^2+1} < \frac{1}{n^2}$ for all $n \in \N$.
Note that $\sum \frac{1}{n^2}$ converges by the $p$-series test.
Therefore, by the comparison test, $\sum \frac{1}{n^2+1}$ converges.
\end{proof}
\end{example}

\subsection*{Ratio test}

\begin{prop}[Ratio test]\index{ratio test for series}
Let $\sum x_n$ be a series such that
\begin{equation*}
L := \lim_{n\to\infty} \frac{\abs{x_{n+1}}}{\abs{x_n}}
\end{equation*}
exists.
Then
\begin{enumerate}[(i)]
\item
If $L < 1$, then $\sum x_n$ converges absolutely.
\item
If $L > 1$, then $\sum x_n$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
From \lemmaref{seq:ratiotest} we note that if $L > 1$, then $x_n$
diverges.
Since it is a necessary condition for the convergence of series
that the terms go to zero, we know that $\sum x_n$ must diverge.

Thus suppose $L < 1$.
We will argue that $\sum \abs{x_n}$ must converge.
The proof is similar to that of \lemmaref{seq:ratiotest}.
Of course $L \geq
0$.  
Pick
$r$ such that $L < r < 1$.
As $r-L > 0$, there exists an $M \in \N$ such that for
all $n \geq M$
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < r-L .
\end{equation*}
Therefore,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} < r .
\end{equation*}
For $n > M$ (that is for $n \geq M+1$)
write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
<
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
For $k > M$ we write the partial sum as
\begin{equation*}
\begin{split}
\sum_{n=1}^k \abs{x_n}
& =
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
\left(\sum_{n=M+1}^{k} \abs{x_n} \right)
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
\left(\sum_{n=M+1}^{k} 
(\abs{x_M} r^{-M}) r^n
\right)
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{k} r^n \right) .
\end{split}
\end{equation*}
As $0 < r < 1$ the geometric series
$\sum_{n=0}^{\infty} r^n$ converges, so
$\sum_{n=M+1}^{\infty} r^n$ converges as well (why?).
We take the
limit as $k$ goes to infinity on the right-hand side above to obtain
\begin{equation*}
\begin{split}
\sum_{n=1}^k \abs{x_n}
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{k} r^n \right) 
\\
& \leq
\left(\sum_{n=1}^{M} \abs{x_n} \right)
+
(\abs{x_M} r^{-M})
\left( \sum_{n=M+1}^{\infty} r^n \right) .
\end{split}
\end{equation*}
The right-hand side is a number that does not depend on $n$.
Hence the sequence of partial sums of $\sum \abs{x_n}$ is bounded
and $\sum \abs{x_n}$ is convergent.
Thus $\sum x_n$ is
absolutely convergent.
\end{proof}

\begin{example}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{2^n}{n!}
\end{equation*}
converges absolutely.

\begin{proof}  We write
\begin{equation*}
\lim_{n\to\infty} \frac{2^{(n+1)}/(n+1)!}{2^n / n!} =
\lim_{n\to\infty} \frac{2}{n+1} = 0 .
\end{equation*}
Therefore, the series converges absolutely by the ratio test.
\end{proof}
\end{example}

\subsection*{Exercises}

\begin{exercise}
For $r \not= 1$, prove
\begin{equation*}
\sum_{k=0}^{n-1} r^k = \frac{1-r^n}{1-r} .
\end{equation*}
Hint:
Let $s := \sum_{k=0}^{n-1} r^k$, then
compute $s(1-r) = s-rs$, and solve for $s$.
\end{exercise}

\begin{exercise} \label{geometric:exr}
Prove that for $-1 < r < 1$ we have
\begin{equation*}
\sum_{n=0}^\infty r^n = \frac{1}{1-r} .
\end{equation*}
Hint:  Use the previous exercise.
\end{exercise}

\begin{exercise}
Decide the convergence or divergence of the following series.

\medskip

\noindent
\begin{tabular}{llllll}
a)
$\displaystyle \sum_{n=1}^\infty \frac{3}{9n+1}$
& &
b)
$\displaystyle \sum_{n=1}^\infty \frac{1}{2n-1}$
& &
c)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n^2}$
\\
d)
$\displaystyle \sum_{n=1}^\infty \frac{1}{n(n+1)}$
& &
e)
$\displaystyle \sum_{n=1}^\infty n e^{-n^2}$
\end{tabular}
\end{exercise}

\begin{exercise}
{\ }
\begin{enumerate}[a)]
\item Prove that if
$\displaystyle
\sum_{n=1}^\infty x_n
$
converges, then
$\displaystyle
\sum_{n=1}^\infty ( x_{2n} + x_{2n+1} )
$
also converges.
\item
Find an explicit example where the converse does not hold.
\end{enumerate}
\end{exercise}

\begin{exercise}
For $j=1,2,\ldots,n$, let $\{ x_{j,k} \}_{k=1}^\infty$ denote $n$
sequences.
Suppose that for each $j$
\begin{equation*}
\sum_{k=1}^\infty x_{j,k}
\end{equation*}
is convergent.
Then show
\begin{equation*}
\sum_{j=1}^n \left( \sum_{k=1}^\infty x_{j,k} \right)
=
\sum_{k=1}^\infty \left( \sum_{j=1}^n x_{j,k} \right) .
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the following stronger version of the ratio test:
Let $\sum x_n$ be a series.
\begin{enumerate}[a)]
\item
If there is an $N$ and a $\rho < 1$ such that for
all $n \geq N$ we have
$\frac{\abs{x_{n+1}}}{\abs{x_n}} < \rho$, then
the series converges absolutely. 
\item
If there is an $N$ such that for
all $n \geq N$ we have
$\frac{\abs{x_{n+1}}}{\abs{x_n}} \geq 1$, then
the series diverges. 
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenging]
Let $\{ x_n \}$ be a decreasing sequence such that $\sum x_n$ converges.
Show
that $\displaystyle \lim_{n\to\infty} n x_n = 0$.
\end{exercise}

\begin{exercise}
Show that $\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n}$ converges.
Hint: consider the sum of two subsequent entries.
\end{exercise}

\begin{exercise}
{\ }
\begin{enumerate}[a)]
\item Prove that if $\sum x_n$ and $\sum y_n$ converge absolutely, then
$\sum x_ny_n$ converges absolutely.
\item Find an explicit example where the converse does not hold.
\item Find an explicit example where all three series are absolutely convergent,
are not just finite sums,
and $(\sum x_n)(\sum y_n) \not= \sum x_ny_n$.
That is, show that series are
not multiplied term-by-term.
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove the triangle inequality for series, that is
if $\sum x_n$ converges absolutely then
\begin{equation*}
\abs{\sum_{n=1}^\infty x_n} \leq
\sum_{n=1}^\infty \abs{x_n} .
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{limit comparison test}}.
That is, prove that if
$a_n > 0$ and $b_n > 0$ for all $n$, and
\begin{equation*}
0 < \lim_{n\to\infty} \frac{a_n}{b_n} < \infty ,
\end{equation*}
then either $\sum a_n$ and $\sum b_n$ both converge of both diverge.
\end{exercise}

\begin{exercise} \label{exercise:badnocauchy}
Let $x_n = \sum_{j=1}^n \nicefrac{1}{j}$.
Show that for every $k$
we have
$\displaystyle \lim_{n\to\infty} \abs{x_{n+k}-x_n} = 0$, yet $\{ x_n \}$ is not Cauchy.
\end{exercise}

\begin{exercise}
Let $s_k$ be the $k$th partial sum of $\sum x_n$.
\begin{enumerate}[a)]
 \item Suppose that there exists a $m \in \N$ such that $\displaystyle \lim_{k\to\infty}
s_{mk}$ exists and $\lim\, x_n = 0$.
Show that $\sum x_n$ converges.
 \item Find an example where $\displaystyle \lim_{k\to\infty} s_{2k}$ exists and
$\lim\, x_n \not= 0$ (and therefore $\sum x_n$ diverges).
 \item (Challenging) Find an example where $\lim\, x_n = 0$, and there exists
a subsequence $\{ s_{k_j} \}$ such that $\displaystyle \lim_{j\to\infty} s_{k_j}$ exists,
but $\sum x_n$ still diverges
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{More on series}
\label{sec:moreonseries}

\sectionnotes{up to 2--3 lectures (optional, can safely be skipped or covered partially)}

\subsection*{Root test}

We have seen the ratio test before.
There is one more similar
test called the \emph{\myindex{root test}}.
In fact, the 
proof of this test is similar and somewhat easier.

\begin{prop}[Root test]
Let $\sum x_n$ be a series and let
\begin{equation*}
L := \limsup_{n\to\infty} \, {\abs{x_n}}^{1/n} .
\end{equation*}
Then
\begin{enumerate}[(i)]
\item If $L < 1$ then $\sum x_n$ converges absolutely.
\item If $L > 1$ then $\sum x_n$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
If $L > 1$, then there exists a subsequence $\{ x_{n_k} \}$ such that
$L = \lim_{k\to\infty} \, {\abs{x_{n_k}}}^{1/n_k}$.
Let
$r$ be such that $L > r > 1$.
There exists an $M$ such
that for all $k \geq M$, we have 
${\abs{x_{n_k}}}^{1/n_k} > r > 1$, or in other words
$\abs{x_{n_k}} > r^{n_k} > 1$.
The
subsequence 
$\{ \abs{x_{n_k}} \}$, and therefore also
$\{ \abs{x_{n}} \}$, cannot possibly converge to zero, and so the series
diverges.

Now suppose $L < 1$.
Pick $r$ such that $L < r < 1$.
By definition of limit supremum,
pick $M$ such that for all $n \geq M$ we have 
\begin{equation*}
\sup \{ {\abs{x_k}}^{1/k} : k \geq n \} < r .
\end{equation*}
Therefore, for all $n \geq M$ we have
\begin{equation*}
{\abs{x_n}}^{1/n} < r , \qquad \text{or in other words} \qquad \abs{x_n} < r^n .
\end{equation*}
Let $k > M$ and let us estimate the $k$th partial sum
\begin{equation*}
\sum_{n=1}^k \abs{x_n} = 
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\left( \sum_{n=M+1}^k \abs{x_n} \right)
\leq
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\left( \sum_{n=M+1}^k r^n \right) .
\end{equation*}
As $0 < r < 1$,
the geometric series $\sum_{n=M+1}^\infty r^n$ converges to
$\frac{r^{M+1}}{1-r}$.
As everything is positive we have
\begin{equation*}
\sum_{n=1}^k \abs{x_n} 
\leq
\left( \sum_{n=1}^M \abs{x_n} \right) + 
\frac{r^{M+1}}{1-r} .
\end{equation*}
Thus the sequence of partial sums of $\sum \abs{x_n}$ is bounded, and
so the series converges.
Therefore $\sum x_n$ converges absolutely.
\end{proof}

\subsection*{Alternating series test}

The tests we have so far only addressed absolute convergence.
The
following test gives a large supply of conditionally convergent series.

\begin{prop}[Alternating series]
Let $\{ x_n \}$ be a monotone decreasing sequence of positive real numbers such
that $\lim\, x_n = 0$.
Then
\begin{equation*}
\sum_{n=1}^\infty {(-1)}^n x_n
\end{equation*}
converges.
\end{prop}

\begin{proof}
Write $s_m := \sum_{k=1}^m {(-1)}^k x_k$ be the $m$th partial sum.
Then write
\begin{equation*}
s_{2n} =
\sum_{k=1}^{2n} {(-1)}^k x_k
=
(-x_1 + x_2) + \cdots + (-x_{2n-1} + x_{2n})
=
\sum_{k=1}^{n} (-x_{2k-1} + x_{2k}) .
\end{equation*}
The sequence $\{ x_k \}$ is decreasing and so $(-x_{2k-1}+x_{2k}) \leq 0$
for all $k$.
Therefore the subsequence $\{ s_{2n} \}$ of partial sums
is a decreasing sequence.
Similarly, $(x_{2k}-x_{2k+1}) \geq 0$, and so
\begin{equation*}
s_{2n} = - x_1 + ( x_2 - x_3 ) + \cdots + ( x_{2n-2} - x_{2n-1} ) + x_{2n}
\geq -x_1 .
\end{equation*}
The sequence $\{ s_{2n} \}$ is decreasing and bounded below, so it converges.
Let $a := \lim\, s_{2n}$.

We wish to show that $\lim\, s_m = a$ (not just for the subsequence).
Notice
\begin{equation*}
s_{2n+1} = s_{2n} + x_{2n+1} .
\end{equation*}
Given $\epsilon > 0$, pick $M$ such that $\abs{s_{2n}-a} <
\nicefrac{\epsilon}{2}$ whenever $2n \geq M$.
Since $\lim\, x_n = 0$, we also
make $M$ possibly larger
to obtain
$x_{2n+1} < \nicefrac{\epsilon}{2}$ whenever $2n \geq M$.

If $2n \geq M$, we have
$\abs{s_{2n}-a} < \nicefrac{\epsilon}{2} < \epsilon$, so we just need
to check the situation for $s_{2n+1}$:
\begin{equation*}
\abs{s_{2n+1}-a} = 
\abs{s_{2n}-a + x_{2n+1}} \leq
\abs{s_{2n}-a} + x_{2n+1} < 
\nicefrac{\epsilon}{2}+ \nicefrac{\epsilon}{2} = \epsilon .\qedhere
\end{equation*}
\end{proof}

In particular, there exist conditionally convergent series
where the absolute values of the terms go to zero arbitrarily slowly.
For example,
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^n}{n^p}
\end{equation*}
converges for arbitrarily small $p > 0$, but it does not converge
absolutely when $p \leq 1$.

\subsection*{Rearrangements}

Generally,
absolutely convergent series behave as we imagine they should.
For example,
absolutely convergent series can be summed in any order whatsoever.
Nothing
of the sort holds for conditionally convergent series
(see \exampleref{example:harmonsumanything}
and \exerciseref{exercise:seriesconvergestoanything}).

Take a series
\begin{equation*}
\sum_{n=1}^\infty x_n .
\end{equation*}
Given a bijective function $\sigma \colon \N \to \N$, the corresponding
rearrangement\index{rearrangement of a series} is the following
series:
\begin{equation*}
\sum_{k=1}^\infty x_{\sigma(k)} .
\end{equation*}
We simply sum the series in a different order.

\begin{prop}
Let $\sum x_n$ be an absolutely convergent series converging to a number
$x$.
Let $\sigma \colon \N \to \N$ be a bijection.
Then
$\sum x_{\sigma(n)}$ is absolutely convergent and converges to $x$.
\end{prop}

In other words,
a rearrangement of an absolutely convergent series converges (absolutely)
to the same number.

\begin{proof}
Let $\epsilon > 0$ be given.
Then take $M$ to be such that
\begin{equation*}
\abs{\left(\sum_{n=1}^M x_n \right) - x} < \frac{\epsilon}{2}
\qquad \text{and} \qquad
\sum_{n=M+1}^\infty \abs{x_n} < \frac{\epsilon}{2} .
\end{equation*}
As $\sigma$ is a bijection,
there exists a number $K$ such that for each
$n \leq M$, there exists $k \leq K$ such that $\sigma(k) = n$.
In other words
$\{ 1,2,\ldots,M \} \subset \sigma\bigl(\{ 1,2,\ldots,K \} \bigr)$.

Then for any $N \geq K$, let $Q := \max \sigma(\{ 1,2,\ldots,K \})$
and compute
\begin{equation*}
\begin{split}
\abs{\left( \sum_{n=1}^N x_{\sigma(n)} \right) - x}
& =
\abs{ \left( \sum_{n=1}^M x_n
+
\sum_{\substack{n=1\\\sigma(n) > M}}^N x_{\sigma(n)} \right) - x}
\\
& \leq
\abs{ \left( \sum_{n=1}^M x_n \right) - x}
+
\sum_{\substack{n=1\\\sigma(n) > M}}^N \abs{x_{\sigma(n)}}
\\
& \leq
\abs{ \left( \sum_{n=1}^M x_n \right) - x}
+
\sum_{n=M+1}^Q \abs{x_{n}}
\\
& < \nicefrac{\epsilon}{2} + \nicefrac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
So 
$\sum x_{\sigma(n)}$ converges to $x$.
To see that the convergence
is absolute, we apply the above argument to $\sum \abs{x_n}$ to show
that $\sum \abs{x_{\sigma(n)}}$ converges.
\end{proof}

\begin{example} \label{example:harmonsumanything}
Let us show that the alternating harmonic series $\sum
\frac{{(-1)}^{n+1}}{n}$, which does not converge absolutely, can be
rearranged to converge to anything.
The odd terms and the even terms both diverge to
infinity (prove this!):
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{2n-1} = \infty, \qquad \text{and} \qquad
\sum_{n=1}^\infty \frac{1}{2n} = \infty .
\end{equation*}
Let $a_n := \frac{{(-1)}^{n+1}}{n}$ for simplicity, 
let an arbitrary number $L \in \R$ be given, and set $\sigma(1) := 1$.
Suppose we have
defined $\sigma(n)$ for all $n \leq N$.
If
\begin{equation*}
\sum_{n=1}^N a_{\sigma(n)} \leq L ,
\end{equation*}
then let $\sigma(N+1) := k$ be the smallest odd $k \in \N$
that we have not used yet,
that is $\sigma(n) \not= k$ for all $n \leq N$.
Otherwise let $\sigma(N+1) := k$ 
be the smallest even $k$ that we have not yet used.

By construction $\sigma \colon \N \to \N$ is one-to-one.
It is also onto, because if we keep adding either odd (resp.\ even) terms,
eventually we will pass $L$ and switch
to the evens (resp.\ odds).
So we switch infinitely many times.

Finally, let $N$ be the $N$ where we just pass $L$ and switch.
For example suppose we have just switched from odd to even (so we start
subtracting),
and let $N' > N$ be where we first switch back from even to odd.
Then
\begin{equation*}
L + \frac{1}{\sigma(N)} \geq \sum_{n=1}^{N-1} a_{\sigma(n)}
> \sum_{n=1}^{N'-1} a_{\sigma(n)} > L- \frac{1}{\sigma(N')}.
\end{equation*}
And similarly for switching in the other direction.
Therefore,
the sum up to $N'-1$ is within $\frac{1}{\min \{ \sigma(N), \sigma(N') \}}$
of $L$.
As
we switch infinitely many times we obtain that $\sigma(N) \to \infty$
and $\sigma(N') \to \infty$, and hence
\begin{equation*}
\sum_{n=1}^\infty a_{\sigma(n)} = 
\sum_{n=1}^\infty \frac{{(-1)}^{\sigma(n)+1}}{\sigma(n)} = L .
\end{equation*}

Here is an example to illustrate the proof.
Suppose $L=1.2$, then the order
is
\begin{equation*}
1+\nicefrac{1}{3}-\nicefrac{1}{2}+\nicefrac{1}{5}+\nicefrac{1}{7}+\nicefrac{1}{9}-\nicefrac{1}{4}+\nicefrac{1}{11}+\nicefrac{1}{13}-\nicefrac{1}{6}
+\nicefrac{1}{15}+\nicefrac{1}{17}+\nicefrac{1}{19} - \nicefrac{1}{8} + \cdots .
\end{equation*}
At this point we are no more than $\nicefrac{1}{8}$ from the limit.
\end{example}

\subsection*{Multiplication of series}

As we have
already mentioned,
multiplication of series is somewhat harder than addition.
If we have that at least one of the series converges
absolutely, than we can use the following theorem.
For this result it is
convenient to start the series at 0, rather than at 1.

\begin{thm}[\myindex{Mertens' theorem}\footnote{Proved by
the German mathematician
\href{http://en.wikipedia.org/wiki/Franz_Mertens}{Franz Mertens}
(1840 -- 1927).}]
Suppose $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$ are two convergent series, converging
to $A$ and $B$ respectively.
If at least one of the series
converges absolutely, then the series $\sum_{n=0}^\infty c_n$ where
\begin{equation*}
c_n = a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0 = \sum_{j=0}^n a_j b_{n-j} ,
\end{equation*}
converges to $AB$.
\end{thm}

The series $\sum c_n$ is called the \emph{\myindex{Cauchy product}} of
$\sum a_n$ and $\sum b_n$.

\begin{proof}
Suppose $\sum a_n$ converges absolutely, and let $\epsilon > 0$ be
given.
In this proof instead of picking complicated estimates just to make
the final estimate come out as less than $\epsilon$,
let us simply obtain an estimate that depends on $\epsilon$
and can be made arbitrarily small.

Write
\begin{equation*}
A_m := \sum_{n=0}^m a_n , \qquad B_m := \sum_{n=0}^m b_n .
\end{equation*}
We rearrange the $m$th partial sum of $\sum c_n$:
\begin{equation*}
\begin{split}
\abs{\left(\sum_{n=0}^m c_n \right) - AB}
& =
\abs{\left( \sum_{n=0}^m \sum_{j=0}^n a_j b_{n-j} \right) - AB}
\\
& =
\abs{\left( \sum_{n=0}^m
  B_n a_{m-n} \right) - AB}
\\
& =
\abs{\left( \sum_{n=0}^m
  ( B_n -  B ) a_{m-n} \right)
    + B A_m - AB}
\\
& \leq
\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\abs{B}\abs{A_m - A}
\end{split}
\end{equation*}
We can surely make the second term on the right hand side go to zero.
The trick is to handle the first term.
Pick $K$ such that for all $m \geq K$ we have 
$\abs{A_m - A} < \epsilon$ and
also
$\abs{B_m - B} < \epsilon$.
Finally,
as $\sum a_n$ converges absolutely,
make sure that $K$ is large enough such that
for all $m \geq K$, % we have
%Take $K$ large enough such that for all $n \geq K$
%we have
\begin{equation*}
\sum_{n=K}^m \abs{a_n} < \epsilon .
\end{equation*}
As $\sum b_n$ converges, then
we have that
$B_{\text{max}} := \sup \{ \abs{ B_n - B } : n = 0,1,2,\ldots \}$
is finite.
Take $m \geq 2K$, then in particular $m-K+1 > K$.
So
\begin{equation*}
\begin{split}
%\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
%\right)
& =
\left(
\sum_{n=0}^{m-K}
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\left(
\sum_{n=m-K+1}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
\\
& <
\left(
\sum_{n=K}^m
\abs{a_{n}}
\right)
B_{\text{max}}
+
\left(
\sum_{n=0}^{K-1}
  \epsilon \abs{a_{n}}
\right)
\\
& <
\epsilon
B_{\text{max}}
+
\epsilon
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right) .
\end{split}
\end{equation*}
Therefore, for $m \geq 2K$ we have
\begin{equation*}
\begin{split}
\abs{\left(\sum_{n=0}^m c_n \right) - AB}
& \leq
\left(
\sum_{n=0}^m
  \abs{ B_n -  B } \abs{a_{m-n}}
\right)
+
\abs{B}\abs{A_m - A}
\\
& <
\epsilon
B_{\text{max}}
+
\epsilon
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right)
+
\abs{B}\epsilon
=
\epsilon 
\left(
B_{\text{max}}
+
\left(
\sum_{n=0}^\infty \abs{a_{n}}
\right)
+
\abs{B}
\right) .
\end{split}
\end{equation*}
The expression in the parenthesis on the right hand side
is a fixed number.
Hence,
we can make the right hand side arbitrarily small by picking a small enough
$\epsilon> 0$.
So $\sum_{n=0}^\infty c_n$ converges to $AB$.
\end{proof}

\begin{example}
If both series are only conditionally convergent, the Cauchy product series
need not even converge.
Suppose we take $a_n = b_n = {(-1)}^n \frac{1}{\sqrt{n+1}}$.
The series $\sum_{n=0}^\infty a_n = \sum_{n=0}^\infty b_n$
converges
by the alternating series test, however, it does not converge
absolutely as can be seen from the $p$-test.
Let us look
at the Cauchy product.
\begin{equation*}
c_n = 
{(-1)}^n
\left(
\frac{1}{\sqrt{n+1}} + 
\frac{1}{\sqrt{2n}} + 
\frac{1}{\sqrt{3(n-1)}} + \cdots +
%\frac{1}{\sqrt{2n}} + 
\frac{1}{\sqrt{n+1}}
\right)
=
{(-1)}^n
\sum_{j=0}^n \frac{1}{\sqrt{(j+1)(n-j+1)}} .
\end{equation*}
Therefore
\begin{equation*}
\abs{c_n} 
=
\sum_{j=0}^n \frac{1}{\sqrt{(j+1)(n-j+1)}} 
\geq
\sum_{j=0}^n \frac{1}{\sqrt{(n+1)(n+1)}} 
= 1 .
\end{equation*}
The terms do not go to zero and hence $\sum c_n$ cannot converge.
\end{example}

\subsection*{Power series}

Fix $x_0 \in \R$.
A \emph{\myindex{power series}} about $x_0$
is a series of the form
\begin{equation*}
\sum_{n=0}^\infty a_n {(x-x_0)}^n .
\end{equation*}
A power series is really a function of $x$, and
many important functions in analysis can be written
as a power series.
%That is, given $x$, $f(x)$ is given by a series of the form
%Of course if we wish to define $f \colon U \subset \R \to \R$ using
%such a series, we must have that the series converges for all $x \in U$.

We say that a power series is
\emph{convergent}\index{convergent power series} if
there is at least one $x \not= x_0$ that makes the series converge.
Note that it is trivial to see that if $x=x_0$ then the series always
converges since all terms except the first are zero.
If the series does not converge for any point $x \not= x_0$, we say that
the series is \emph{divergent}\index{divergent power series}.

\begin{example} \label{ps:expex}
The series
\begin{equation*}
\sum_{n=0}^\infty \frac{1}{n!} x^n
\end{equation*}
is absolutely convergent for all $x \in \R$.
This can be seen using the ratio test:
For any $x$ notice that
\begin{equation*}
\lim_{n \to \infty}
\frac{\bigl(1/(n+1)!\bigr) \, x^{n+1}}{(1/n!) \, x^{n}}
=
\lim_{n \to \infty}
\frac{x}{n+1}
=
0.
\end{equation*}
In fact, you may recall from calculus that this series converges to $e^x$.
\end{example}

\begin{example} \label{ps:1kex}
The series
\begin{equation*}
\sum_{n=1}^\infty \frac{1}{n} x^n
\end{equation*}
converges absolutely for all $x \in (-1,1)$ via the ratio test:
\begin{equation*}
\lim_{n \to \infty}
\abs{
\frac{\bigl(1/(n+1) \bigr) \, x^{n+1}}{(1/n) \, x^{n}}
}
=
\lim_{n \to \infty}
\abs{x} \frac{n}{n+1}
=
\abs{x} < 1 .
\end{equation*}
It converges at $x=-1$,
as
$\sum_{n=1}^\infty \frac{{(-1)}^n}{n}$ converges
by the alternating series
test.
But the power series does not converge absolutely at $x=-1$, because
$\sum_{n=1}^\infty \frac{1}{n}$ does not converge.
The series
diverges at $x=1$.
When $\abs{x} > 1$, then the series diverges via the ratio test.
\end{example}

\begin{example} \label{ps:divergeex}
The series
\begin{equation*}
\sum_{n=1}^\infty n^n x^n
\end{equation*}
diverges for all $x \not= 0$.
Let us apply the root test
\begin{equation*}
\limsup_{n\to\infty}
\,
\abs{n^n x^n}^{1/n}
=
\limsup_{n\to\infty}
\,
n \abs{x}
= \infty .
\end{equation*}
Therefore the series diverges for all $x \not= 0$.
\end{example}

In fact, convergence of power series in general always works analogously to
one of the three examples above.
%The three examples show what happens for power series in general.

\begin{prop}
Let $\sum a_n {(x-x_0)}^n$ be a power series.
If the series is convergent, then either it converges at
all $x \in \R$, or
there exists a number $\rho$, such that
the series converges absolutely on the interval
$(x_0-\rho,x_0+\rho)$ and diverges when $x < x_0-\rho$ or $x > x_0+\rho$.
\end{prop}

The number $\rho$ is called the \emph{\myindex{radius of convergence}} of the
power series.
We write $\rho = \infty$ if the series converges for
all $x$, and we write $\rho = 0$ if the series is divergent.
See \figureref{ps:convfig}.
In \exampleref{ps:1kex}
the radius of convergence is $\rho=1$.
In \exampleref{ps:expex} the radius of convergence is $\rho=\infty$,
and in \exampleref{ps:divergeex} the radius of convergence is $\rho=0$.

\begin{figure}[h!t]
\begin{center}
\input ps-conv.pdf_t
\caption{Convergence of a power series.\label{ps:convfig}}
\end{center}
\end{figure}

\begin{proof}
Write
\begin{equation*}
R := \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n} .
\end{equation*}
We use the root test to prove the proposition:
\begin{equation*}
L = \limsup_{n\to\infty} \, {\abs{a_n {(x-x_0)}^n}}^{1/n} 
=
\abs{x-x_0} \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n}
=
\abs{x-x_0} R .
\end{equation*}
In particular if $R = \infty$, then $L=\infty$ for any $x \not= x_0$, and
the series diverges by the root test.
On the
other hand if $R = 0$, then $L=0$ for any $x$, and the series 
converges absolutely for all $x$.

Suppose $0 < R < \infty$.
The series
converges absolutely if
$1 > L = R \abs{x-x_0}$,
or in other words when
\begin{equation*}
\abs{x-x_0} < \nicefrac{1}{R} .
\end{equation*}
The series diverges when
$1 < L = R \abs{x-x_0}$,
or
\begin{equation*}
\abs{x-x_0} > \nicefrac{1}{R} .
\end{equation*}
Letting $\rho = \nicefrac{1}{R}$ completes the proof.
\end{proof}

It may be useful to restate what we have learned in the proof
as a separate proposition.

\begin{prop}
Let $\sum a_n {(x-x_0)}^n$ be a power series, and let
\begin{equation*}
R := \limsup_{n\to\infty} \, {\abs{a_n}}^{1/n} .
\end{equation*}
If $R = \infty$, the power series is divergent.
If
$R=0$, then the power series converges everywhere.
 Otherwise
the radius of convergence $\rho = \nicefrac{1}{R}$.
\end{prop}

Often, radius of convergence is written as $\rho = \nicefrac{1}{R}$ in all
three cases, with
the obvious understanding of what $\rho$ should be if $R = 0$ or $R =
\infty$.

Convergent power series can be added and multiplied together, and multiplied
by constants.
The proposition has an easy proof using what we know about series
in general, and power series in particular.
We leave the proof to the reader.

\begin{prop}
Let $\sum_{n=0}^\infty a_n {(x-x_0)}^n$ and
$\sum_{n=0}^\infty b_n {(x-x_0)}^n$ be two convergent power series
with radius of convergence at least $\rho > 0$ and $\alpha \in \R$.
Then
for all $x$ such that $\abs{x-x_0} < \rho$, we have 
\begin{equation*}
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
+
\left(\sum_{n=0}^\infty b_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty (a_n+b_n) {(x-x_0)}^n ,
\end{equation*}
\begin{equation*}
\alpha
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty \alpha a_n {(x-x_0)}^n ,
\end{equation*}
and
\begin{equation*}
\left(\sum_{n=0}^\infty a_n {(x-x_0)}^n\right)
\,
\left(\sum_{n=0}^\infty b_n {(x-x_0)}^n\right)
=
\sum_{n=0}^\infty c_n {(x-x_0)}^n ,
\end{equation*}
where
$c_n = a_0b_n + a_1 b_{n-1} + \cdots + a_n b_0$.
\end{prop}

That is, after performing the algebraic operations, the
radius of convergence of the resulting series is at least $\rho$.
For all $x$ with $\abs{x-x_0} < \rho$, we have two convergent series so
their term by term addition and multiplication by constants
follows by what we learned in the last section.
For multiplication of two power series,
the series are absolutely convergent inside
the radius of convergence and that is why for those $x$
we can apply Mertens' theorem.
Note that after applying an algebraic operation the radius of convergence
could increase.
See the exercises.

Let us look at some examples of power series.
Polynomials are simply finite power series.
That is, a polynomial
is a power series where
the $a_n$ are zero for all $n$ large enough.
We expand
a polynomial as a power series about any point $x_0$ by writing
the polynomial as a polynomial in $(x-x_0)$.
For example,
$2x^2-3x+4$ as a power series around $x_0 = 1$ is
\begin{equation*}
2x^2-3x+4 = 3 + (x-1) + 2{(x-1)}^2 .
\end{equation*}

We can also expand
\emph{\myindex{rational functions}}, that is, ratios of polynomials
as power series, although we will not completely prove this fact here.
Notice that a series for a rational function only defines the function
on an interval even if the function is defined elsewhere.
For example, for
the \emph{\myindex{geometric series}} we have that for
$x \in (-1,1)$
\begin{equation*}
\frac{1}{1-x} =
\sum_{n=0}^\infty x^n .
\end{equation*}
The series diverges when $\abs{x} > 1$, even though $\frac{1}{1-x}$ is
defined for all $x \not= 1$.

We can use the geometric series together with rules for addition and
multiplication of power series to expand rational functions as power
series around $x_0$,
as long as the denominator is not zero at $x_0$.
We state without
proof that this is always possible, and we give an example of such
a computation using the geometric series.

\begin{example}
Let us expand $\frac{x}{1+2x+x^2}$ as a power series around the origin ($x_0 = 0$) and
find the radius of convergence.

Write $1+2x+x^2 = {(1+x)}^2 = {\bigl(1-(-x)\bigr)}^2$, and suppose
$\abs{x} < 1$.
Compute
\begin{equation*}
\begin{split}
\frac{x}{1+2x+x^2}
&=
x \,
{\left(
\frac{1}{1-(-x)}
\right)}^2
\\
&=
x \,
{\left( 
\sum_{n=0}^\infty {(-1)}^n x^n 
\right)}^2
\\
&=
x \,
\left(
\sum_{n=0}^\infty c_n x^n 
\right)
\\
&=
\sum_{n=0}^\infty c_n x^{n+1} ,
\end{split}
\end{equation*}
where using the formula for the product of series
we obtain, $c_0 = 1$, $c_1 = -1 -1 = -2$, $c_2 = 1+1+1 = 3$, etc\ldots.
Therefore we get that for $\abs{x} < 1$, 
\begin{equation*}
\frac{x}{1+2x+x^2}
=
\sum_{n=1}^\infty {(-1)}^{n+1} n x^n .
\end{equation*}
The radius of convergence is at least 1.
We leave it to the reader to
verify that the radius of convergence is exactly equal to 1.
%We use the ratio test
%\begin{equation*}
%\lim_{n\to\infty}
%\left\lvert \frac{a_{k+1}}{a_k} \right\rvert
%=
%\lim_{k\to\infty}
%\left\lvert \frac{{(-1)}^{k+2} (k+1)}{{(-1)}^{k+1}k} \right\rvert
%=
%\lim_{k\to\infty}
%\frac{k+1}{k}
%= 1 .
%\end{equation*}
%So the radius of convergence is actually equal to 1.
\end{example}

You can use the method of partial fractions you know from calculus.
For example, to find the power series for $\frac{x^3+x}{x^2-1}$ at 0, write
\begin{equation*}
\frac{x^3+x}{x^2-1}
=
x + \frac{1}{1+x} - \frac{1}{1-x}
=
x + \sum_{n=0}^\infty {(-1)}^n x^n - \sum_{n=0}^\infty x^n .
\end{equation*}

\subsection*{Exercises}

%This kind of sucks as an exercise ....
%\begin{exercise}
%Strengthen the root test.  Suppose that
%$\limsup_{n\to\infty} \, {\abs{x_n}}^{1/n} = 1$.
%Find an easy condition
%
%but
%that ${\abs{x_n}}^{1/n} \geq 1$ infinitely often, then show that
%the series diverges.
%\end{exercise}

\begin{exercise}
Decide the convergence or divergence of the following series.

\medskip

\noindent
\begin{tabular}{lllllll}
a)
$\displaystyle \sum_{n=1}^\infty \frac{1}{2^{2n+1}}$
& &
b)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^{n}(n-1)}{n}$
& &
c)
$\displaystyle \sum_{n=1}^\infty \frac{{(-1)}^n}{n^{1/10}}$
& &
d)
$\displaystyle \sum_{n=1}^\infty \frac{n^n}{{(n+1)}^{2n}}$
\end{tabular}
\end{exercise}

\begin{exercise}
Suppose both $\sum_{n=0}^\infty a_n$ and $\sum_{n=0}^\infty b_n$ 
converge absolutely.
Show that the product series, $\sum_{n=0}^\infty c_n$ where
$c_n = a_0 b_n + a_1 b_{n-1} + \cdots + a_n b_0$, also converges absolutely.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:seriesconvergestoanything}
Let $\sum a_n$ be conditionally convergent.
Show that given any number $x$
there exists a rearrangement of $\sum a_n$
such that the rearranged series converges to $x$.
Hint: See \exampleref{example:harmonsumanything}.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Show that the alternating harmonic series $\sum
\frac{{(-1)}^{n+1}}{n}$ has a rearrangement
such that for any $x < y$, there exists a partial sum $s_n$
of the rearranged series such that $x < s_n < y$.
  \item Show that the rearrangement you found does not converge.
See \exampleref{example:harmonsumanything}.
   \item Show that for any $x \in \R$, there exists a subsequence of
partial sums $\{ s_{n_k} \}$ of your rearrangement such that 
$\lim \, s_{n_k} = x$.
\end{enumerate}
\end{exercise}

\begin{exercise}
For the following power series, find if they are convergent or not, and
if so find their radius of convergence.

\medskip

\noindent
\begin{tabular}{llllll}
a)
$\displaystyle \sum_{n=0}^\infty 2^n x^n$
&
b) $\displaystyle \sum_{n=0}^\infty n x^n$
& 
c) 
$\displaystyle \sum_{n=0}^\infty n! \, x^n$
&
d) $\displaystyle \sum_{n=0}^\infty \frac{1}{(2k)!} {(x-10)}^n$
&
e) $\displaystyle \sum_{n=0}^\infty x^{2n}$
&
f) $\displaystyle \sum_{n=0}^\infty n! \, x^{n!}$
\end{tabular}
\end{exercise}

\begin{exercise}
Suppose $\sum a_n x^n$ converges for $x=1$.
\begin{enumerate}[a)]\item What can you say about the radius of convergence?
\item If you further know that at $x=1$ the convergence is not absolute, what can you say?
\end{enumerate}
\end{exercise}

\begin{exercise}
Expand
$\dfrac{x}{4-x^2}$ as a power series around $x_0 = 0$ and compute its radius
of convergence.
\end{exercise}%???too early???

\begin{exercise}
\begin{enumerate}[a)]
 \item Find an example where the radius of convergence of $\sum a_n x^n$ and
$\sum b_n x^n$ are 1, but the radius of convergence of
the sum of the two series is infinite.
 \item (Trickier)
Find an example where the radius of convergence of $\sum a_n x^n$ and
$\sum b_n x^n$ are 1, but the radius of convergence of
the product of the two series is infinite.
\end{enumerate}
\end{exercise}

\begin{exercise}
Figure out how to compute the radius of convergence using the ratio test.
That is, suppose $\sum a_n x^n$ is a power series and
$R := \lim \, \frac{\abs{a_{n+1}}}{\abs{a_n}}$ exists or is $\infty$.
Find the radius of convergence and prove your claim.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Prove that $\lim \, n^{1/n} = 1$.
Hint:  Write $n^{1/n} = 1+b_n$ and
note $b_n > 0$.
Then show that ${(1+b_n)}^n \geq 
\frac{n(n-1)}{2}b_n^2$ and use this to show that $\lim \, b_n = 0$.
  \item Use
the result of part a) to show that if $\sum a_n x^n$ is a convergent power series with
radius of convergence $R$, then $\sum n a_n x^n$ is also convergent with the
same radius of convergence.
\end{enumerate}
\end{exercise}

\begin{exnote}
There are different notions of summability (convergence)
of a series
than just the one we have seen.
A common one is \emph{\myindex{Ces{\`a}ro summability}}%
\footnote{Named for the Italian mathematician
\href{http://en.wikipedia.org/wiki/Ernesto_Ces\%C3\%A0ro}{Ernesto Ces{\`a}ro}
(1859 -- 1906).}.
Let $\sum a_n$ be a series
and let $s_n$ be the $n$th partial sum.
The series is said to
be Ces{\`a}ro summable to $a$ if
\begin{equation*}
a = \lim_{n\to \infty} \frac{s_1 + s_2 + \cdots + s_n}{n} .
\end{equation*}
\end{exnote}

\begin{exercise}[Challenging]
\begin{enumerate}[a)]
 \item If $\sum a_n$ is convergent to $a$ (in the usual sense), show that
$\sum a_n$ is Ces{\`a}ro summable to $a$.
  \item Show that in the sense of Ces{\`a}ro $\sum {(-1)}^n$ is summable to
$\nicefrac{1}{2}$.
   \item Let $a_n := k$ when $n = k^3$ for some $k \in \N$,
$a_n := -k$ when $n = k^3+1$ for some $k \in \N$,
otherwise
let $a_n := 0$.
Show that $\sum a_n$ diverges in the usual sense,
(partial sums are unbounded), but it is
Ces{\`a}ro summable to 0 (seems a little paradoxical at first sight).
\end{enumerate}
\end{exercise}

\begin{exercise}
Show that the monotonicity in the alternating series test
is necessary.
That is, find a sequence of positive real numbers
$\{ x_n \}$ with $\lim\, x_n = 0$ but such that
$\sum {(-1)}^n x_n$ diverges.
\end{exercise}

\chapter{Sets of real numbers}

\section{Countable sets}\label{rn:counable}


The set $A$ is called countable if it can be counted;
that is, each element of $A$ can be labeled by unique natural number. 
Here is a formal definition.

\begin{defn}\label{def:countabe-simple}
The set $A$ called \emph{\myindex{countable}} if there is a bijecection from a subset of $\N$ to $A$.

A countable infinite set is called \emph{\myindex{infinite countable}}.

If $A$ is not countable, then $A$ is said to be \emph{\myindex{uncountable}}.
\end{defn}

The following proposition follow directly from the definition.
Intuitively, it states that countable sets are ``small'' and uncountable sets are ``big''.

\begin{prop}\label{prop:countable-subset}
Assume $A$ and $B$ are two sets such that $A\subset B$.

If $B$ is countable then so is $A$; 
equivalently, 
if $A$ is uncountable then so is $B$.
\end{prop}

Note that all finite sets are countable.
The infinite countable sets admit a bijection from $\N$;
this easily follow from well ordering property.
In other words $A$ is countable if it is finite or can be written as a sequence $A=\{x_1,x_2\dots\}$;
in the later case the function $n\mapsto x_n$ gives a bijection from $\N\to A$.

The set of natural numbers is the first example of countable set.
Since set of integers can be written as 
$\Z=\{0,1,-1,2,-2,3,\dots\},$
it is also countable.
The following example is more advanced.

\begin{example}\label{example:NxN}
There is a bijection
$$\N\to \N\times \N.$$
In particular, the set of all pairs of natural numbers $\N\times \N$ is a countably infinite set.

\begin{proof}
Arrange the elements of $\N \times \N$ as follows
$$\N \times \N= \{(1,1), (1,2), (2,1), (1,3), (2,2), (3,1),\dots\}$$
That is, always write down first all the elements whose two entries sum to $k$, then write down all the elements whose entries sum to $k+1$ and so on.
Note that this list includes all the pairs.

It remains to define the needed bijection as $1\mapsto(1,1)$, $2\mapsto(1,2)$, $3\mapsto(2,1)$, and so on.
\end{proof}
\end{example}%???+PIC

\begin{exercise}
Assume $f(a,b)$ is a bijection $f\colon\N\times \N\to \N$. 
Show that 
$$h(a,b,c):=f(f(a,b),c)$$ 
defines a bijection $h\colon\N\times\N\times\N\to \N$.
\end{exercise}

This example above implies the following.

\begin{prop}\label{prop:countable-union}
Union of countable collection of countable set is countable.

That is, assume $A_1,A_2,\dots$ be finite or infinite sequence of countable sets then the union
$$A=\bigcup_n A_n$$
is countable.
\end{prop}

\begin{proof}[Informal proof]
Note that the elements of $A$ can be enumerated by $\N\times \N$.
Indeed, each set $A_m$ is countable therefore it can be written as a sequence $A=\{x_{m,n}\}$, where $n$ runs in a subset of $\N$.
The function 
defined by $(m,n)\mapsto x_{m,n}$ maps a subset of $\N\times \N$ to $A$;
it covers all the elements of $A$, possibly with repetitions.
Removing the repetition gives a bijection from a subset of $\N\times \N$ to $A$.

By \exampleref{example:NxN}, there is a bijection $\N\to \N\times\N$.
Hence the statement follows.
\end{proof}

\begin{example}\label{Q-countable}
The set of rational numbers $\Q$ is infinite countable.

\begin{proof}
Note that the set of rational numbers is a union of sets 
$$A_n=\{\dots,-\nicefrac2n,-\nicefrac1n,\nicefrac0n,\nicefrac1n,\nicefrac2n,\dots\}$$
for $n\in \N$.
It remains to apply Proposition~\ref{prop:countable-union}.
\end{proof}
\end{example}

As it stated in the next theorem, 
the set of real numbers is uncountable.
It means that countable sets of real numbers are ``small'' compared to the set of real numbers. 
In particular, from \exampleref{Q-countable}, existence of irational numbers follow. 

\begin{exercise}\label{ex:sentences}
Show that the set of all sentences in English is countable.
\end{exercise}

\begin{exercise}[Challenging]
A number $x$ is \emph{algebraic}\index{algebraic number} if $x$ is a root of a polynomial with
integer coefficients, in other words, $a_n x^n + a_{n-1} x^{n-1}  + \ldots
+ a_1 x + a_0 = 0$ where all $a_n \in \Z$. 
\begin{enumerate}[a)]
 \item Show that there are only countably many algebraic numbers.
 \item Show that there exist non-algebraic
numbers (follow in the footsteps of Cantor, use uncountability of $\R$).
\end{enumerate}
Hint: Feel free to use the fact that a polynomial of degree $n$ has finite number of real roots.
\end{exercise}

\begin{exercise}
Suppose $S$ is a set of disjoint open intervals in $\R$.
That is, 
if $(a,b) \in S$ and $(c,d) \in S$, then either $(a,b) = (c,d)$
or $(a,b) \cap (c,d) = \emptyset$.
Prove $S$ is a countable set. 
Hint: use \exampleref{Q-countable} and Theorem~\ref{thm:arch:ii}.
\end{exercise}

\begin{exercise}
Show that the set $S$ is countable if and only if one can introduce an order ``$<$'' on $S$ such that for any $s\in S$ the set $\{\,x\in S: x<s\,\}$ is finite.
\end{exercise}

\begin{thm}[Cantor]\index{Cantor's theorem}\label{thm:Cantor}
The set of real numbers is uncountable.
\end{thm}

We give a modified version of
Cantor's original proof from
1874.  

\begin{proof}
Let $X \subset \R$ be a countably infinite
subset such that for any two real numbers
$a < b$, there is an $x \in X$ such that $a < x < b$.
Were $\R$ countable,
then we could take $X = \R$.
If we show that $X$ is necessarily
a proper subset, then $X$ cannot equal $\R$, and $\R$ must be
uncountable.

As $X$ is countably infinite, 
there is a bijection from $\N$ to $X$.
Consequently, we write $X$ as
a sequence of real numbers $x_1, x_2, x_3,\ldots$, such that
each number in $X$
is given by $x_j$ for some $j \in \N$.

Let us inductively
construct two sequences of real numbers $a_1,a_2,a_3,\ldots$ and
$b_1,b_2,b_3,\ldots$.
Let
$a_1 := x_1$ and $b_1 := x_1+1$.
Note that $a_1 < b_1$ and
$x_1 \notin (a_1,b_1)$.
For $k > 1$, suppose $a_{k-1}$ and $b_{k-1}$ has been defined.
Let us also suppose $(a_{k-1},b_{k-1})$ does not contain any $x_j$
for any $j=1,\ldots,k-1$.
\begin{enumerate}[(i)]
\item Define $a_k := x_j$, where $j$ is the smallest $j \in \N$
such that $x_j \in (a_{k-1},b_{k-1})$.
Such an $x_j$ exists by our
assumption on $X$.  
\item Next, define $b_k := x_j$ where $j$ is the smallest $j \in \N$
such that $x_j \in (a_{k},b_{k-1})$.
\end{enumerate}
Notice that $a_k < b_k$ and $a_{k-1} < a_k < b_k < b_{k-1}$.
Also notice that $(a_{k},b_{k})$ does not contain $x_k$ and hence
does not contain any $x_j$ for $j=1,\ldots,k$.

Claim: $a_j < b_k$ for all $j$ and $k$ in $\N$.
Let us first
assume $j < k$.
Then $a_j < a_{j+1} < \cdots < a_{k-1} < a_k < b_k$.
Similarly for $j > k$.
The claim follows.

Let $A = \{ a_j : j \in \N \}$ and $B = \{ b_j : j \in \N \}$.
By \propref{infsupineq:prop} and the claim above we have
\begin{equation*}
\sup\, A \leq \inf\, B .
\end{equation*}
Define $y := \sup\, A$.
The number $y$ cannot be a member of $A$.
If $y = a_j$
for some $j$, then $y < a_{j+1}$, which is impossible.
Similarly $y$ cannot be a member of $B$.
Therefore,
$a_j < y$ for all $j\in \N$
and $y < b_j$ for all $j\in \N$.
In other words $y \in (a_j,b_j)$ for all $j\in \N$.

Finally we must show that $y \notin X$.
If we do so, then we will have
constructed a real number not in $X$ showing that $X$ must have been a
proper subset.
Take any $x_k \in X$.
By the above construction
$x_k \notin (a_k,b_k)$, so $x_k \not= y$ as $y \in (a_k,b_k)$.

Therefore, the sequence $x_1,x_2,\ldots$ cannot contain all elements of $\R$.
Since the sequence is arbitrary, it implies that $\R$ is uncountable.
\end{proof}

Summarizing, the sets of real numbers can be divided into two categories: 
the countable sets --- these sets might be infinite but they are  ``small'' in some sense --- and uncountable sets which are ``big'' infinite sets.
The latter includes the set of all reals $\R$ and the former the set of all rational $\Q$.

Note that the function $f(x) := \tan(x)$ is a bijective map from $(-\nicefrac{\pi}{2},\nicefrac{\pi}{2})$ to $\R$.  
Since $\R$ is uncountable, so is $(-\nicefrac{\pi}{2},\nicefrac{\pi}{2})$.  
A linear function provides a bijection between any pair of real intervals.
Since any interval contains an open interval the discussion above leads to the following.

\begin{prop}\label{prop:uncountable-intervals}
The real intervals are all uncountable.
\end{prop}

We have seen that there exist irrational numbers, that is
$\R \setminus \Q$ is nonempty. 
Now let us show that there are a lot more irrational numbers than rational
numbers.

\begin{exercise} Use Proposition~\ref{prop:uncountable-intervals} to build an other solution of \exerciseref{ex:irrationals-are-dense}.
\end{exercise}


\begin{prop}
The set of irrational numbers is uncountable
\end{prop}

\begin{proof}
The set $\R$ can be presented as a union of two sets --- the set of rationals $\Q$ and irrational numbers $\R\backslash \Q$.

Arguing by contradiction, assume $\R\backslash \Q$ is countable.
Then by Proposition~\ref{prop:countable-union} and \exampleref{Q-countable}, so is $\R$.
The later contradicts Cantor's theorem (\ref{thm:Cantor}).
\end{proof}

The following exercise is closely related to \exerciseref{ex:sentences}.

\begin{exercise}
Some real numbers can be uniquely described by a written sentence in English.
For example the number $\pi$ can be described as
``The ratio of a circle's circumference to its diameter.''

Show that there are real numbers which do not admit such a description.
\end{exercise}

\begin{exercise}
Assume $A\subset \R$ is a set such that any real number can be presented as a sum of two elements in $A$. 
Show that $A$ is uncountable.
\end{exercise}


\subsection{Density of rational numbers}

The proof of the following theorem relies on Archimedean property.

\begin{thm}\label{thm:arch:ii} \emph{($\Q$ is dense in $\R$\index{density of
rational numbers})} Any real interval contains a rational number.

Equivalently, given a real numbers $x$ and $\epsilon>0$ there is a rational number $r$ such that $\abs{x-r}<\epsilon$.
\end{thm}

\begin{proof} 
Note that it is sufficient to consider the case of bounded open inteval $(x,y)$.
That is, assuming for any $x, y \in \R$ such that
$x < y$, we need to find $r \in \Q$ such that
$x < r < y$.

Note that $y-x > 0$.
By \ref{thm:arch:i}, there exists an $n \in \N$ such that $n(y-x) > 1$,
or equivalently
\begin{equation*}
x<y-\nicefrac1n.
\end{equation*}

Consider the set of integers $A=\{k\in\N : k>nx \}$.
Notice that $A$ is bounded below therefore it has the minimal element $m=\min A$;
that is $m$ is the minimal integer such that $m>nx$.

Note that $m-1\le nx$,
or equivalently 
$$\nicefrac{m}{n}-\nicefrac1{n}\le x<\nicefrac{m}{n}.$$
Since $x<y-\nicefrac1n$, the first inequality implies 
$\nicefrac{m}{n}<y$.
That is 
$$x<\nicefrac{m}{n}<y.$$
So let $r = \nicefrac{m}{n}$.

The last statement follows since $\abs{x-r}<\epsilon$ if and only if $r$ lies in the interval $(x-\epsilon,x+\epsilon)$.
\end{proof}

\begin{exercise}
Show that any interval contains a \emph{dyadic rational number};
that is, a rational number whose denominator is a power of two
(for example, 1/2 or 3/8, but not 1/3).
\end{exercise}

\begin{exercise}\label{ex:irrationals-are-dense}
Show that for any interval $(x,y)$ contains an irrational number.
Hint: Apply Theorem~\ref{thm:arch:ii} to the interval $(x+\sqrt{2},y+\sqrt{2})$.
\end{exercise}

\sectionnewpage
\section{Decimal representation}
\label{sec:decimals}

\sectionnotes{1 lecture (optional)}

We often think of real numbers as their
\emph{\myindex{decimal representation}}.
For
a positive integer $n$, we find the digits $d_K,d_{K-1},\ldots,d_2,d_1,d_0$ for some
$K$,
where each $d_j$ is an integer between $0$ and $9$, then
\begin{equation*}
n = d_K {10}^K + d_{K-1} {10}^{K-1} + \cdots + d_2 {10}^2 + d_1 10 + d_0 .
\end{equation*}
We often assume $d_K \not= 0$.
To represent $n$ we write the sequence of
digits: $n = d_K d_{K-1} \cdots d_2 d_1 d_0$.
By a (decimal)
\emph{\myindex{digit}}\index{decimal digit}, we mean an integer
between $0$ and $9$.

Similarly we
represent some rational numbers.
That is, for certain
numbers $x$, we can find
negative integer $-M$, a positive integer $K$, and digits
$d_K,d_{K-1},\ldots,d_1,d_0,d_{-1},\ldots,d_{-M}$, such that
\begin{equation*}
x = d_K {10}^K + d_{K-1} {10}^{K-1} + \cdots + d_2 {10}^2 + d_1 10 + d_0 
+ d_{-1} {10}^{-1} + d_{-2} {10}^{-2} + \cdots + d_{-M} {10}^{-M} .
\end{equation*}
We write $x = d_K d_{K-1} \cdots d_1 d_0 \, . \, d_{-1} d_{-2} \cdots d_{-M}$.

Not every real number has such a representation, even the simple
rational number $\nicefrac{1}{3}$ does not.
The irrational number $\sqrt{2}$ 
does not have such a representation either.
To get a representation for
all real numbers we must allow infinitely many digits.

Let us from now on consider only real numbers in the interval $(0,1]$.
If
we find a representation for these, we simply add
integers to them to obtain a representation for all real numbers.
Suppose we take an infinite sequence of decimal digits:
\begin{equation*}
0.d_1d_2d_3\ldots.
\end{equation*}
That is, we have a digit $d_j$ for every $j \in \N$.
We have renumbered the digits to avoid the negative signs.
We say this
sequence of digits represents a real number $x$ if
\begin{equation*}
x =
\sup_{n \in \N} \left(
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n}
\right),
\end{equation*}
or equivalently
\begin{equation*}
x =
\sum_{n=1}^\infty\frac{d_n}{{10}^n}.
\end{equation*}
We call
\begin{equation*}
D_n := 
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n} 
\end{equation*}
the truncation of $x$ to $n$ decimal digits.

\begin{prop} \label{prop:decimalprop}
{~}
\begin{enumerate}[(i)]
\item
Every infinite sequence of digits
$0.d_1d_2d_3\ldots$ represents a unique real number $x \in [0,1]$.
\item
For every $x \in (0,1]$ there exists an infinite sequence of digits
$0.d_1d_2d_3\ldots$ that represents $x$.
There exists a unique representation such that
\begin{equation*}
D_n < x \leq D_n+\frac{1}{{10}^n} \qquad \text{for all $n \in \N$}.
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.
Suppose there is an infinite sequence of
digits $0.d_1d_2d_3\ldots$.
We use the geometric sum formula to write
\begin{equation*}
\begin{split}
D_n =
\frac{d_1}{10} + 
\frac{d_2}{{10}^2} + 
\frac{d_3}{{10}^3} + 
\cdots +
\frac{d_n}{{10}^n} 
& \leq
\frac{9}{10} + 
\frac{9}{{10}^2} + 
\frac{9}{{10}^3} + 
\cdots +
\frac{9}{{10}^n} 
\\
& =
\frac{9}{10}
\bigl( 1 + \nicefrac{1}{10} + 
{(\nicefrac{1}{10})}^2 + \cdots + 
{(\nicefrac{1}{10})}^{n-1} \bigr)
\\
& =
\frac{9}{10}
\left(
\frac{1-{(\nicefrac{1}{10})}^{n}}{1-\nicefrac{1}{10}}
\right)
= 1-{(\nicefrac{1}{10})}^{n}
< 1 .
\end{split}
\end{equation*}
In particular, $D_n < 1$ for all $n$.
As $D_n \geq 0$ is obvious, we obtain
\begin{equation*}
0 \leq \sup_{n\in \N} \, D_n \leq 1 ,
\end{equation*}
and therefore $0.d_1d_2d_3\ldots$ represents a unique number $x \in [0,1]$.

We move on to the
second item.
Take $x \in (0,1]$.
First let us tackle the existence.
By convention define $D_0 := 0$,
then automatically we obtain
$D_0 < x \leq D_0 + {10}^{-0}$.
Suppose for induction that we defined all the digits $d_1,d_2,\ldots,d_n$,
and that 
$D_n < x \leq D_n + {10}^{-n}$.
We need to define $d_{n+1}$.

By the 
\hyperref[thm:arch:i]{Archimedean property} of the real numbers
we find an integer $j$ such that
$x-D_n \leq j {10}^{-(n+1)}$.
We take the least such $j$ and obtain 
\begin{equation} \label{eq:theDnjineq}
(j-1){10}^{-(n+1)} < x-D_n \leq j {10}^{-(n+1)} .
\end{equation}
Let $d_{n+1} := j-1$.
As $D_n < x$,
then $d_{n+1} = j-1 \geq 0$.
On the other hand since
$x-D_n \leq {10}^{-n}$ we have that
$j$ is at most 10, and therefore $d_{n+1} \leq 9$.
So $d_{n+1}$ is a
decimal digit.
Since $D_{n+1} = D_n + d_{n+1} {10}^{-(n+1)}$
we add $D_n$ to the inequality
\eqref{eq:theDnjineq} above:
\begin{equation*}
D_{n+1} = D_n + (j-1){10}^{-(n+1)} < x \leq
%D_n + j {10}^{-(n+1)} =
D_n + (j-1) {10}^{-(n+1)} +
{10}^{-(n+1)} = D_{n+1} + {10}^{-(n+1)} .
\end{equation*}
And so
$D_{n+1} < x \leq D_{n+1} + {10}^{-(n+1)}$ holds.
We have inductively
defined an infinite sequence of digits $0.d_1d_2d_3\ldots$.
As $D_n < x$ for all $n$, then
$\sup \{ D_n : n \in \N \} \leq x$.
As $x-{10}^{-n} \leq D_n$, then
$x-{10}^{-n} \leq \sup \{ D_m : m \in \N \}$ for all $n$.  
The two inequalities together imply
$\sup \{ D_n : n \in \N \} = x$.

What is left to show is the uniqueness.
Suppose $0.e_1e_2e_3\ldots$ is another representation of $x$.
Let $E_n$ be the $n$-digit truncation of $0.e_1e_2e_3\ldots$, and suppose
$E_n < x \leq E_n + {10}^{-n}$ for all $n \in \N$.
Suppose for some $K \in \N$, $e_n = d_n$ for all $n < K$, so
$D_{K-1} = E_{K-1}$.
Then
\begin{equation*}
E_K = D_{K-1} + e_K{10}^{-K} < x \leq E_K + {10}^{-K} = D_{K-1} +
e_K{10}^{-K} + {10}^{-K} .
\end{equation*}
Subtracting $D_{K-1}$ and multiplying by ${10}^{K}$ we get
\begin{equation*}
e_K < (x - D_{K-1}){10}^K \leq e_K + 1 .
\end{equation*}
Similarly we obtain
\begin{equation*}
d_K < (x - D_{K-1}){10}^K \leq d_K + 1 .
\end{equation*}
Hence, both $e_K$ and $d_K$ are the largest integer $j$
such that $j < (x - D_{K-1}){10}^K$, and therefore $e_K = d_K$.
That is,
the representation is unique.
\end{proof}

\begin{exercise}[Easy]
What is the decimal representation of $1$ guaranteed by
\propref{prop:decimalprop}?
  Make sure to show that it does satisfy
the condition.
\end{exercise}

\begin{exercise}
Show that real numbers $x \in (0,1)$ with nonunique decimal representation
are exactly the rational numbers that can be written
as $\frac{m}{10^n}$ for some integers $m$ and $n$.
In this case show that
there exist exactly two representations of $x$.
\end{exercise}

\begin{exercise}\label{ex:binary}
Let $b \geq 2$ be an integer.
Define a representation of a real number in
$[0,1]$ in terms of base $b$ rather than base 10 and prove
\propref{prop:decimalprop} for base $b$.
\end{exercise}

The representation is not unique if we do not require
the extra condition in the proposition.
For example, for the
number $\nicefrac{1}{2}$ the method in the proof obtains the representation
\begin{equation*}
0.49999\ldots .
\end{equation*}
However, we also have the representation $0.5000\ldots$.
The key requirement that makes the representation unique is
$D_n < x$ for all $n$.
The inequality
$x \leq D_n + {10}^{-n}$ is true for every representation
by the computation in the beginning of the proof.

The only numbers that have nonunique
representations are ones that end either in an infinite sequence of $0$s
or $9$s, because the only representation for which
$D_n = x$ is one where all digits past $n$th one are zero.
In this case
there are exactly two representations of $x$ (see the exercises).

Using decimal digits we can also find lots of numbers that are not rational.
The following proposition is true for every
rational number, but we give it only for $x \in (0,1]$ for simplicity.

\begin{prop} \label{prop:rationaldecimal}
If $x \in (0,1]$ is a rational number and $x = 0.d_1d_2d_3\ldots$,
then the decimal digits eventually start repeating.
That is, there are 
positive integers $N$ and $P$, such that for all $n \geq N$, $d_n = d_{n+P}$.
\end{prop}

\begin{proof}
Let $x = \nicefrac{p}{q}$ for positive integers $p$ and $q$.
Let us suppose $x$ is a number with a unique representation, as
otherwise we have seen above that both its representations are repeating.

To compute the first digit we take $10 p$ and divide by
$q$.
The quotient is the first digit $d_1$ and the remainder $r$ is some integer
between 0 and $q-1$.
That is, $d_1$ is the largest integer
such that $d_1 q \leq 10p$ and then $r = 10p - d_1q$.

The next digit is computed by dividing $10 r$ by $q$, and so on.
We notice that at each step there are at most $q$ possible remainders
and hence at some point the process must start repeating.
In fact we see that $P$ is at most $q$.
\end{proof}

\begin{example}
The number
\begin{equation*}
x = 0.101001000100001000001\ldots,
\end{equation*}
is irrational.
That is, the digits are $n$ zeros, then a one, then $n+1$
zeros, then a one, and so on and so forth.
The fact that $x$ is irrational follows from the
proposition; the digits never start repeating.
For every $P$,
if we go far enough, we find a 1 that is followed by at least $P+1$ zeros.
\end{example}

\begin{exercise}
Prove the converse of \propref{prop:rationaldecimal}, that is,
if the digits in the decimal representation of $x$ are eventually repeating, then 
$x$ must be rational.
\end{exercise}

\begin{exercise}\label{exercise:RxR}
Construct a bijection between $[0,1]$ and $[0,1] \times [0,1]$.  Hint:
consider even and odd digits, and be careful about the uniqueness of
representation.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Continuous Functions} \label{lim:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limits of functions}
\label{sec:limoffunc}

\sectionnotes{2--3 lectures}

Before we define continuity of functions, we need to visit a somewhat
more general notion of a limit.
That is, given a function $f \colon S \to
\R$, we want to see how $f(x)$ behaves as $x$ tends to a certain point.

\subsection*{Cluster points}

First,
let us return to a concept we have previously seen in an exercise.

\begin{defn}
Let $S \subset \R$ be a set.
A number $x \in \R$ is called
a \emph{\myindex{cluster point}} of $S$
if for every $\epsilon > 0$, the set $(x-\epsilon,x+\epsilon) \cap S
\setminus \{ x \}$ is not empty.
\end{defn}

That is, $x$ is a cluster point of $S$ if there are points of $S$
arbitrarily close to $x$.
Another way of phrasing the definition is to say
that $x$ is a cluster point of $S$ if for every $\epsilon > 0$, there
exists a $y \in S$ such that $y \not= x$ and $\abs{x - y} < \epsilon$.
Note that a cluster point of $S$ need not lie in $S$.

Let us see some examples.
\begin{enumerate}[(i)]
\item The set
$\{ \nicefrac{1}{n} : n \in \N \}$ has a unique cluster point zero.
\item The cluster points of the open interval $(0,1)$ are
all points in the closed interval $[0,1]$.
\item For the set $\Q$, the set of
cluster points is the whole real line $\R$.
\item For the set $[0,1) \cup \{ 2 \}$,
the set of cluster points is the interval $[0,1]$.
\item The set $\N$ has no cluster points in $\R$.
\end{enumerate}

\begin{prop}
Let $S \subset \R$.
Then $x \in \R$ is a cluster point of $S$
if and only if
there exists a convergent sequence of numbers $\{ x_n \}$ such that
$x_n \not= x$, $x_n \in S$, and $\lim\, x_n = x$.
\end{prop}

\begin{proof}
First suppose $x$ is a cluster point of $S$.
For any $n \in \N$, we pick $x_n$ to be an arbitrary point of
$(x-\nicefrac{1}{n},x+\nicefrac{1}{n}) \cap S \setminus \{x\}$, which
we know is nonempty because $x$ is a cluster point of $S$.
Then
$x_n$ is within $\nicefrac{1}{n}$ of $x$, that is,
\begin{equation*}
\abs{x-x_n} < \nicefrac{1}{n} .
\end{equation*}
As $\{ \nicefrac{1}{n} \}$ converges to zero, $\{ x_n \}$ converges to $x$.

On the other hand, if we start with a sequence of numbers $\{ x_n \}$ in $S$
converging to $x$ such that $x_n \not= x$ for all $n$, then for every
$\epsilon > 0$ there is an $M$ such that in particular $\abs{x_M - x} <
\epsilon$.
That is, $x_M \in (x-\epsilon,x+\epsilon) \cap S \setminus \{x\}$.
\end{proof}

\subsection*{Limits of functions}

If a function $f$ is defined on a set $S$ and $c$ is a cluster point of $S$,
then we can define the limit of $f(x)$ as $x$ gets close to $c$.
Do note
that it is irrelevant for the definition if $f$ is defined at $c$ or not.
Furthermore, even if the function is defined at $c$, the limit of the
function as $x$ goes to $c$ could very well be different
from $f(c)$.

\begin{defn}
\index{limit of a function}%
Let $f \colon S \to \R$ be a function and $c$ a cluster point of $S$.
Suppose there exists an $L \in \R$ and for every $\epsilon > 0$,
there exists a $\delta > 0$ such that whenever $x \in S \setminus \{ c \}$
and $\abs{x - c} < \delta$, then
\begin{equation*}
\abs{f(x) - L} < \epsilon .
\end{equation*}
In this case we say $f(x)$ \emph{\myindex{converges}} to $L$ as $x$ goes
to $c$.
We say $L$ is the \emph{\myindex{limit}} of $f(x)$ as $x$
goes to $c$.
We write
\begin{equation*}
\lim_{x \to c} f(x) := L ,
\end{equation*}
or 
\begin{equation*}
f(x) \to L \quad\text{as}\quad x \to c .
\end{equation*}
If no such $L$ exists, then we say that the limit does not exist or
that $f$ \emph{\myindex{diverges}} at $c$.
\end{defn}

Again the notation and language we are using above assumes the limit
is unique even though we have not yet proved that.
Let us do that now.

\begin{prop}
Let $c$ be a cluster point of $S \subset \R$ and let $f \colon S \to \R$
be a function such that $f(x)$ converges as $x$ goes to $c$.
Then
the limit of $f(x)$ as $x$ goes to $c$ is unique.
\end{prop}

\begin{proof}
Let $L_1$ and $L_2$ be two numbers that both satisfy the definition.
Take an $\epsilon > 0$ and find a $\delta_1 > 0$ such that
$\abs{f(x)-L_1} < \nicefrac{\epsilon}{2}$ 
for all $x \in S \setminus \{c\}$ with $\abs{x-c} < \delta_1$.
Also find $\delta_2 > 0$ such that
$\abs{f(x)-L_2} < \nicefrac{\epsilon}{2}$
for all $x \in S \setminus \{c\}$ with $\abs{x-c} < \delta_2$.
Put $\delta := \min \{ \delta_1, \delta_2 \}$.
Suppose $x \in S$,
$\abs{x-c} < \delta$, and $x \not= c$.
Then
\begin{equation*}
\abs{L_1 - L_2} =
\abs{L_1 - f(x) + f(x) - L_2} \leq
\abs{L_1 - f(x)} + \abs{f(x) - L_2} < \frac{\epsilon}{2} + \frac{\epsilon}{2}
= \epsilon.
\end{equation*}
As $\abs{L_1-L_2} < \epsilon$ for arbitrary $\epsilon > 0$, then
$L_1 = L_2$.
\end{proof}

\begin{example}
Let $f \colon \R \to \R$ be defined as $f(x) := x^2$.
Then
\begin{equation*}
\lim_{x\to c} f(x) = \lim_{x\to c} x^2 = c^2 .
\end{equation*}

\begin{proof} First let $c$ be fixed.
Let $\epsilon > 0$ be given.
Take
\begin{equation*}
\delta := \min \left\{ 1 , \, \frac{\epsilon}{2\abs{c}+1} \right\} .
\end{equation*}
Take $x \not= c$ such that $\abs{x-c} < \delta$.
In particular,
$\abs{x-c} < 1$.
Then by reverse triangle inequality we get
\begin{equation*}
\abs{x}-\abs{c} \leq \abs{x-c} < 1 .
\end{equation*}
Adding $2\abs{c}$ to both sides we obtain
$\abs{x} + \abs{c} < 2\abs{c} + 1$.
We compute
\begin{equation*}
\begin{split}
\abs{f(x) - c^2} &= \abs{x^2-c^2} \\
&= \abs{(x+c)(x-c)} \\
&= \abs{x+c}\abs{x-c} \\
&\leq (\abs{x}+\abs{c})\abs{x-c} \\
&< (2\abs{c}+1)\abs{x-c} \\
&< (2\abs{c}+1)\frac{\epsilon}{2\abs{c}+1} = \epsilon .
\end{split}
\end{equation*}
\end{proof}
\end{example}

\begin{example}
Define $f \colon [0,1) \to \R$ by
\begin{equation*}
f(x) := 
\begin{cases}
x & \text{if $x > 0$} , \\
1 & \text{if $x = 0$} .
\end{cases}
\end{equation*}
Then
\begin{equation*}
\lim_{x\to 0} f(x) = 0 ,
\end{equation*}
even though $f(0) = 1$.

\begin{proof}  Let $\epsilon > 0$ be given.
Let $\delta := \epsilon$.
Then for $x \in [0,1)$, $x \not= 0$, and $\abs{x-0} < \delta$ we get
\begin{equation*}
\abs{f(x) - 0} = \abs{x} < \delta = \epsilon .
\end{equation*}
\end{proof}
\end{example}

\subsection*{Sequential limits} \label{subseq:sequentiallimits}

Let us connect the limit as defined above with limits of sequences.

\begin{lemma}\label{seqflimit:lemma}
Let $S \subset \R$ and $c$ be a cluster point of $S$.
Let $f \colon S \to
\R$ be a function.

Then
$f(x) \to L$ as $x \to c$, if and only if for every sequence $\{ x_n \}$
of numbers such that $x_n \in S \setminus \{c\}$ for all $n$,
and such that $\lim\, x_n = c$,
we have that the sequence $\{ f(x_n) \}$ converges to $L$.
\end{lemma}

\begin{proof}
Suppose 
$f(x) \to L$ as $x \to c$, and $\{ x_n \}$ is a sequence
such that
$x_n \in S \setminus \{c\}$ and
$\lim\, x_n = c$.
We wish to show that $\{ f(x_n) \}$ converges to $L$.
Let $\epsilon > 0$ be given.
Find a $\delta > 0$ such that
if $x \in S \setminus \{c\}$ and $\abs{x-c} < \delta$, then
$\abs{f(x) - L} < \epsilon$.
As
$\{ x_n \}$  converges to $c$, find an $M$ such that for $n \geq M$
we have that $\abs{x_n - c} < \delta$.
Therefore, for $n \geq M$,
\begin{equation*}
\abs{f(x_n) - L} < \epsilon .
\end{equation*}
Thus $\{ f(x_n) \}$ converges to $L$.

For the other direction, we use proof by contrapositive.
Suppose 
it is not true that $f(x) \to L$ as $x \to c$.
The negation of the
definition is that there exists an $\epsilon > 0$ such that for every
$\delta > 0$ there exists an $x \in S \setminus \{c\}$, where
$\abs{x-c} < \delta$
and $\abs{f(x)-L} \geq \epsilon$.

Let us use $\nicefrac{1}{n}$ for $\delta$ in the above statement to
construct a sequence $\{ x_n \}$.
We have
that there exists an $\epsilon > 0$ such that for every $n$,
there exists a point $x_n \in S \setminus \{c\}$, where
$\abs{x_n-c} < \nicefrac{1}{n}$
and $\abs{f(x_n)-L} \geq \epsilon$.
The sequence $\{ x_n \}$ just constructed converges to $c$, but
the sequence $\{ f(x_n) \}$ does not converge to $L$.
And we are done.
\end{proof}

It is possible to strengthen the reverse direction of
the lemma by simply stating that
$\{ f(x_n) \}$ converges without requiring a specific limit.
See \exerciseref{exercise:seqflimitalt}.

\begin{example}
$\displaystyle \lim_{x \to 0} \, \sin( \nicefrac{1}{x} )$
does not exist, but 
$\displaystyle \lim_{x \to 0} \, x\sin( \nicefrac{1}{x} ) = 0$.
See \figureref{figsin1x}.

\begin{figure}[h!t]
\begin{center}
%\includegraphics[width=3in]{sin1x}
\includegraphics{sin1xfig}
\qquad
%\includegraphics[width=3in]{xsin1x}
\includegraphics{xsin1xfig}
\caption{Graphs of $\sin(\nicefrac{1}{x})$ and $x \sin(\nicefrac{1}{x})$.
Note that the computer cannot properly graph $\sin(\nicefrac{1}{x})$
near zero as it oscillates too fast.\label{figsin1x}}
\end{center}
\end{figure}

\begin{proof}
Let us work with $\sin(\nicefrac{1}{x})$ first.
Let us define the sequence
$x_n := \frac{1}{\pi n + \nicefrac{\pi}{2}}$.
It is not hard to see
that $\lim\, x_n = 0$.
Furthermore,
\begin{equation*}
\sin ( \nicefrac{1}{x_n} )
=
\sin (\pi n + \nicefrac{\pi}{2})
= {(-1)}^n .
\end{equation*}
Therefore, $\{ \sin ( \nicefrac{1}{x_n} ) \}$ does not converge.
Thus, by
\lemmaref{seqflimit:lemma}, 
$\lim_{x \to 0} \, \sin( \nicefrac{1}{x} )$ does not exist.

Now let us look at $x\sin(\nicefrac{1}{x})$.
Let $x_n$ be a sequence
such that $x_n \not= 0$ for all $n$ and such that $\lim\, x_n = 0$.
Notice
that $\abs{\sin(t)} \leq 1$ for any $t \in \R$.
Therefore,
\begin{equation*}
\abs{x_n\sin(\nicefrac{1}{x_n})-0}
=
\abs{x_n}\abs{\sin(\nicefrac{1}{x_n})}
\leq
\abs{x_n} .
\end{equation*}
As $x_n$ goes to 0, then $\abs{x_n}$ goes to zero, and hence
$\{ x_n\sin(\nicefrac{1}{x_n}) \}$ converges to zero.
By
\lemmaref{seqflimit:lemma}, 
$\displaystyle \lim_{x \to 0} \, x\sin( \nicefrac{1}{x} ) = 0$.
\end{proof}
\end{example}

Keep in mind the phrase ``for every sequence'' in the lemma.
For example, take $\sin(\nicefrac{1}{x})$ and the sequence $x_n = \nicefrac{1}{\pi n}$.
Then $\{ \sin (\nicefrac{1}{x_n}) \}$ is the constant zero sequence, and
therefore converges to zero.

Using \lemmaref{seqflimit:lemma}, 
we can start applying everything we know about
sequential limits to limits of functions.
Let us give a few important
examples.

\begin{cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.
Let $f \colon S \to
\R$ and $g \colon S \to \R$ be functions.
Suppose the limits of $f(x)$ and $g(x)$ as $x$ goes to $c$ both exist,
and that
\begin{equation*}
f(x) \leq g(x) \qquad \text{for all $x \in S$}.
\end{equation*}
Then
\begin{equation*}
\lim_{x\to c} f(x) \leq \lim_{x\to c} g(x) .
\end{equation*}
\end{cor}

\begin{proof}
Take $\{ x_n \}$ be a sequence of numbers in $S \setminus \{ c \}$
that converges to $c$.
Let
\begin{equation*}
L_1 := \lim_{x\to c} f(x), \qquad \text{and} \qquad L_2 := \lim_{x\to c} g(x) .
\end{equation*}
By \lemmaref{seqflimit:lemma} we know $\{ f(x_n) \}$ converges to
$L_1$ and $\{ g(x_n) \}$ converges to $L_2$.
We also
have $f(x_n) \leq g(x_n)$.
We obtain $L_1 \leq L_2$ using
\lemmaref{limandineq:lemma}.
\end{proof}

By applying constant functions, we get the following corollary.
The
proof is left as an exercise.

\begin{cor} \label{fconstineq:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.
Let $f \colon S \to
\R$ be a function.
And suppose the limit of $f(x)$ as $x$ goes to $c$
exists.
Suppose there are two real numbers $a$ and $b$ such that
\begin{equation*}
a \leq f(x) \leq b \qquad \text{for all $x \in S$}.
\end{equation*}
Then
\begin{equation*}
a \leq \lim_{x\to c} f(x) \leq b .
\end{equation*}
\end{cor}

Using \lemmaref{seqflimit:lemma} in the same way as above we also get
the following corollaries, whose proofs are again left as an exercise.

\begin{cor} \label{fsqueeze:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.
Let $f \colon S \to
\R$,
$g \colon S \to \R$, and $h \colon S \to \R$ be functions.
Suppose 
\begin{equation*}
f(x) \leq g(x) \leq h(x) \qquad \text{for all $x \in S$},
\end{equation*}
and the limits of $f(x)$ and $h(x)$ as $x$ goes to $c$ both exist, and
\begin{equation*}
\lim_{x\to c} f(x) = \lim_{x\to c} h(x) .
\end{equation*}
Then the limit of $g(x)$ as $x$ goes to $c$ exists and
\begin{equation*}
\lim_{x\to c} g(x) =
\lim_{x\to c} f(x) = \lim_{x\to c} h(x) .
\end{equation*}
\end{cor}

\begin{cor} \label{falg:cor}
Let $S \subset \R$ and $c$ be a cluster point of $S$.
Let $f \colon S \to
\R$ and
$g \colon S \to \R$ be functions. 
Suppose limits of $f(x)$ and $g(x)$ as $x$ goes to $c$ both exist.
Then
\begin{enumerate}[(i)]
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)+g(x)\bigr) = \left(\lim_{x\to c} f(x)\right) + 
\left(\lim_{x\to c} g(x)\right) .
$
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)-g(x)\bigr) = \left(\lim_{x\to c} f(x)\right) -
\left(\lim_{x\to c} g(x)\right) .
$
\item
$\displaystyle
\lim_{x\to c} \bigl(f(x)g(x)\bigr) = \left(\lim_{x\to c} f(x)\right)
\left(\lim_{x\to c} g(x)\right) .
$
\item \label{falg:cor:iv} If
$\displaystyle \lim_{x\to c} g(x) \not= 0$,
and $g(x) \not= 0$ for all $x \in S \setminus \{ c \}$, then
\begin{equation*}
\lim_{x\to c} \frac{f(x)}{g(x)} =
\frac{\lim_{x\to c} f(x)}{\lim_{x\to c} g(x)} .
\end{equation*}
\end{enumerate}
\end{cor}

\subsection*{Limits of restrictions and one-sided limits}

%It is not necessary to always consider all of $S$.
Sometimes we work with the function defined on a subset.

\begin{defn}
Let $f \colon S \to \R$ be a function.
Let $A \subset S$.
Define the
function $f|_A \colon A \to \R$ by
\begin{equation*}
f|_A (x) := f(x)  \qquad \text{for $x \in A$}.
\end{equation*}
The function
$f|_A$ is called the \emph{\myindex{restriction}} of $f$ to $A$.
\end{defn}

The function $f|_A$ is simply the function $f$ taken on a smaller domain.
The following proposition is the analogue of taking a tail of a sequence.

\begin{prop} \label{prop:limrest}
Let $S \subset \R$, $c \in \R$, and
let $f \colon S
\to \R$ be a function.
Suppose
$A \subset S$ is such that there is some $\alpha > 0$ such that
$A \cap (c-\alpha,c+\alpha) = S \cap (c-\alpha,c+\alpha)$.
\begin{enumerate}[(i)]
\item The point $c$ is a cluster point of $A$ if and only if $c$ is a cluster point
of $S$.
\item Supposing $c$ is a cluster point of $S$, then $f(x) \to L$ as $x \to c$ if and only if
$f|_A(x) \to L$ as $x \to c$.
\end{enumerate}
\end{prop}

\begin{proof}
First, let $c$ be a cluster point of $A$.
Since $A \subset S$, then if $( A \setminus \{ c\} ) \cap
(c-\epsilon,c+\epsilon)$ is nonempty for every $\epsilon > 0$,
then $( S \setminus \{ c\} ) \cap
(c-\epsilon,c+\epsilon)$ is nonempty for every $\epsilon > 0$.
Thus $c$ is a cluster point of $S$.
Second, suppose $c$ is a cluster
point of $S$.
Then for $\epsilon > 0$ such that $\epsilon < \alpha$
we get that $( A \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon) =
( S \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon)$, which is nonempty.
This is true for all
$\epsilon < \alpha$ and hence 
$( A \setminus \{ c\} ) \cap (c-\epsilon,c+\epsilon)$ must be nonempty for all
$\epsilon > 0$.
Thus $c$ is a cluster point of $A$.

Now suppose $f(x) \to L$ as $x \to c$.
That is, for every $\epsilon > 0$
there is a $\delta > 0$ such that if $x \in S \setminus \{ c \}$
and $\abs{x-c} < \delta$, then $\abs{f(x)-L} < \epsilon$.
Because $A \subset S$,
if $x$ is in $A \setminus \{ c \}$, then $x$ is in $S \setminus \{ c
\}$, and hence $f|_A(x) \to L$ as $x \to c$.

Finally suppose $f|_A(x) \to L$ as $x \to c$.
Hence for every $\epsilon > 0$
there is a $\delta > 0$ such that if $x \in A \setminus \{ c \}$
and $\abs{x-c} < \delta$, then $\bigl\lvert f|_A(x)-L \bigr\rvert < \epsilon$.
Without loss of generality assume $\delta \leq \alpha$.
%If 
%$\delta > \alpha$, then set $\delta := \alpha$.
If
$\abs{x-c} < \delta$, then $x \in S \setminus \{c \}$ if and only if
$x \in A \setminus \{c \}$.
Thus $\abs{f(x)-L} = \bigl\lvert f|_A(x)-L
\bigr\rvert < \epsilon$.
\end{proof}

The hypothesis of the proposition is necessary.
For an arbitrary
restriction we generally only get implication in only one direction,
see \exerciseref{exercise:restrictionlimitexercise}.

A common use of restriction with respect to limits
are \emph{\myindex{one-sided limits}}.

\begin{defn} \label{defn:onesidedlimits}
Let $f \colon S \to \R$ be function and let $c$ be a cluster point of
$S \cap (c,\infty)$.
Then if the limit
of the restriction of $f$ to $S \cap (c,\infty)$ 
 as $x \to c$ exists, we define
\begin{equation*}
\lim_{x \to c^+} f(x) := \lim_{x\to c} f|_{S \cap (c,\infty)}(x) .
\end{equation*}
Similarly if $c$ is a cluster point of 
$S \cap (-\infty,c)$ and the limit of the restriction as $x \to c$
exists, we define
\begin{equation*}
\lim_{x \to c^-} f(x) := \lim_{x\to c} f|_{S \cap (-\infty,c)}(x) .
\end{equation*}
\end{defn}

The proposition above does not apply to one-sided limits.
It is possible to have one-sided limits, but no limit at a point.
For
example, define $f \colon \R \to \R$ by $f(x) := 1$ for $x < 0$ and
$f(x) :=
0$ for $x \geq 0$.
We leave it to the reader to verify that
\begin{equation*}
\lim_{x \to 0^-} f(x) = 1, \qquad
\lim_{x \to 0^+} f(x) = 0, \qquad
\lim_{x \to 0} f(x) \quad \text{does not exist.}
\end{equation*}
We have the following replacement.


\begin{prop} \label{prop:onesidedlimits}
Let $S \subset \R$ be a set such that $c$ is a cluster point
of both $S \cap (-\infty,c)$ and $S \cap (c,\infty)$, and let
$f \colon S \to \R$ be a function.
Then
\begin{equation*}
\lim_{x \to c} f(x) = L
\qquad \text{if and only if} \qquad
\lim_{x \to c^-} f(x) =
\lim_{x \to c^+} f(x) =
L .
\end{equation*}
\end{prop}

That is, a limit exists if both one-sided limits exist and are equal, and
vice-versa.
The
proof is a straightforward application of the definition of limit
and is left as an exercise.
The key point is that
$\bigl( S \cap (-\infty,c) \bigr) \cup \bigl( S \cap (c,\infty) \bigr)
= S \setminus \{ c \}$.

\subsection*{Exercises}

\begin{exercise}
Find the limit or prove that the limit does not exist

\medskip

\noindent
\begin{tabular}{lllll}
a)
$\displaystyle
\lim_{x\to c} \sqrt{x}
$, for $c \geq 0$
& &
b)
$\displaystyle
\lim_{x\to c} x^2+x+1
$, for any $c \in \R$
& &
c)
$\displaystyle
\lim_{x\to 0} x^2 \cos (\nicefrac{1}{x})
$
\\
d)
$\displaystyle
\lim_{x\to 0}\, \sin(\nicefrac{1}{x}) \cos (\nicefrac{1}{x})
$
& &
e)
$\displaystyle
\lim_{x\to 0}\, \sin(x) \cos (\nicefrac{1}{x})
$ & 
\end{tabular}
\end{exercise}

\begin{exercise}
Prove \corref{fconstineq:cor}.
\end{exercise}

\begin{exercise}
Prove \corref{fsqueeze:cor}.
\end{exercise}

\begin{exercise}
Prove \corref{falg:cor}.
\end{exercise}

\begin{exercise}
Let $A \subset S$.
Show that if $c$ is a cluster point of $A$, then $c$
is a cluster point of $S$.
Note the difference from
\propref{prop:limrest}.
\end{exercise}

\begin{exercise} \label{exercise:restrictionlimitexercise}
Let $A \subset S$.
Suppose $c$ is a cluster point of $A$ and
it is also a cluster point of $S$.
Let $f \colon S \to \R$ be a function.
Show that if
$f(x) \to L$ as $x \to c$, then
$f|_A(x) \to L$ as $x \to c$.
Note the difference from
\propref{prop:limrest}.
\end{exercise}

\begin{exercise}
Find an example of a function $f \colon [-1,1] \to \R$ such that
for $A:=[0,1]$, the restriction
$f|_A(x) \to 0$ as $x \to 0$, but the limit of $f(x)$ as $x \to 0$
does not exist.
Note why you cannot apply
\propref{prop:limrest}.
\end{exercise}

\begin{exercise}
Find example functions $f$ and $g$ such that the limit of neither $f(x)$
nor $g(x)$ exists as $x \to 0$, but such that the limit of $f(x)+g(x)$ exists
as $x \to 0$.
\end{exercise}

\begin{exercise} \label{exercise:contlimitcomposition}
Let $c_1$ be a cluster point of $A \subset \R$ and $c_2$ be
a cluster point of $B \subset \R$.
Suppose 
$f \colon A \to B$ and $g \colon B \to \R$ are functions
such that
$f(x) \to c_2$ as $x \to c_1$ and
$g(y) \to L$ as $y \to c_2$.
If $c_2 \in B$ also suppose that $g(c_2) = L$.
Let $h(x) := g\bigl(f(x)\bigr)$ and show
$h(x) \to L$ as $x \to c_1$.
Hint: note that $f(x)$ could equal $c_2$ for many $x \in A$,
see also
\exerciseref{exercise:contlimitbadcomposition}.
\end{exercise}

\begin{exercise}
Let $c$ be a cluster point of $A \subset \R$, and $f \colon A \to \R$
be a function.
Suppose for every sequence $\{x_n\}$ in $A$,
such that $\lim\, x_n = c$,
the sequence $\{ f(x_n) \}_{n=1}^\infty$ is Cauchy.
Prove that
$\lim_{x\to c} f(x)$ exists.
\end{exercise}

\begin{exercise} \label{exercise:seqflimitalt}
Prove the following stronger version of one direction of
\lemmaref{seqflimit:lemma}:
Let $S \subset \R$, $c$ be a cluster point of $S$, and $f \colon S \to
\R$ be a function.
Suppose that for every sequence $\{x_n\}$ in $S \setminus \{c\}$ such that
$\lim\, x_n = c$ the sequence $\{ f(x_n) \}$ is convergent.
Then show $f(x) \to L$ as $x \to c$ for some $L \in \R$.
\end{exercise}

\begin{exercise}
Prove \propref{prop:onesidedlimits}.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$ and $c$ is a cluster point of $S$.
Suppose $f \colon
S \to \R$ is bounded.
Show that there exists a sequence $\{ x_n \}$
with $x_n \in S \setminus \{ c \}$ and $\lim\, x_n = c$ such that
$\{ f(x_n) \}$ converges.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:contlimitbadcomposition}
Show that the hypothesis that $g(c_2) = L$ in
\exerciseref{exercise:contlimitcomposition} is necessary.
That is, find $f$
and $g$ such that $f(x) \to c_2$ as $x \to c_1$ and
$g(y) \to L$ as $y \to c_2$, but $g\bigl(f(x)\bigr)$ does not go to $L$
as $x \to c_1$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Continuous functions}
\label{sec:cont}

\sectionnotes{2--2.5 lectures}

You undoubtedly heard of continuous functions in your schooling.
A
high-school criterion for this concept is that a function is continuous if
we can draw its graph without lifting the pen from the paper.
While that
intuitive concept may be useful in simple situations, we require
rigor.
The following definition took three great mathematicians
(Bolzano, Cauchy, and finally Weierstrass) to get correctly and its final
form dates only to the late 1800s.

\subsection*{Definition and basic properties}

\begin{defn}
Let $S \subset \R$, $c \in S$, and let $f \colon S \to \R$ be a function.
We say
that $f$ is \emph{continuous at $c$}\index{continuous at $c$}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x \in S$ and $\abs{x-c} <
\delta$, then
$\abs{f(x)-f(c)} < \epsilon$.

%\medskip

When $f \colon S \to \R$ is continuous at all $c \in S$, then we simply say
$f$ is a \emph{\myindex{continuous function}}.
\end{defn}
\begin{figure}[h!t]
\begin{center}
\input contigr.pdf_t
\caption{For $\abs{x-c} < \delta$, $f(x)$ should be within the gray region.\label{fig:contigr}}
\end{center}
\end{figure}

If $f$ is continuous for all $c \in A$, we say
$f$ is continuous on $A \subset S$.
It is left as an easy exercise to
show that this implies that $f|_A$ is continuous, although
the converse does not hold.

Continuity may be the most important definition to understand in analysis,
and it is not an easy one.
See \figureref{fig:contigr}.
Note that $\delta$ not only
depends on $\epsilon$, but also on $c$;  we need not pick
one $\delta$ for all $c \in S$.
It is no accident 
that the definition of continuity is similar to the definition of a
limit of a function.
The main feature of continuous functions
is that these are precisely the functions that behave nicely with limits.

\enlargethispage{\baselineskip}
\begin{prop} \label{contbasic:prop}
Suppose $f \colon S \to \R$ is a function and $c \in S$.
Then
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
\begin{enumerate}[(i)]
\item If $c$ is not a cluster point of $S$, then $f$ is continuous at $c$.
\item If $c$ is a cluster point of $S$, then $f$ is continuous at $c$
if and only if the limit of $f(x)$ as $x \to c$ exists and
\begin{equation*}
\lim_{x\to c} f(x) = f(c) .
\end{equation*}
\item $f$ is continuous at $c$ if and only if for every sequence $\{ x_n \}$
where $x_n \in S$ and $\lim\, x_n = c$, the sequence $\{ f(x_n) \}$ converges
to $f(c)$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.
Suppose $c$ is not a cluster point of
$S$.
Then there exists a $\delta > 0$
such that $S \cap (c-\delta,c+\delta) = \{
c \}$.
Therefore, for any $\epsilon > 0$, simply pick this given delta.
The only $x \in S$ such that $\abs{x-c} < \delta$ is $x=c$.
Then
$\abs{f(x)-f(c)} = \abs{f(c)-f(c)} = 0 < \epsilon$.

Let us move to the second item.
Suppose $c$ is a cluster point of $S$.
Let us first suppose
that $\lim_{x\to c} f(x) = f(c)$.
Then for every $\epsilon > 0$
there is a $\delta > 0$ such that if $x \in S \setminus \{ c \}$
and $\abs{x-c} < \delta$, then $\abs{f(x)-f(c)} < \epsilon$.
As
$\abs{f(c)-f(c)} = 0 < \epsilon$, then the definition of continuity at
$c$ is satisfied.
On the other hand, suppose $f$ is continuous
at $c$.
For every $\epsilon > 0$, there exists a $\delta > 0$
such that for $x \in S$ where $\abs{x-c} < \delta$ we have
$\abs{f(x)-f(c)} < \epsilon$.
Then the statement is, of course, still true if
$x \in S \setminus \{ c \} \subset S$.
Therefore $\lim_{x\to c} f(x) =
f(c)$.

For the third item, suppose $f$ is continuous at $c$.
Let $\{ x_n \}$
be a sequence such that $x_n \in S$ and $\lim\, x_n = c$.
Let $\epsilon > 0$
be given.
Find a $\delta > 0$ such that $\abs{f(x)-f(c)} < \epsilon$
for all $x \in S$ where $\abs{x-c} < \delta$.
Find an $M \in \N$
such that for $n \geq M$ we have $\abs{x_n-c} < \delta$.
Then for
$n \geq M$ we have that $\abs{f(x_n)-f(c)} < \epsilon$, so $\{ f(x_n) \}$
converges to $f(c)$.

Let us prove the converse of the third item by contrapositive.
Suppose $f$ is not
continuous at $c$.
Then there exists an $\epsilon > 0$
such that for all $\delta > 0$, there exists an $x \in S$
such that $\abs{x-c} < \delta$ and $\abs{f(x)-f(c)} \geq \epsilon$.
Let us define a sequence $\{ x_n \}$ as follows.
Let $x_n \in S$ be such that $\abs{x_n-c} < \nicefrac{1}{n}$
and $\abs{f(x_n)-f(c)} \geq \epsilon$.
%As $f$ is not continuous at $c$,
%we can do this.
Now $\{ x_n \}$ is
a sequence of numbers in $S$ such that
$\lim\, x_n = c$ and such that
$\abs{f(x_n)-f(c)} \geq \epsilon$ for all $n \in \N$.
Thus $\{ f(x_n) \}$
does not converge to $f(c)$.
It may or may not converge, but it definitely
does not converge to $f(c)$.  
\end{proof}

From \propref{contbasic:prop} and \propref{prop:onesidedlimits} we also get the following useful characterization. 

\begin{cor}
Let $S \subset \R$ be a set such that $c$ is a cluster point
of both $S \cap (-\infty,c)$ and $S \cap (c,\infty)$, and let
$f \colon S \to \R$ be a function.
Then $f$ is continuous at $c$ if and only if 
\begin{equation*}
\lim_{x \to c^-} f(x) =
\lim_{x \to c^+} f(x)
\end{equation*}
That is, both one-sided limits exist and are equal.
\end{cor}


The last item in the proposition is particularly powerful.
It allows us to
quickly apply what we know about limits of sequences to continuous functions
and even to prove that certain functions are continuous.
It can also be strengthened, see \exerciseref{exercise:contseqalt}.

\begin{example}
$f \colon (0,\infty) \to \R$ defined by
$f(x) := \nicefrac{1}{x}$ is continuous.

\begin{proof} Fix $c \in (0,\infty)$.  
Let $\{ x_n \}$ be a sequence in $(0,\infty)$ such that
$\lim\, x_n = c$.
Then we know that
\begin{equation*}
f(c) = \frac{1}{c}
=
\frac{1}{\lim\, x_n}
=
\lim_{n \to \infty} \frac{1}{x_n}
=
\lim_{n \to \infty} f(x_n) .
\end{equation*}
Thus $f$ is continuous at $c$.
As $f$ is continuous at all $c \in
(0,\infty)$, $f$ is continuous.
\end{proof}
\end{example}

We have previously shown $\lim_{x \to c} x^2 = c^2$ directly.
Therefore
the function $x^2$ is continuous.
We can use the continuity of
algebraic operations with respect to limits of sequences, which we proved in
the previous chapter, to prove a much more general result.

\begin{prop}
Let $f \colon \R \to \R$ be a \emph{\myindex{polynomial}}.
That is
\begin{equation*}
f(x) = a_d x^d + a_{d-1} x^{d-1} + \cdots + a_1 x + a_0 ,
\end{equation*}
for some constants $a_0, a_1, \ldots, a_d$.
Then $f$ is continuous.
\end{prop}

\begin{proof}
Fix $c \in \R$.  
Let $\{ x_n \}$ be a sequence such that
$\lim\, x_n = c$.
Then
\begin{equation*}
\begin{split}
f(c) &=
a_d c^d + a_{d-1} c^{d-1} + \cdots + a_1 c + a_0 
\\
&= 
a_d {(\lim\, x_n)}^d + a_{d-1} {(\lim\, x_n)}^{d-1} + \cdots + a_1 (\lim\, x_n) + a_0 
\\
& =
\lim_{n \to \infty}
\left(
a_d x_n^d + a_{d-1} x_n^{d-1} + \cdots + a_1 x_n + a_0 
\right)
=
\lim_{n \to \infty}
f(x_n) .
\end{split}
\end{equation*}
Thus $f$ is continuous at $c$.
As $f$ is continuous at all $c \in \R$,
$f$ is continuous.
\end{proof}

By similar reasoning, or by appealing to \corref{falg:cor},
we can prove the following.
The details of the proof are left as an
exercise.

\begin{prop} \label{contalg:prop}
Let $f \colon S \to \R$ and $g \colon S \to \R$ be functions
continuous at $c \in S$.
\begin{enumerate}[(i)]
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)+g(x)$ is continuous at $c$.
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)-g(x)$ is continuous at $c$.
\item The function $h \colon S \to \R$ defined by
$h(x) := f(x)g(x)$ is continuous at $c$.
\item If $g(x)\not=0$ for all $x \in S$, the function $h \colon S \to \R$
defined by $h(x) := \frac{f(x)}{g(x)}$ is continuous at $c$.
\end{enumerate}
\end{prop}

\begin{example} \label{sincos:example}
The functions $\sin(x)$ and $\cos(x)$ are continuous.
In the following computations we use the sum-to-product
trigonometric identities.
We also use the simple facts that
$\abs{\sin(x)} \leq \abs{x}$, $\abs{\cos(x)} \leq 1$,
and $\abs{\sin(x)} \leq 1$.
\begin{equation*}
\begin{split}
\abs{\sin(x)-\sin(c)} & =
\abs{
2 \sin \left( \frac{x-c}{2} \right) \cos \left( \frac{x+c}{2} \right)
}
\\
& =
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\abs{ \cos \left( \frac{x+c}{2} \right) }
\\
& \leq
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\\
& \leq
2
\abs{ \frac{x-c}{2} }
= \abs{x-c}
\end{split}
\end{equation*}
\begin{equation*}
\begin{split}
\abs{\cos(x)-\cos(c)} & =
\abs{
-2 \sin \left( \frac{x-c}{2} \right) \sin \left( \frac{x+c}{2} \right)
}
\\
& =
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\abs{ \sin \left( \frac{x+c}{2} \right) }
\\
& \leq
2
\abs{ \sin \left( \frac{x-c}{2} \right) }
\\
& \leq
2
\abs{ \frac{x-c}{2} }
= \abs{x-c}
\end{split}
\end{equation*}

The claim that sin and cos are continuous follows by taking an
arbitrary sequence $\{ x_n \}$ converging to $c$, or by applying the
definition of continuity directly.
Details are left to the
reader.
\end{example}

\subsection*{Composition of continuous functions}

You have probably already realized that one of the basic tools in
constructing complicated functions out of simple ones is composition.
A useful property of continuous functions is that compositions
of continuous functions are again
continuous.
Recall that for two functions $f$ and $g$,
the composition $f \circ g$ is defined by
$(f \circ g)(x) := f\bigl(g(x)\bigr)$.

\begin{prop}
Let $A, B \subset \R$ and $f \colon B \to \R$ and $g \colon A \to B$ be
functions.
If $g$ is continuous at $c \in A$ and
$f$ is continuous at $g(c)$, then $f \circ g \colon A \to \R$ is continuous
at $c$.
\end{prop}

\begin{proof}
Let $\{ x_n \}$ be a sequence in $A$ such that $\lim\, x_n = c$.
Then as $g$ is continuous at $c$, then $\{ g(x_n) \}$ converges to $g(c)$.
As $f$ is continuous at $g(c)$, then $\{ f\bigl(g(x_n)\bigr) \}$ converges
to $f\bigl(g(c)\bigr)$.
Thus $f \circ g$ is continuous at $c$.
\end{proof}

\begin{example}
Claim: ${\bigl(\sin(\nicefrac{1}{x})\bigr)}^2$ is a continuous function on $(0,\infty)$.

\begin{proof} First note that $\nicefrac{1}{x}$ is a continuous function on
$(0,\infty)$ and $\sin(x)$ is a continuous function on $(0,\infty)$ (actually
on all of $\R$, but $(0,\infty)$ is the range for $\nicefrac{1}{x}$).
Hence the composition $\sin(\nicefrac{1}{x})$ is continuous.
We also
know that $x^2$ is continuous on the interval $(-1,1)$ (the range of sin).
Thus
the composition
${\bigl(\sin(\nicefrac{1}{x})\bigr)}^2$ is also continuous on $(0,\infty)$.
\end{proof}
\end{example}

\subsection*{Discontinuous functions}

%Let us spend a bit of time on discontinuous functions.
When $f$ is not continuous at $c$, we
say $f$ is \emph{\myindex{discontinuous}} at $c$, or that it has a
\emph{\myindex{discontinuity}} at $c$.
If we state the
contrapositive of the third item of \propref{contbasic:prop} as
a separate claim we get an easy to use test for discontinuities.

\begin{prop}
Let $f \colon S \to \R$ be a function.
Suppose that for some $c \in S$,
there exists a sequence $\{ x_n \}$, $x_n \in S$, and $\lim\, x_n = c$
such that $\{ f(x_n) \}$ does not converge to $f(c)$ (or does not
converge at all), then $f$ is not
continuous at $c$.
\end{prop}

\begin{example} \label{example:stepdiscont}
The function $f \colon \R \to \R$ defined by
\begin{equation*}
f(x) := 
\begin{cases}
-1 & \text{ if $x < 0$,} \\
1 & \text{ if $x \geq 0$,}
\end{cases}
\end{equation*}
is not continuous at 0.

\begin{proof} Take the sequence $\{ - \nicefrac{1}{n} \}$.
Then
$f(-\nicefrac{1}{n}) = -1$
and so
$\lim\, f(-\nicefrac{1}{n}) = -1$, but $f(0) = 1$.
\end{proof}
\end{example}

\begin{example}
For an extreme example we take the so-called
\emph{\myindex{Dirichlet function}}.
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x$ is rational,} \\
0 & \text{ if $x$ is irrational.}
\end{cases}
\end{equation*}
The function $f$ is discontinuous at all $c \in \R$.

\begin{proof}
Suppose $c$ is rational.
Take a sequence $\{ x_n \}$
of irrational numbers such that $\lim\, x_n = c$ (why can we?).
Then $f(x_n) = 0$
and so $\lim\, f(x_n) = 0$, but $f(c) = 1$.
If $c$ is irrational, take a sequence of rational numbers $\{ x_n \}$
that converges to $c$ (why can we?).
Then $\lim\, f(x_n) = 1$, but $f(c) = 0$.
\end{proof}
\end{example}

Let us yet again test the limits of your intuition.
Can
there exist a function that is continuous on all irrational numbers, but
discontinuous at all rational numbers?
  There are rational numbers
arbitrarily close to any irrational number.
Perhaps strangely, the
answer is yes.
The following example is called the
\emph{\myindex{Thomae function}}\footnote{Named after the German
mathematician
\href{http://en.wikipedia.org/wiki/Thomae}{Johannes Karl Thomae}
(1840 -- 1921).} or the
\emph{\myindex{popcorn function}}.

\begin{example} \label{popcornfunction:example}
Let $f \colon (0,1) \to \R$ be defined by
\begin{equation*}
f(x) := 
\begin{cases}
\nicefrac{1}{k} & \text{ if $x=\nicefrac{m}{k}$ where $m,k \in \N$
and $m$ and $k$ have no common divisors,} \\
0 & \text{ if $x$ is irrational}.
\end{cases}
\end{equation*}
Then $f$ is continuous at all irrational $c \in (0,1)$ and 
discontinuous at all rational $c$.
See the graph of $f$
in \figureref{popcornfig}.
\begin{figure}[h!t]
\begin{center}
\includegraphics{popcornfig}
\caption{Graph of the ``popcorn function.''\label{popcornfig}}
\end{center}
\end{figure}

\begin{proof}
Suppose $c = \nicefrac{m}{k}$ is rational.
Take a sequence of
irrational numbers $\{ x_n \}$ such that $\lim\, x_n = c$.
Then
$\lim\, f(x_n) = \lim \, 0 = 0$, but $f(c) = \nicefrac{1}{k} \not= 0$.
So $f$
is discontinuous at $c$.

Now let $c$ be irrational, so $f(c) = 0$.
Take a sequence 
$\{ x_n \}$ of numbers in $(0,1)$ such that $\lim\, x_n = c$.
Given $\epsilon > 0$, find $K \in \N$ such
that $\nicefrac{1}{K} < \epsilon$
by the \hyperref[thm:arch:i]{Archimedean property}.
If $\nicefrac{m}{k} \in (0,1)$ is lowest terms
(no common divisors), then $m < k$.
So there are only finitely many rational numbers in $(0,1)$
whose denominator $k$ in lowest terms is less than $K$.
Hence
there is an $M$ such that for $n \geq M$, all the numbers $x_n$
that are rational
have a denominator larger than or equal to $K$.
Thus for $n \geq M$
\begin{equation*}
\abs{f(x_n) - 0} = f(x_n) \leq \nicefrac{1}{K} < \epsilon .
\end{equation*}
Therefore $f$ is continuous at irrational $c$.
\end{proof}
\end{example}

Let us end on an easier example.

\begin{example}
Define
$g \colon \R \to \R$ by $g(x) := 0$ if $x \not= 0$ and
$g(0) := 1$.
Then $g$ is not continuous at zero, but continuous everywhere else (why?).
The point $x=0$ is called a \emph{\myindex{removable discontinuity}}.
That
is because if we would change the definition of $g$, by insisting that
$g(0)$ be $0$, we would obtain a continuous function.
On the other hand
let $f$ be the function of example \exampleref{example:stepdiscont}.
Then $f$ does not have a
removable discontinuity at $0$.
No matter how we would define $f(0)$ the function
will still fail to be continuous.
The difference is that 
$\lim_{x\to 0} g(x)$ exists while
$\lim_{x\to 0} f(x)$ does not.

Let us stay with this example but show another phenomenon.
Let $A = \{ 0
\}$, then $g|_A$ is continuous (why?), while $g$ is not continuous on $A$.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Using the definition of continuity directly prove that
$f \colon \R \to \R$ defined by
$f(x) := x^2$ is continuous.
\end{exercise}

\begin{exercise}
Using the definition of continuity directly prove that
$f \colon (0,\infty) \to \R$ defined by
$f(x) := \nicefrac{1}{x}$ is continuous.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be defined by
\begin{equation*}
f(x) :=
\begin{cases}
x & \text{ if $x$ is rational,} \\
x^2 & \text{ if $x$ is irrational.}
\end{cases}
\end{equation*}
Using the definition of continuity directly prove that
$f$ is continuous at $1$ and discontinuous at $2$.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be
defined by
\begin{equation*}
f(x) :=
\begin{cases}
\sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Is $f$ continuous?
  Prove your assertion.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be
defined by
\begin{equation*}
f(x) :=
\begin{cases}
x \sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Is $f$ continuous?
  Prove your assertion.
\end{exercise}

\begin{exercise}
Prove \propref{contalg:prop}.
\end{exercise}

\begin{exercise}
Prove the following statement.
Let $S \subset \R$ and $A \subset S$.
Let $f \colon S \to \R$
be a continuous function.
Then the restriction $f|_A$ is continuous.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$.
Suppose for some $c \in \R$
and $\alpha > 0$, we have $A=(c-\alpha,c+\alpha) \subset S$.
Let $f \colon S \to \R$ be a function.
Prove that
if $f|_A$ is continuous at $c$, then $f$ is continuous at $c$.
\end{exercise}

\begin{exercise}
Give an example of functions $f \colon \R \to \R$ and $g \colon \R \to \R$
such that the function $h$ defined by $h(x) := f(x) + g(x)$ is continuous,
but $f$ and $g$ are not continuous.
Can you find $f$ and $g$ that are nowhere
continuous, but $h$ is a continuous function?
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ and 
$g \colon \R \to \R$ be continuous functions.
Suppose that for
all rational numbers $r$, $f(r) = g(r)$.
Show that $f(x) = g(x)$ for all
$x$.
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be continuous.
Suppose $f(c) > 0$.
Show that
there exists an $\alpha > 0$ such that for all $x \in (c-\alpha,c+\alpha)$
we have $f(x) > 0$.
\end{exercise}

\begin{exercise}
Let $f \colon \Z \to \R$ be a function.
Show that $f$ is continuous.
\end{exercise}

\begin{exercise} \label{exercise:contseqalt}
Let $f \colon S \to \R$ be a function and $c \in S$, such that for every
sequence $\{ x_n \}$ in $S$ with $\lim\, x_n = c$, the sequence
$\{ f(x_n) \}$ converges.
Show that $f$ is continuous at $c$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [-1,0] \to \R$ and $g \colon [0,1] \to \R$ are continuous
and $f(0) = g(0)$.
Define $h \colon [-1,1] \to \R$ by 
$h(x) := f(x)$ if $x \leq 0$ and $h(x) := g(x)$ if $x > 0$.
Show that
$h$ is continuous.
\end{exercise}

\begin{exercise}
Suppose $g \colon \R \to \R$ is a continuous function such that $g(0) = 0$,
and supppse $f \colon \R \to \R$ is such that
$\abs{f(x)-f(y)} \leq g(x-y)$ for all $x$ and $y$.
Show that $f$ is
continuous.
\end{exercise}

\begin{exercise}[Challenging]
Suppose $f(x+y) = f(x) + f(y)$ for some $f \colon \R \to \R$
such that $f$ is continuous at 0.
Show that $f(x) = ax$ for some $a \in \R$.
Hint: Show that $f(nx) = nf(x)$, then show $f$ is continuous on $\R$.
Then show that $\nicefrac{f(x)}{x} = f(1)$ for all rational $x$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Min-max and intermediate value theorems}
\label{sec:minmaxint}

\sectionnotes{1.5 lectures}

Continuous functions defined on closed and bounded intervals
have some interesting and very useful properties.
%Let us now state and prove some very important results about continuous
%functions defined on closed and bounded intervals of the real line.
%In particular, on closed bounded
%intervals of
%the real line.

\subsection*{Min-max theorem}

Recall a function $f \colon [a,b] \to \R$ is
\emph{bounded\index{bounded function}} if there exists a $B \in \R$ such that
$\abs{f(x)} \leq B$ for all $x \in [a,b]$.
We have the following lemma.

\begin{lemma}
Let $f \colon [a,b] \to \R$ be a continuous function.
Then $f$ is bounded.
\end{lemma}

\begin{proof}
Let us prove this claim by contrapositive.
Suppose $f$ is not bounded,
then for each
$n \in \N$, there is an $x_n \in [a,b]$, such that
\begin{equation*}
\abs{f(x_n)} \geq n .
\end{equation*}
Now $\{ x_n \}$ is a bounded sequence as $a \leq x_n \leq b$.
By the \hyperref[thm:bwseq]{Bolzano--Weierstrass theorem},
there is a convergent subsequence $\{ x_{n_i} \}$.
Let $x := \lim\, x_{n_i}$.
Since $a \leq x_{n_i} \leq b$ for all $i$, then $a \leq x \leq b$.
The limit $\lim\, f(x_{n_i})$ does
not exist as the sequence is not bounded as $\abs{f(x_{n_i})} \geq n_i \geq
i$.
On the other hand $f(x)$ is a finite number and
\begin{equation*}
f(x)
=
f\left( \lim_{i\to\infty} x_{n_i} \right) .
\end{equation*}
Thus $f$ is not continuous at $x$.
\end{proof}

In fact, for a continuous $f$, we will see that the minimum and
the maximum are actually achieved.
Recall from calculus that $f \colon S \to \R$ achieves an
\emph{\myindex{absolute minimum}} at $c \in S$ if
\begin{equation*}
f(x) \geq f(c) \qquad \text{ for all $x \in S$.}
\end{equation*}
On the other hand, $f$ achieves an 
\emph{\myindex{absolute maximum}} at $c \in S$ if
\begin{equation*}
f(x) \leq f(c) \qquad \text{ for all $x \in S$.}
\end{equation*}
We say $f$ achieves an absolute minimum or an absolute maximum on
$S$ if such a $c \in S$ exists.
If $S$ is a closed
and bounded interval, then a continuous $f$
must have an absolute minimum and an absolute
maximum on $S$.

\begin{thm}[Minimum-maximum theorem]
\index{Minimum-maximum theorem}
\index{Maximum-minimum theorem}
Let $f \colon [a,b] \to \R$ be a continuous function.
Then $f$
achieves both an absolute minimum and an absolute maximum on $[a,b]$.
\end{thm}

\begin{proof}
We have shown that $f$ is bounded by the lemma.
Therefore, the
set $f([a,b]) = \{ f(x) : x \in [a,b] \}$ has a supremum and an infimum.
From what we know about suprema and infima, there exist sequences
in the set $f([a,b])$ that approach them.
That is, there are sequences
$\{ f(x_n) \}$ and $\{ f(y_n) \}$, where $x_n, y_n$ are in $[a,b]$,
such that
\begin{equation*}
\lim_{n\to\infty} f(x_n) = \inf f([a,b]) \qquad \text{and} \qquad
\lim_{n\to\infty} f(y_n) = \sup f([a,b]).
\end{equation*}
We are not done yet, we need to find where the minimum and the maxima are.
The problem is that the sequences $\{ x_n \}$ and $\{ y_n \}$ need not
converge.
We know $\{ x_n \}$ and $\{ y_n \}$ are bounded (their elements
belong to 
a bounded interval $[a,b]$).
We apply the 
\hyperref[thm:bwseq]{Bolzano--Weierstrass theorem}.
Hence there exist convergent subsequences
$\{ x_{n_i} \}$ and 
$\{ y_{m_i} \}$.
Let
\begin{equation*}
x := \lim_{i\to\infty} x_{n_i}
\qquad \text{and} \qquad
y := \lim_{i\to\infty} y_{m_i}.
\end{equation*}
Then as $a \leq x_{n_i} \leq b$, we have that $a \leq x \leq b$.
Similarly $a \leq y \leq b$, so $x$ and $y$ are in $[a,b]$.
We apply that a limit of a subsequence is the same as the limit of the
sequence, and we apply the continuity of $f$ to obtain
\begin{equation*}
\inf f([a,b]) = \lim_{n\to\infty} f(x_n)
= \lim_{i\to\infty} f(x_{n_i}) = 
f \left( \lim_{i\to\infty} x_{n_i} \right) = f(x) .
\end{equation*}
Similarly,
\begin{equation*}
\sup f([a,b]) = \lim_{n\to\infty} f(m_n)
= \lim_{i\to\infty} f(y_{m_i}) = 
f \left( \lim_{i\to\infty} y_{m_i} \right) = f(y) .
\end{equation*}
Therefore, $f$ achieves an absolute minimum at $x$ and
$f$ achieves an absolute maximum at $y$.
\end{proof}

\begin{example}
The function $f(x) := x^2+1$ defined on the interval $[-1,2]$ achieves a minimum
at $x=0$ when $f(0) = 1$.
It achieves a maximum at $x=2$ where $f(2) = 5$.
Do note that the domain of definition matters.
If we instead took the domain
to be $[-10,10]$, then $x=2$ would no longer be a maximum of $f$.
Instead
the maximum would be achieved at either $x=10$ or $x=-10$.
\end{example}

Let us show by examples that the different hypotheses of the theorem are
truly needed.

\begin{example}
The function $f(x) := x$, defined on the whole real line,
achieves neither a minimum, nor a maximum.
So it is important that
we are looking at a bounded interval.
\end{example}

\begin{example}
The function $f(x) := \nicefrac{1}{x}$, defined on $(0,1)$ 
achieves neither a minimum, nor a maximum.
The values of the function are
unbounded as we approach 0.
Also as we approach $x=1$, the values of the
function approach 1, but $f(x) > 1$ for all $x \in (0,1)$.
There is
no $x \in (0,1)$ such that $f(x) = 1$.
So it is important that
we are looking at a closed interval.
\end{example}

\begin{example}
Continuity is important.
Define $f \colon [0,1] \to \R$ by 
$f(x) := \nicefrac{1}{x}$ for $x > 0$ and let $f(0) := 0$.
Then
the function does not achieve a maximum.
The problem is that
the function is not continuous at 0.
\end{example}

\begin{exercise}
Find an example of a \emph{bounded} discontinuous function $f \colon [0,1]
\to \R$ that has neither an absolute minimum nor an absolute maximum.
\end{exercise}

\begin{exercise}
Find an example of a bounded function $f \colon \R \to \R$ that does
not achieve an absolute minimum nor an absolute maximum on $\R$.
\end{exercise}

\begin{exercise}
Let $f \colon (0,1) \to \R$ be a continuous function such that
$\displaystyle \lim_{x\to 0} f(x) =
\displaystyle \lim_{x\to 1} f(x) = 0$.
Show that
$f$ achieves either an absolute minimum or an absolute maximum on $(0,1)$
(but perhaps not both).
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuous and periodic with period $P > 0$.
That is, $f(x+P) = f(x)$ for all $x \in \R$.
Show that $f$ achieves an absolute minimum and an absolute maximum.
\end{exercise}



\subsection*{Intermediate value theorem}

Intermediate value theorem is one of the cornerstones of analysis.
It is sometimes called Bolzano's intermediate value theorem, or just Bolzano's theorem.
To prove the theorem we prove the following simpler lemma.

\begin{lemma} \label{IVT:lemma}
Let $f \colon [a,b] \to \R$ be a continuous function.
Suppose $f(a) < 0$ and $f(b) > 0$. 
Then there exists a number $c \in (a,b)$
such that $f(c) = 0$.
\end{lemma}

\begin{proof}
We define two sequences $\{ a_n \}$ and $\{ b_n \}$ inductively:
\begin{enumerate}[(i)]
\item Let $a_0 := a$ and $b_0 := b$.
\item If $f\left(\frac{a_n+b_n}{2}\right) \geq 0$, let $a_{n+1} := a_n$ and
$b_{n+1} := \frac{a_n+b_n}{2}$.
\item If $f\left(\frac{a_n+b_n}{2}\right) < 0$, let $a_{n+1} := \frac{a_n+b_n}{2}$ and
$b_{n+1} := b_n$.
\end{enumerate}
\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input bisect.eepic
\caption{Finding roots (bisection method).\label{bisectfig}}
\end{center}
\end{figure}
See \figureref{bisectfig} for an example defining the first five steps.
From the definition of the two sequences it is obvious that if $a_n < b_n$,
then $a_{n+1} < b_{n+1}$.
Thus by \hyperref[induction:thm]{induction} $a_n < b_n$ for all $n$.
Furthermore, $a_n \leq a_{n+1}$ and 
$b_n \geq b_{n+1}$ for all $n$, that is the sequences are monotone.
As $a_n < b_n \leq b_1 = b$ and 
$b_n > a_n \geq a_1 = a$ for all $n$,
the sequences are also bounded.
Therefore, the
sequences converge.
Let $c := \lim\, a_n$ and $d := \lim\, b_n$.

We now want to show that $c=d$.
Notice
\begin{equation*}
b_{n+1} - a_{n+1} = \frac{b_n-a_n}{2}.
\end{equation*}
By \hyperref[induction:thm]{induction} we see that
\begin{equation*}
b_n - a_n = \frac{b-a}{2^n}.
\end{equation*}
As $\tfrac{b-a}{2^n}$ converges to zero, we take the limit as $n$ goes to infinity to get
\begin{equation*}
d-c = \lim_{n\to\infty} (b_n - a_n) =
\lim_{n\to\infty} \frac{b-a}{2^n} = 0.
\end{equation*}
In other words $d=c$.


By construction, for all $n$ we have
\begin{equation*}
f(a_n) < 0
\qquad \text{and} \qquad
f(b_n) \geq 0 .
\end{equation*}
We use the fact that
$\lim\, a_n = \lim\, b_n = c$
and 
the continuity of $f$ to take limits in those inequalities to get
\begin{equation*}
f(c) = \lim\, f(a_n) \leq 0
\qquad \text{and} \qquad
f(c) = \lim\, f(b_n) \geq 0 .
\end{equation*}
As $f(c) \geq 0$ and 
$f(c) \leq 0$, we conclude $f(c) = 0$.
Obviously, $a < c < b$.
\end{proof}

Notice that
the proof tells us how to find the $c$.
The
proof is not only useful for us pure mathematicians,
but it is a useful idea in applied mathematics.

\begin{thm}[intermediate value theorem] \label{IVT:thm}
\index{Bolzano's theorem}
\index{Bolzano's intermediate value theorem}
\index{intermediate value theorem}
Let $f \colon [a,b] \to \R$ be a continuous function.
Suppose there exists a $y$ such that $f(a) < y < f(b)$
or $f(a) > y > f(b)$.
Then there exists a $c \in (a,b)$
such that $f(c) = y$.
\end{thm}

The theorem says that a continuous function on a closed interval
achieves all the values between the values at the endpoints; see the example~\ref{bisection method} below.

\begin{proof}
If $f(a) < y < f(b)$, then define $g(x) := f(x)-y$.
Then we see
that $g(a) < 0$ and $g(b) > 0$ and we can apply \lemmaref{IVT:lemma}
to $g$.
If $g(c) = 0$, then $f(c) = y$.

Similarly if $f(a) > y > f(b)$, then define $g(x) := y-f(x)$.
Then
again $g(a) < 0$ and $g(b) > 0$ and we can apply \lemmaref{IVT:lemma}.
Again if $g(c) = 0$, then $f(c) = y$.
\end{proof}

\begin{exercise}
Find an example of a discontinuous function $f \colon [0,1] \to \R$
where the intermediate value theorem fails.
\end{exercise}

\begin{exercise}
Let
\begin{equation*}
f(x) :=
\begin{cases}
\sin(\nicefrac{1}{x}) & \text{ if $x \not= 0$,} \\
0 & \text{ if $x=0$.}
\end{cases}
\end{equation*}
Show that $f$ has the intermediate value property.
That is, for any $a < b$, if there exists a $y$ such that $f(a) < y < f(b)$
or $f(a) > y > f(b)$, then
there exists a $c \in (a,b)$ such that $f(c) = y$.
\end{exercise}

\begin{exercise} \label{exercise:imageofinterval}
Suppose $f \colon [a,b] \to \R$ is a continuous function.
Prove that the direct image $f([a,b])$ is a closed and bounded interval or a single number.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a continuous function such that $x \leq f(x) \leq x+1$ for all $x \in \R$.
Show that $f(\R)=\R$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to [0,1]$ is continuous.
Show that $f$
has a fixed point, in other words, show that there exists an $x \in [0,1]$ such that
$f(x) = x$.
\end{exercise}

If a function is continuous, then its restriction to any subset is continuous.
So if $f \colon S \to \R$ is continuous and
$[a,b] \subset S$, then $f|_{[a,b]}$ is also continuous.
Hence, we generally
apply the theorem to a function continuous on some large set $S$,
but we restrict attention to an interval.

\begin{example}[Bisection method]\index{bisection method}\label{bisection method}
The polynomial $f(x) := x^3-2x^2+x-1$ has a real root in $(1,2)$.
We simply
notice that $f(1) = -1$ and $f(2) = 1$.
Hence there must exist a point $c
\in (1,2)$ such that $f(c) = 0$.
To find a better approximation of
the root we could follow the proof of \lemmaref{IVT:lemma}.
For example,
next we would look at 1.5 and find that $f(1.5) = -0.625$.
Therefore,
there is a root of the equation in $(1.5,2)$.
Next we look at 1.75
and note that $f(1.75) \approx -0.016$.
Hence there is a root of $f$ in
$(1.75,2)$.
Next we look at 1.875 and find that $f(1.875) \approx 0.44$,
thus there is a root in $(1.75,1.875)$.
We follow this procedure until we gain
sufficient precision.
\end{example}

The technique above is the simplest method of finding roots of polynomials.
Finding roots of polynomials is perhaps the most common problem in applied
mathematics.
In general it is hard to do quickly, precisely
and automatically.
We can use the intermediate value theorem to find
roots for any continuous function, not just a polynomial.

There are better and faster methods of finding roots of equations, such
as Newton's method.
One advantage of the above method is its
simplicity.
The
moment we find an initial interval where the intermediate value theorem
applies, we are guaranteed to find a root up to a desired
precision in finitely many steps.
Furthermore, the method only requires
a continuous function.

The theorem guarantees at least one $c$ such that $f(c) = y$, but there
may be many different roots of the equation $f(c) = y$.
If we follow
the procedure of the proof, we are guaranteed to find approximations to
one such root.
We need to work harder to find any other roots.

Polynomials of even degree may not have any real roots.
For example,
there is no real number $x$ such that $x^2+1 = 0$.
Odd polynomials, on the
other hand, always have at least one real root.

\begin{prop}
Let $f(x)$ be a polynomial of odd degree.
Then $f$ has a real root.
\end{prop}

\begin{proof}
Suppose $f$ is a polynomial of odd degree $d$.
We write
\begin{equation*}
f(x) = a_d x^d + a_{d-1} x^{d-1} + \cdots + a_1 x + a_0 ,
\end{equation*}
where $a_d \not= 0$.
We divide by $a_d$ to obtain a polynomial
\begin{equation*}
g(x) := x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
where $b_k = \nicefrac{a_k}{a_d}$.
Let us show that $g(n)$ is
positive for some large $n \in \N$.
\begin{equation*}
\begin{split}
\abs{\frac{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}{n^d}}
& =
\frac{\abs{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}}{n^d}
\\
& \leq
\frac{\abs{b_{d-1}} n^{d-1} + \cdots + \abs{b_1} n + \abs{b_0}}{n^d}
\\
& \leq
\frac{\abs{b_{d-1}} n^{d-1} + \cdots + \abs{b_1} n^{d-1} + \abs{b_0} n^{d-1}}{n^d}
\\
& =
\frac{n^{d-1}\bigl(\abs{b_{d-1}} + \cdots + \abs{b_1} + \abs{b_0}\bigr)}{n^d}
\\
& =
\frac{1}{n}
\bigl(\abs{b_{d-1}} + \cdots + \abs{b_1} + \abs{b_0}\bigr) .
\end{split}
\end{equation*}
Therefore
\begin{equation*}
\lim_{n\to\infty} \frac{b_{d-1} n^{d-1} + \cdots + b_1 n + b_0}{n^d}
= 0 .
\end{equation*}
Thus there exists an $M \in \N$ such that 
\begin{equation*}
\abs{\frac{b_{d-1} M^{d-1} + \cdots + b_1 M + b_0}{M^d}} < 1 ,
\end{equation*}
which implies
\begin{equation*}
-(b_{d-1} M^{d-1} + \cdots + b_1 M + b_0) < M^d .
\end{equation*}
Therefore $g(M) > 0$.

Next we look at $g(-n)$ for $n \in \N$.
By a similar argument (exercise)
we find that there exists some $K \in \N$ such that
$b_{d-1} {(-K)}^{d-1} + \cdots + b_1 (-K) + b_0 < K^d$
and therefore $g(-K) < 0$ (why?).
In the
proof make sure you use the fact that $d$ is odd.
In particular, 
if $d$ is odd then ${(-n)}^d = -(n^d)$.

We appeal to the intermediate value theorem, to find a
$c \in [-K,M]$ such that $g(c) = 0$.
As $g(x) = \frac{f(x)}{a_d}$,
we see that $f(c) = 0$, and the proof is done.
\end{proof}


\begin{exercise}
Suppose $g(x)$ is a polynomial of positive even degree $d$ such that
\begin{equation*}
g(x) = x^d + b_{d-1} x^{d-1} + \cdots + b_1 x + b_0 ,
\end{equation*}
for some real numbers $b_{0}, b_1, \ldots, b_{d-1}$.
Suppose 
$g(0) < 0$.
Show that $g$ has at least two distinct real roots.
\end{exercise}

\begin{exercise}[Challenging]
Suppose $f(x)$ is a bounded polynomial,
in other words, there is an $M$ such that $\abs{f(x)} \leq M$
for all $x \in \R$.
Prove that $f$ must be a constant.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Uniform continuity}
\label{sec:unifcont}

\sectionnotes{1.5--2 lectures (Continuous extension and Lipschitz can be
optional)}

\subsection*{Uniform continuity}

We made a fuss of saying that the $\delta$ in the definition of
continuity depended on the point $c$.
There are situations when it is
advantageous to have a $\delta$ independent of any point.
Let
us give a name to this concept.

\begin{defn}
Let $S \subset \R$, and let $f \colon S \to \R$ be a function.
Suppose for any $\epsilon > 0$ there exists a $\delta > 0$
such that whenever $x, c \in S$ and
$\abs{x-c} < \delta$, then $\abs{f(x)-f(c)} < \epsilon$.
Then we say $f$ is \emph{\myindex{uniformly continuous}}.
\end{defn}

It is not hard to see that a uniformly continuous function
must be continuous.
The only difference in the definitions
is that for a given $\epsilon > 0$ we pick a $\delta > 0$ that
works for all $c \in S$.
That is, $\delta$ can no longer depend on $c$,
it only depends on $\epsilon$.
The domain of definition
of the function makes a difference now.
A function that is not uniformly
continuous on a larger set, may be uniformly continuous when restricted to a
smaller set.

\begin{example}
The function $f \colon (0,1) \to \R$, defined by $f(x) := \nicefrac{1}{x}$ is not
uniformly continuous, but it is continuous.

\begin{proof} Given $\epsilon > 0$, then for $\epsilon >
\abs{\nicefrac{1}{x}-\nicefrac{1}{y}}$ to hold we must have
\begin{equation*}
\epsilon >
\abs{\nicefrac{1}{x}-\nicefrac{1}{y}}
=
\frac{\abs{y-x}}{\abs{xy}} 
=
\frac{\abs{y-x}}{xy} ,
\end{equation*}
or
\begin{equation*}
\abs{x-y} < xy \epsilon .
\end{equation*}
Therefore, to satisfy the definition of uniform continuity we would have to
have $\delta \leq xy \epsilon$ for all $x,y$ in $(0,1)$, but that would mean
that $\delta \leq 0$.
Therefore there is no single $\delta > 0$.
\end{proof}
\end{example}

\begin{example}
$f \colon [0,1] \to \R$, defined by $f(x) := x^2$ is uniformly continuous.

\begin{proof} Note that $0 \leq x,c \leq 1$.
Then
\begin{equation*}
\abs{x^2-c^2} = \abs{x+c}\abs{x-c}
\leq (\abs{x}+\abs{c}) \abs{x-c}
\leq (1+1)\abs{x-c} .
\end{equation*}
Therefore given $\epsilon > 0$, let $\delta := \nicefrac{\epsilon}{2}$.
If $\abs{x-c} < \delta$, then $\abs{x^2-c^2} < \epsilon$.
\end{proof}

On the other hand, $f \colon \R \to \R$, defined by $f(x) := x^2$ is not uniformly
continuous.

\begin{proof} Suppose it is uniformly continuous, then for all $\epsilon > 0$,
there would exist a $\delta > 0$ such that
if $\abs{x-c} < \delta$, then $\abs{x^2 -c^2} < \epsilon$.
Take $x > 0$ and let
$c := x+\nicefrac{\delta}{2}$.
Write
\begin{equation*}
\epsilon >
\abs{x^2-c^2} = \abs{x+c}\abs{x-c}
=
(2x+\nicefrac{\delta}{2})\nicefrac{\delta}{2} 
\geq 
\delta x .
\end{equation*}
Therefore $x < \nicefrac{\epsilon}{\delta}$ for all $x > 0$, which is a
contradiction.
\end{proof}
\end{example}

We have seen that if $f$ is defined on an interval that is either not closed
or not bounded, then $f$ can be continuous, but not uniformly continuous.
For a closed and bounded interval $[a,b]$, we can, however,
make the following statement.

\begin{thm} \label{unifcont:thm}
Let $f \colon [a,b] \to \R$ be a continuous function.
Then $f$
is uniformly continuous.
\end{thm}

\begin{proof}
We prove the statement by contrapositive.
Suppose $f$ is not uniformly continuous.
We will prove
that there is some
$c \in [a,b]$ where $f$ is not continuous.
Let us negate
the definition of uniformly continuous.
There exists an $\epsilon > 0$
such that for every $\delta > 0$, there exist points $x, y$ in $S$ with
$\abs{x-y} < \delta$ and $\abs{f(x)-f(y)} \geq \epsilon$.

So for the $\epsilon > 0$ above,
we find sequences $\{ x_n \}$ and $\{ y_n \}$ such that
$\abs{x_n-y_n} < \nicefrac{1}{n}$ and such that $\abs{f(x_n)-f(y_n)} \geq
\epsilon$.
By
\hyperref[thm:bwseq]{Bolzano--Weierstrass},
there exists a convergent subsequence
$\{ x_{n_k} \}$.
Let $c := \lim\, x_{n_k}$.
As $a \leq x_{n_k} \leq b$, then $a \leq c \leq b$.
Write
\begin{equation*}
\abs{y_{n_k} - c} =
\abs{y_{n_k} - x_{n_k} + x_{n_k} - c} \leq
\abs{y_{n_k} - x_{n_k}}
+
\abs{x_{n_k}-c}
<
\nicefrac{1}{n_k} 
+
\abs{x_{n_k}-c} .
\end{equation*}
As $\nicefrac{1}{n_k}$ and $\abs{x_{n_k}-c}$ both go to zero when
$k$ goes to infinity, $\{ y_{n_k} \}$ converges and the limit
is $c$.
We now show that $f$ is not continuous at $c$.
We
estimate
\begin{equation*}
\begin{split}
\abs{f(x_{n_k}) - f(c)} & =
\abs{f(x_{n_k}) - f(y_{n_k}) + f(y_{n_k}) - f(c)} \\
& \geq
\abs{f(x_{n_k}) - f(y_{n_k})} - \abs{f(y_{n_k}) - f(c)} \\
& \geq
\epsilon - \abs{f(y_{n_k})-f(c)} .
\end{split}
\end{equation*}
Or in other words
\begin{equation*}
\abs{f(x_{n_k})-f(c)} 
+
\abs{f(y_{n_k})-f(c)}  \geq
\epsilon .
\end{equation*}
At least one of the sequences $\{ f(x_{n_k}) \}$  or
$\{ f(y_{n_k}) \}$ cannot converge to $f(c)$, otherwise the left
hand side of the inequality would go to zero while the right-hand side is positive.
Thus $f$ cannot be continuous at $c$.
\end{proof}

\subsection*{Continuous extension}

Before we get to continuous extension, we show the following useful lemma.
It says that uniformly continuous functions behave nicely with respect
to Cauchy sequences.
The new issue here is that for a Cauchy sequence
we no longer know where the limit ends up; it may not end up in the domain
of the function.

\begin{lemma} \label{unifcauchycauchy:lemma}
Let $f \colon S \to \R$ be a uniformly continuous function.
Let
$\{ x_n \}$ be a Cauchy sequence in $S$.
Then $\{ f(x_n) \}$ is Cauchy.
\end{lemma}

\begin{proof}
Let $\epsilon > 0$ be given.
Then there is a $\delta > 0$ such that
$\abs{f(x)-f(y)} < \epsilon$ whenever $\abs{x-y} < \delta$.
Now find an $M
\in \N$ such that for all $n, k \geq M$ we have $\abs{x_n-x_k} < \delta$.
Then for all $n, k \geq M$ we have $\abs{f(x_n)-f(x_k)} < \epsilon$.
\end{proof}

An application of the above lemma is the following theorem.
It says that
a function on an open interval is uniformly continuous if and only if
it can be extended to a continuous function on the closed interval.

\begin{thm} \label{context:thm}
A function $f \colon (a,b) \to \R$ is uniformly continuous if and only if
the limits 
\begin{equation*}
L_a := \lim_{x \to a} f(x) \qquad \text{and} \qquad
L_b := \lim_{x \to b} f(x)
\end{equation*}
exist and the function $\widetilde{f} \colon [a,b] \to \R$
defined by
\begin{equation*}
\widetilde{f}(x) :=
\begin{cases}
f(x) & \text{ if $x \in (a,b)$,} \\
L_a & \text{ if $x = a$,} \\
L_b & \text{ if $x = b$,}
\end{cases}
\end{equation*}
is continuous.
\end{thm}

\begin{proof}
One direction is not hard to prove.
If $\widetilde{f}$ is continuous, then
it is uniformly continuous by \thmref{unifcont:thm}.
As $f$ is the
restriction of $\widetilde{f}$ to $(a,b)$, then $f$ is also uniformly continuous
(easy exercise).

Now suppose $f$ is uniformly continuous.
We must first show
that the limits $L_a$ and $L_b$ exist.
Let us concentrate on $L_a$.
Take a sequence $\{ x_n \}$ in $(a,b)$ such that $\lim\, x_n = a$.
The sequence is a Cauchy sequence and hence by
\lemmaref{unifcauchycauchy:lemma},
the sequence $\{ f(x_n) \}$ is Cauchy and therefore convergent.
We have some number $L_1 := \lim\, f(x_n)$.
Take another sequence
$\{ y_n \}$ in $(a,b)$ such that $\lim\, y_n = a$.
By the same reasoning
we get $L_2 := \lim\, f(y_n)$.
If we show that $L_1 = L_2$, then
the limit $L_a = \lim_{x\to a} f(x)$ exists.
Let $\epsilon > 0$ be given,
find $\delta > 0$ such that $\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} <
\nicefrac{\epsilon}{3}$.
Find $M \in \N$ such that for
$n \geq M$ we have $\abs{a-x_n} < \nicefrac{\delta}{2}$,
$\abs{a-y_n} < \nicefrac{\delta}{2}$,
$\abs{f(x_n)-L_1} < \nicefrac{\epsilon}{3}$, and
$\abs{f(y_n)-L_2} < \nicefrac{\epsilon}{3}$.
Then for $n \geq M$ we have
\begin{equation*}
\abs{x_n-y_n} = 
\abs{x_n-a+a-y_n} \leq
\abs{x_n-a}+\abs{a-y_n} < \nicefrac{\delta}{2} + \nicefrac{\delta}{2} =
\delta.
\end{equation*}
So
\begin{equation*}
\begin{split}
\abs{L_1-L_2} &=
\abs{L_1-f(x_n)+f(x_n)-f(y_n)+f(y_n)-L_2} \\
& \leq 
\abs{L_1-f(x_n)}+\abs{f(x_n)-f(y_n)}+\abs{f(y_n)-L_2} \\
& \leq
\nicefrac{\epsilon}{3} + \nicefrac{\epsilon}{3} + \nicefrac{\epsilon}{3}
=
\epsilon .
\end{split}
\end{equation*}
Therefore $L_1 = L_2$.
Thus $L_a$ exists.
To show that $L_b$ exists is left as an exercise.

Now that we know that the
limits $L_a$ and $L_b$ exist, we are done.
If $\lim_{x\to a} f(x)$
exists, then $\lim_{x\to a} \widetilde{f}(x)$ exists
(See \propref{prop:limrest}).
Similarly with $L_b$.
Hence $\widetilde{f}$ is continuous at $a$ and $b$.

And since $f$ is continuous at $c \in (a,b)$, then
$\widetilde{f}$ is continuous at $c \in (a,b)$.
\end{proof}

\subsection*{Lipschitz continuous functions}

\begin{defn}
Let $f \colon S \to \R$ be a function such that there exists a number $K$
such that for all $x$ and $y$ in $S$ we have
\begin{equation*}
\abs{f(x)-f(y)} \leq K \abs{x-y} .
\end{equation*}
Then $f$ is said to be \emph{\myindex{Lipschitz continuous}}%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Rudolf_Lipschitz}{Rudolf Otto Sigismund Lipschitz}
(1832--1903).}.
\end{defn}

A large class of functions is Lipschitz continuous.
Be careful, just as
for uniformly continuous functions, the
domain of definition of the function is important.
See the examples below
and the exercises.
First we justify the use of the word \emph{continuous}.

\begin{prop}
A Lipschitz continuous function is uniformly continuous.
\end{prop}

\begin{proof}
Let $f \colon S \to \R$ be a function and let $K$ be a constant such that
for all $x, y$ in $S$ we have
$\abs{f(x)-f(y)} \leq K \abs{x-y}$.

Let $\epsilon > 0$ be given.
Take $\delta :=
\nicefrac{\epsilon}{K}$.
For any $x$ and $y$ in $S$ such that
$\abs{x-y} < \delta$
we have that
\begin{equation*}
\abs{f(x)-f(y)} \leq K \abs{x-y} < K \delta = K \frac{\epsilon}{K} =
\epsilon .
\end{equation*}
Therefore $f$ is uniformly continuous.
\end{proof}

We interpret Lipschitz continuity geometrically.
If $f$ is a Lipschitz
continuous function with some constant $K$.
We rewrite the inequality 
to say that for $x \not=y$ we have
\begin{equation*}
\abs{\frac{f(x)-f(y)}{x-y}} \leq K .
\end{equation*}
The quantity $\frac{f(x)-f(y)}{x-y}$ is the slope of the line
between the points $\bigl(x,f(x)\bigr)$
and $\bigl(y,f(y)\bigr)$, that is, a \emph{\myindex{secant line}}.
Therefore, $f$ is Lipschitz
continuous if and only if every line that intersects the graph of $f$ in at least two
distinct
points has slope less than or equal to $K$.
See \figureref{fig:lipschitz}.
\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input lipschitzfig.eepic
\caption{Illustration of the slope of a secant line.
The function is Lipschitz if $\abs{\text{slope}} =
\abs{\frac{f(x)-f(y)}{x-y}} \leq K$ for all $x$ and $y$.\label{fig:lipschitz}}
\end{center}
\end{figure}

\begin{example}
The functions $\sin(x)$ and $\cos(x)$ are Lipschitz continuous.
We have seen (\exampleref{sincos:example}) the following two inequalities.
\begin{equation*}
\abs{\sin(x)-\sin(y)} 
\leq \abs{x-y}
\qquad \text{and} \qquad
\abs{\cos(x)-\cos(y)}
\leq \abs{x-y} .
\end{equation*}

Hence sin and cos are Lipschitz continuous with $K=1$.
\end{example}

\begin{example}
The function $f \colon [1,\infty) \to \R$ defined by $f(x) := \sqrt{x}$
is Lipschitz continuous. 

\begin{proof}
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} = 
\abs{\frac{x-y}{\sqrt{x}+\sqrt{y}}}
=
\frac{\abs{x-y}}{\sqrt{x}+\sqrt{y}} .
\end{equation*}
As $x \geq 1$ and $y \geq 1$, we see that $\frac{1}{\sqrt{x}+\sqrt{y}}
\leq \frac{1}{2}$.
Therefore
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} = 
\abs{\frac{x-y}{\sqrt{x}+\sqrt{y}}}
\leq
\frac{1}{2}
\abs{x-y}.
\end{equation*}
\end{proof}

On the other hand $f \colon [0,\infty) \to \R$ defined by
$f(x) := \sqrt{x}$ is not Lipschitz continuous.
Let us see why:
Suppose we have
\begin{equation*}
\abs{\sqrt{x}-\sqrt{y}} 
\leq
K \abs{x-y} ,
\end{equation*}
for some $K$.
Let $y=0$ to obtain
$\sqrt{x} \leq K x$.
 If $K > 0$, then for $x > 0$ we then get
$\nicefrac{1}{K} \leq \sqrt{x}$.
This cannot possibly be true for all
$x > 0$.
Thus no such $K > 0$ exists and $f$ is not
Lipschitz continuous.

The last example is a function that is uniformly
continuous but not Lipschitz continuous.
To see that $\sqrt{x}$
is
uniformly continuous on $[0,\infty)$ note that it is uniformly continuous on
$[0,1]$ by \thmref{unifcont:thm}.
It is also Lipschitz (and
therefore uniformly continuous) on $[1,\infty)$.
It is not hard (exercise)
to show that this means that $\sqrt{x}$ is uniformly continuous on
$[0,\infty)$.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Let $f \colon S \to \R$ be uniformly continuous.
Let $A \subset S$.
Then the restriction $f|_A$ is uniformly continuous.
\end{exercise}

\begin{exercise}
Let $f \colon (a,b) \to \R$ be a uniformly continuous function.
Finish the proof of \thmref{context:thm} by showing that
the limit
$\lim\limits_{x \to b} f(x)$
exists.
\end{exercise}

\begin{exercise}
Show that $f \colon (c,\infty) \to \R$ for some $c > 0$
and defined by $f(x) := \nicefrac{1}{x}$ is Lipschitz continuous.
\end{exercise}

\begin{exercise}
Show that $f \colon (0,\infty) \to \R$
defined by $f(x) := \nicefrac{1}{x}$ is not Lipschitz continuous.
\end{exercise}

\begin{exercise}
Let $A, B$ be intervals.
Let $f \colon A \to \R$ and $g \colon B \to \R$ be uniformly continuous
functions such that $f(x) = g(x)$ for $x \in A \cap B$.
Define
the function $h \colon A \cup B \to \R$ by $h(x) := f(x)$ if
$x \in A$ and $h(x) := g(x)$ if $x \in B \setminus A$.
a) Prove that if $A \cap B \not= \emptyset$, then $h$ is uniformly continuous.
b) Find an example where $A \cap B = \emptyset$ and $h$ is not even
continuous.
\end{exercise}

\begin{exercise}[Challenging]
Let $f \colon \R \to \R$ be a polynomial of degree 
$d \geq 2$.
Show that $f$ is not Lipschitz
continuous.
\end{exercise}

\begin{exercise}
Let $f \colon (0,1) \to \R$ be a bounded continuous function.
Show that
the function
$g(x) := x(1-x)f(x)$ is uniformly continuous.
\end{exercise}

\begin{exercise}
Show that $f \colon (0,\infty) \to \R$ defined by $f(x) := \sin
(\nicefrac{1}{x})$ is not uniformly continuous.
\end{exercise}

\begin{exercise}[Challenging]
Let $f \colon \Q \to \R$ be a uniformly continuous function.
Show that
there exists a uniformly continuous function $\widetilde{f} \colon \R \to \R$
such that $f(x) = \widetilde{f}(x)$ for all $x \in \Q$.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Find a continuous $f \colon (0,1) \to \R$ and a sequence $\{ x_n \}$ in
$(0,1)$ that is Cauchy, but such that $\{ f(x_n) \}$ is not Cauchy.
 \item Prove that if $f \colon \R \to \R$ is continuous, and $\{ x_n \}$ is
Cauchy, then $\{ f(x_n) \}$ is Cauchy.
\end{enumerate}
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item If $f \colon S \to \R$ and $g \colon S \to \R$ are uniformly continuous,
then show that $h \colon S \to \R$ given by $h(x) := f(x) + g(x)$
is uniformly continuous.
  \item If $f \colon S \to \R$ is uniformly continuous and $a \in \R$,
then show that $h \colon S \to \R$ given by $h(x) := a f(x)$
is uniformly continuous.
\end{enumerate}
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item If $f \colon S \to \R$ and $g \colon S \to \R$ are Lipschitz,
then show that $h \colon S \to \R$ given by $h(x) := f(x) + g(x)$
is Lipschitz.
  \item If $f \colon S \to \R$ is Lipschitz and $a \in \R$,
then show that $h \colon S \to \R$ given by $h(x) := a f(x)$
is Lipschitz.
\end{enumerate}
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item If $f \colon [0,1] \to \R$ is given by $f(x) := x^m$ for an integer
$m \geq 0$,
show $f$ is Lipschitz and find the best (the smallest) Lipschitz constant
$K$ (depending on $m$ of course).
Hint: $(x-y)(x^{m-1} + x^{m-2}y + x^{m-3}y^2 + \cdots + x y^{m-2} + y^{m-1}) = x^m - y^m$.
 \item Using the previous exercise, show that if $f \colon [0,1] \to \R$
is a polynomial, that is, $f(x) := a_m x^m + a_{m-1} x^{m-1} + \cdots + a_0$,
then $f$ is Lipschitz.
\end{enumerate}
\end{exercise}

\begin{exercise}
Suppose for $f \colon [0,1] \to \R$ we have $\abs{f(x)-f(y)} \leq K \abs{x-y}$,
and $f(0) = f(1) = 0$.
Prove that $\abs{f(x)} \leq \nicefrac{K}{2}$.
Further show by example that
$\nicefrac{K}{2}$ is the best possible, that is, there exists such a continuous function
for which $\abs{f(x)} = \nicefrac{K}{2}$ for some $x \in [0,1]$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Limits at infinity}
\label{sec:limitatinf}

\sectionnotes{less than 1 lecture (optional, can safely be omitted unless
\sectionref{sec:monotonefunc} or
\sectionref{sec:impropriemann} is also covered)}

\subsection*{Limits at infinity}

As for sequences, a continuous variable can also approach infinity.
Let us make this notion precise.

\begin{defn}
We say $\infty$ is a cluster point of $S \subset \R$, if for every
$M \in \R$, there exists an $x \in S$ such that $x \geq M$.
Similarly
$- \infty$ is a cluster point of $S \subset \R$, if for every
$M \in \R$, there exists an $x \in S$ such that $x \leq M$.

\index{limit of a function at infinity}%
Let $f \colon S \to \R$ be a function, where 
$\infty$ is a cluster point of $S$.
If there exists an $L \in \R$
such that for every $\epsilon > 0$, there is an $M \in \R$ such that
\begin{equation*}
\abs{f(x) - L} < \epsilon 
\end{equation*}
whenever $x \geq M$, then we say $f(x)$ \emph{\myindex{converges}} to $L$
as $x$ goes to $\infty$.
We call $L$ the \emph{\myindex{limit}} and write
\begin{equation*}
\lim_{x \to \infty} f(x) := L .
\end{equation*}
Alternatively we write $f(x) \to L$ as $x \to \infty$.

Similarly, if $-\infty$ is a cluster point of $S$
and
there exists an $L \in \R$
such that for every $\epsilon > 0$, there is an $M \in \R$ such that
\begin{equation*}
\abs{f(x) - L} < \epsilon 
\end{equation*}
whenever $x \leq M$, then we say $f(x)$ \emph{converges} to $L$
as $x$ goes to $-\infty$.
We call $L$ the \emph{limit} and write
\begin{equation*}
\lim_{x \to -\infty} f(x) := L .
\end{equation*}
Alternatively we write $f(x) \to L$ as $x \to -\infty$.
\end{defn}

We cheated a little bit again and said \emph{the} limit.
We leave it as an exercise for the reader to prove the following proposition.

\begin{prop} \label{liminfty:unique}
The limit at $\infty$ or $-\infty$ as defined above is unique if it exists.
\end{prop}

\begin{example}
Let $f(x) := \frac{1}{\abs{x}+1}$.
Then
\begin{equation*}
\lim_{x\to \infty} f(x) = 0 \qquad \text{and} \qquad
\lim_{x\to -\infty} f(x) = 0 .
\end{equation*}

\begin{proof}
Let $\epsilon > 0$ be given.
Find $M > 0$ large enough
so that $\frac{1}{M+1} < \epsilon$.
If
$x \geq M$, then $\frac{1}{x+1} \leq \frac{1}{M+1} < \epsilon$.
Since $\frac{1}{\abs{x}+1} > 0$ for all $x$ the first limit is proved.
The proof for $-\infty$ is left to the reader.
\end{proof}
\end{example}

\begin{example}
Let $f(x) := \sin(\pi x)$.
Then $\lim_{x\to\infty} f(x)$ does not exist.
To prove this fact note that if $x = 2n+\nicefrac{1}{2}$ for some $n \in \N$ then $f(x)=1$,
while if $x = 2n+\nicefrac{3}{2}$ then $f(x)=-1$, so they cannot both be
within a small $\epsilon$ of a single real number.

We must be careful not to confuse continuous limits with limits of sequences.
For $f(x) = \sin(\pi x)$ we could say
\begin{equation*}
\lim_{n \to \infty} f(n) = 0, \qquad \text{but} \qquad
\lim_{x \to \infty} f(x) ~ \text{does not exist}.
\end{equation*}
Of course the notation is ambiguous.
We are simply using the convention
that $n \in \N$, while $x \in \R$.
When the notation is not clear,
it is good to explicitly mention where the variable lives, or what kind
of limit are you using.
\end{example}

There is a connection of continuous limits to limits of sequences, but we must take all
sequences going to infinity, just as before in \lemmaref{seqflimit:lemma}.

\begin{lemma} \label{seqflimitinf:lemma}
Suppose $f \colon S \to \R$ is a function, $\infty$ is a cluster
point of $S \subset \R$, and $L \in \R$.
Then
\begin{equation*}
\lim_{x\to\infty} f(x) = L
% \qquad \text{if and only if} \qquad
\end{equation*}
if and only if
\begin{equation*}
\lim_{n\to\infty} f(x_n) = L% ~~\text{for all sequences $\{ x_n \}$ such that $\lim\, x_n = \infty$} .
\end{equation*}
for all sequences $\{ x_n \}$ such that $\lim\limits_{n\to\infty} x_n = \infty$.
\end{lemma}

The lemma holds for the limit as $x \to -\infty$.
Its proof is almost identical and
is left as an exercise.

\begin{proof}
First suppose $f(x) \to L$ as $x \to \infty$.
Given an $\epsilon > 0$, there exists an $M$ such that for all $x \geq M$
we have $\abs{f(x)-L} < \epsilon$.
Let $\{ x_n \}$
be a sequence in $S$ such that $\lim \, x_n = \infty$.
Then there exists an
$N$ such that for all $n \geq N$ we have $x_n \geq M$.
And thus
$\abs{f(x_n)-L} < \epsilon$.

We prove the converse by contrapositive.
Suppose $f(x)$ does
not go to $L$ as $x \to \infty$.
This means that there exists an $\epsilon > 0$,
such that for every $M \in \N$, there exists an $x \in S$, $x \geq M$, let
us call it $x_M$, such that $\abs{f(x_M)-L} \geq \epsilon$.
Consider the sequence $\{ x_n \}$.
Clearly 
$\{ f(x_n) \}$ does not converge to $L$.
It remains to note
that $\lim\, x_n = \infty$, because $x_n \geq n$ for all $n$.
\end{proof}

Using the lemma, we again translate results about sequential
limits into results about continuous limits as $x$ goes to infinity.
That
is, we have almost immediate analogues of the corollaries
in \sectionref{subseq:sequentiallimits}.
We simply allow 
the cluster point $c$ to be either $\infty$ or $-\infty$, in addition
to a real number.
We leave it to
the student to verify these statements.

\subsection*{Infinite limit}

Just as for sequences, it is often convenient to distinguish certain
divergent sequences, and talk about limits being infinite
almost as if the limits existed.

\begin{defn}
\index{infinite limit of a function}%
Let $f \colon S \to \R$ be a function and suppose 
$S$ has $\infty$ as a cluster point.
We say $f(x)$
\emph{\myindex{diverges to infinity}} 
as $x$ goes to $\infty$,
if for every $N \in \R$
there exists an $M \in \R$ such that
\begin{equation*}
f(x) > N
\end{equation*}
whenever $x \in S$ and $x \geq M$.
We write
\begin{equation*}
\lim_{x \to \infty} f(x) := \infty ,
\end{equation*}
or we say that $f(x) \to \infty$ as $x \to \infty$.
\end{defn}

A similar definition can be made for limits as $x \to -\infty$
or as $x \to c$ for a finite $c$.
Also similar definitions can be
made for limits being $-\infty$.
Stating these definitions is left
as an exercise.
Note that
sometimes \emph{\myindex{converges to infinity}} is used.
We can again use sequential limits, and an analogue of 
\lemmaref{seqflimit:lemma} is left as an exercise.

\begin{example}
Let us show that $\lim_{x \to \infty} \frac{1+x^2}{1+x} = \infty$.

\begin{proof} For $x \geq 1$ we have
\begin{equation*}
\frac{1+x^2}{1+x} \geq 
\frac{x^2}{x+x}  = 
\frac{x}{2} .
\end{equation*}
Given $N \in \R$, take $M = \max \{ 2N+1 , 1 \}$.
If $x \geq M$, then $x \geq 1$ and $\nicefrac{x}{2} > N$.
So
\begin{equation*}
\frac{1+x^2}{1+x} \geq 
\frac{x}{2} > N .
\end{equation*}
\end{proof}
\end{example}

\subsection*{Compositions}

Finally, just as for limits at finite numbers we can compose functions
easily.

\begin{prop} \label{prop:inflimcompositions}
Suppose $f \colon A \to B$, $g \colon B \to \R$, $A, B \subset \R$, 
$a \in \R \cup \{ -\infty, \infty\}$ is a cluster point of $A$,
and $b \in \R \cup \{ -\infty, \infty\}$ is a cluster point of $B$.
Suppose 
\begin{equation*}
\lim_{x \to a} f(x) = b\qquad \text{and} \qquad \lim_{y \to b} g(y) = c
\end{equation*}
for some $c \in \R \cup \{ -\infty, \infty \}$.
If $b \in B$, then suppose $g(b) = c$.
Then
\begin{equation*}
\lim_{x \to a} g\bigl(f(x)\bigr) = c .
\end{equation*}
\end{prop}

The proof is straightforward, and left as an exercise.
We already
know the proposition when $a, b, c \in \R$, see Exercises
\ref{exercise:contlimitcomposition} and
\ref{exercise:contlimitbadcomposition}.
Again the requirement that $g$ is
continuous at $b$, if $b \in B$, is necessary.

\begin{example}
Let $h(x) := e^{-x^2+x}$.
Then
\begin{equation*}
\lim_{x\to \infty} h(x) = 0 .
\end{equation*}

\begin{proof}
The claim follows once we know
\begin{equation*}
\lim_{x\to \infty} -x^2+x = -\infty
\end{equation*}
and
\begin{equation*}
\lim_{y\to -\infty} e^y = 0 ,
\end{equation*}
which is usually proved when the exponential function is defined.
\end{proof}
\end{example}

\subsection*{Exercises}

\begin{exercise}
Prove \propref{liminfty:unique}.
\end{exercise}

\begin{exercise}
Let $f \colon [1,\infty) \to \R$ be a function.
Define
$g \colon (0,1] \to \R$ via $g(x) := f(\nicefrac{1}{x})$.
Using the definitions of limits directly,
show that $\lim_{x\to 0^+} g(x)$
exists if and only if $\lim_{x\to \infty} f(x)$ exists, in which
case they are equal.
\end{exercise}

\begin{exercise}
Prove \propref{prop:inflimcompositions}.
\end{exercise}

\begin{exercise}
Let us justify terminology.
Let $f \colon \R \to \R$ be a function such that
$\lim_{x \to \infty} f(x) = \infty$ (diverges to infinity).
Show that $f(x)$ diverges (i.e.\ does not converge) as $x \to \infty$.
\end{exercise}

\begin{exercise}
Come up with the definitions for limits of $f(x)$ going to $-\infty$ as $x \to
\infty$, $x \to -\infty$, and as $x \to c$ for a finite $c \in \R$.
Then state the definitions for limits of $f(x)$ going to $\infty$ 
as $x \to -\infty$, and as $x \to c$ for a finite $c \in \R$.
\end{exercise}

\begin{exercise}
Suppose $P(x) := x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0$ is a \emph{\myindex{monic polynomial}}
of degree $n \geq 1$ (monic means that the coefficient of $x^n$ is 1). 
\begin{enumerate}[a)]
 \item Show that if $n$ is even then $\lim_{x\to\infty} P(x) = 
\lim_{x\to-\infty} P(x) = \infty$.
 \item Show that if $n$ is odd then
$\lim_{x\to\infty} P(x) = \infty$ and
$\lim_{x\to-\infty} P(x) = -\infty$ (see previous exercise).
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $\{ x_n \}$ be a sequence.
Consider $S := \N \subset \R$, and
$f \colon S \to \R$ defined by $f(n) := x_n$.
Show that
the two notions of limit,
\begin{equation*}
\lim_{n\to\infty} x_n \qquad \text{and} \qquad
\lim_{x\to\infty} f(x) 
\end{equation*}
are equivalent.
That is, show that if one exists so does
the other one, and in this case they are equal.
\end{exercise}

\begin{exercise}
Extend \lemmaref{seqflimitinf:lemma} as follows.
Suppose $S \subset \R$ has a cluster point $c \in \R$, $c = \infty$,
or $c = -\infty$.
Let $f \colon S \to \R$ be a function and let
$L = \infty$ or $L = -\infty$.
Show that
\begin{equation*}
\lim_{x\to c} f(x) = L \qquad \text{if and only if} \qquad
\lim_{n\to\infty} f(x_n) = L ~~\text{for all sequences $\{ x_n \}$ such that $\lim\, x_n =
c$} .
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Monotone functions and continuity}
\label{sec:monotonefunc}

\sectionnotes{1 lecture (optional, can safely be omitted unless
\sectionref{sec:ift} is also covered, requires \sectionref{sec:limitatinf})}

\begin{defn}
Let $S \subset \R$.
We say $f \colon S \to \R$ is \emph{\myindex{increasing}}
(resp.\  \emph{\myindex{strictly increasing}}) if $x,y \in S$ with
$x < y$ implies $f(x) \leq f(y)$ (resp.\ $f(x) < f(y)$).
We define
\emph{\myindex{decreasing}} and
\emph{\myindex{strictly decreasing}} in the same way by switching the
inequalities for $f$.

If a function is either increasing or decreasing we say it is
\emph{monotone}\index{monotone function}.
If it is
strictly increasing or strictly decreasing we say it is
\emph{strictly monotone}\index{strictly monotone function}.
\end{defn}

Sometimes \emph{\myindex{nondecreasing}}
(resp.\ \emph{\myindex{nonincreasing}}) is used
for increasing (resp.\ decreasing) function to emphasize it is not
strictly increasing (resp.\ strictly decreasing).

\subsection*{Continuity of monotone functions}

It is easy to compute one-sided limits for monotone functions.

\begin{prop} \label{prop:monotlimits}
Let $S \subset \R$, $c \in \R$, and $f \colon S \to \R$ be increasing.
If $c$ is a cluster point of $S \cap (-\infty,c)$, then
\begin{equation*}
\lim_{x \to c^-} f(x) = \sup \{ f(x) : x < c, x \in S \} ,
\end{equation*}
and if $c$ is a cluster point of $S \cap (c,\infty)$, then
\begin{equation*}
\lim_{x \to c^+} f(x) = \inf \{ f(x) : x > c, x \in S \} .
\end{equation*}

Similarly, if $f$ is decreasing and
$c$ is a cluster point of $S \cap (-\infty,c)$, then
\begin{equation*}
\lim_{x \to c^-} f(x) = \inf \{ f(x) : x < c, x \in S \} ,
\end{equation*}
and if $c$ is a cluster point of $S \cap (c,\infty)$, then
\begin{equation*}
\lim_{x \to c^+} f(x) = \sup \{ f(x) : x > c, x \in S \} .
\end{equation*}
\end{prop}

In particular all the one-sided limits exist whenever they make
sense.
If we from now on say that say a the left hand limit $x \to c^-$
exists we mean that $c$ is a cluster point of $S \cap (-\infty,c)$.

\begin{proof}
Let us assume $f$ is increasing, and we will show the first
equality.
The rest of the proof is very similar and is left as an
exercise.

Let $a := \sup \{ f(x) : x < c, x \in S \}$.
If $a = \infty$,
then for every $M \in \R$, there exists an $x_M$ such that $f(x_M) > M$. 
As $f$ is increasing, $f(x) \geq f(x_M) >  M$ for all $x \in S$ with $x > x_M$.
If
we take $\delta = c-x_M$ we obtain the definition of the limit going to
infinity.

So assume $a < \infty$.
Let $\epsilon > 0$ be given.
Because $a$ is the supremum,
there exists an $x_\epsilon < c$,
$x_\epsilon \in S$,
such that $f(x_\epsilon) > a-\epsilon$.
As $f$ is increasing,
if $x \in S$ and $x_\epsilon < x < c$, we have
$a-\epsilon < f(x_\epsilon) \leq f(x) \leq a$.
Let
$\delta := c-x_\epsilon$.
Then for $x \in S \cap (-\infty,c)$
with $\abs{x-c} < \delta$,
we have $\abs{f(x)-a} < \epsilon$.
\end{proof}

Suppose $f \colon S \to \R$, $c \in S$ and
that both one-sided limits exist.
Since $f(x) \leq f(c) \leq f(y)$
whenever $x < c < y$, taking the limits we obtain
\begin{equation*}
\lim_{x \to c^-} f(x) \leq f(c) \leq \lim_{x \to c^+} f(x) .
\end{equation*}
Then $f$ is continuous at $c$ if and only if both limits are equal
to each other (and hence equal to $f(c)$).
See also
\propref{prop:onesidedlimits}.
See \figureref{fig:figinccont} to get an idea of a what a discontinuity
looks like.

%The proposition has an easy corollary, the proof of which we
%leave to the reader.
%
%\begin{cor} \label{cor:contofmonotone}
%Let $S \subset \R$ and $f \colon S \to \R$ be increasing.
%If $c \in S$ is a cluster point of both 
%$S \cap (-\infty,c)$ and
%$S \cap (c,\infty)$, then
%\begin{equation*}
%\lim_{x \to c^-} f(x) \leq \lim_{x \to c^+} f(x) .
%\end{equation*}
%The function $f$ is continuous at $c$ if and only if equality is achieved above.
%
%If $c \in S$ is a cluster point of
%$S \cap (-\infty,c)$ but not of $S \cap (c,\infty)$ then
%\begin{equation*}
%\lim_{x \to c^-} f(x) \leq f(c) .
%\end{equation*}
%
%If $c \in S$ is a cluster point of
%$S \cap (c,\infty)$ but not of $S \cap (-\infty,c)$ then
%\begin{equation*}
%f(c) \leq \lim_{x \to c^+} f(x) .
%\end{equation*}
%
%The function $f$ is continuous at $c$ if and only if equality is achieved in
%the inequalities above or if $c$ is not a cluster point of $S$.
%
%If $f$ is decreasing instead, the same result holds with
%the inequality reversed.
%\end{cor}

\begin{cor} \label{cor:continterval}
If $I \subset \R$ is an interval and $f \colon I \to \R$ is 
monotone and not constant, then $f(I)$ is an interval if and only if $f$
is continuous.
\end{cor}

Assuming $f$ is not constant is to avoid the technicality
that $f(I)$ is a single point in that case; $f(I)$ is a single
point if and only if $f$ is constant.
A constant function is 
continuous.

\begin{proof}
If $f$ is continuous then $f(I)$ being an interval is a consequence of
\hyperref[IVT:thm]{intermediate value theorem}.
See also
\exerciseref{exercise:imageofinterval}.

Let us prove the reverse direction by contrapositive.
Suppose $f$ is not continuous at $c \in I$,
and that $c$ is not an endpoint of $I$.
Without loss of generality suppose $f$ is increasing.
Let
\begin{equation*}
a := \lim_{x \to c^-} f(x) = \sup \{ f(x) : x \in I, x < c \} ,
\qquad
b := \lim_{x \to c^+} f(x) = \inf \{ f(x) : x \in I, x > c \} .
\end{equation*}
As $c$ is a discontinuity, $a < b$.
If $x < c$, then $f(x) \leq a$, and
if $x > c$, then $f(x) \geq b$.
Therefore
any point
in $(a,b) \setminus \{ f(c) \}$ is not in $f(I)$.
However there exists $x_1 \in S$, $x_1 < c$ so
$f(x_1) \leq a$, and there exists $x_2 \in S$, $x_2 > c$
so $f(x_2) \geq b$.
Both $f(x_1)$ and $f(x_2)$ are in $f(I)$,
and so $f(I)$ is not an interval.
See \figureref{fig:figinccont}.

When $c \in I$ is an endpoint, the proof is similar and is left as an exercise.
\end{proof}

\begin{figure}[h!t]
\begin{center}
\input figinccont.pdf_t
\caption{Increasing function $f \colon I \to \R$ discontinuity at
$c$.\label{fig:figinccont}}
\end{center}
\end{figure}

A striking property of monotone functions is that they cannot have
too many discontinuities.

\begin{cor} \label{cor:monotcountcont}
Let $I \subset \R$ be an interval and
$f \colon I \to \R$ be monotone.
Then $f$ has at most
countably many discontinuities.
\end{cor}

\begin{proof}
Let $E \subset I$ be the set of all discontinuities
that are not endpoints of $I$.
As there are
only two endpoints, it is enough to show that $E$ is countable.
Without loss of generality, suppose $f$ is increasing.
We will define an injection $h \colon E \to \Q$.
For each $c \in E$
the one-sided limits of $f$ both exist as $c$ is not an endpoint.
Let
\begin{equation*}
a := \lim_{x \to c^-} f(x) = \sup \{ f(x) : x \in I, x < c \} ,
\qquad
b := \lim_{x \to c^+} f(x) = \inf \{ f(x) : x \in I, x > c \} .
\end{equation*}
As $c$ is a discontinuity, we have $a < b$.
There exists a rational number $q \in (a,b)$, so let $h(c) := q$.
Because $f$ is increasing, $q$ cannot correspond to any
other discontinuity, so after making this choice for all $c \in E$, we have
that $h$ is one-to-one (injective).
Therefore, $E$ is countable.
\end{proof}

\begin{example} \label{example:countdiscont}
By $\lfloor x \rfloor$ denote the largest integer less than or equal to $x$.
Define $f \colon [0,1] \to \R$ by
\begin{equation*}
f(x) :=
x +
\sum_{n=0}^{\lfloor 1/(1-x) \rfloor}
2^{-n} ,
\end{equation*}
for $x < 1$ and $f(1) = 3$.
It is left as an exercise to show that $f$ is strictly increasing, bounded, and
has a discontinuity at all points $1-\nicefrac{1}{k}$ for $k \in \N$.
In particular,
there are countably many discontinuities, but the function is bounded and
defined on a closed bounded interval.
\end{example}


\subsection*{Continuity of inverse functions}


A strictly monotone function $f$ is one-to-one (injective).
To see this
notice that if $x \not= y$ then we can assume $x < y$.
Then either $f(x) <
f(y)$ if $f$ is strictly increasing or $f(x) > f(y)$ if $f$ is strictly
decreasing, so $f(x) \not= f(y)$.
Hence, it
must have an inverse $f^{-1}$ defined on its range.

\begin{prop} \label{prop:invcont}
If $I \subset \R$ is an interval and $f \colon I \to \R$ is strictly
monotone.
Then the inverse $f^{-1} \colon f(I) \to I$ is continuous.
\end{prop}

\begin{proof}
Let us suppose $f$ is strictly increasing.
The proof is almost
identical for a strictly decreasing function.
Since $f$ is strictly increasing, so is $f^{-1}$.
That is, if $f(x) <
f(y)$, then we must have $x < y$ and therefore
$f^{-1}\bigl(f(x)\bigr) < f^{-1}\bigl(f(y)\bigr)$.

Take $c \in f(I)$.
If $c$ is not a cluster point of $f(I)$, then $f^{-1}$ is continuous at $c$
automatically.
So let $c$ be a cluster point of $f(I)$.
Suppose both of the following one-sided limits exist:
\begin{align*}
x_0 & := \lim_{y \to c^-} f^{-1}(y) =
\sup \{ f^{-1}(y) : y < c, y \in f(I) \}
=
\sup \{ x : f(x) < c, x \in I \} , \\
x_1 & := \lim_{y \to c^+} f^{-1}(y) =
\inf \{ f^{-1}(y) : y > c, y \in f(I) \}
=
\inf \{ x : f(x) > c, x \in I \} .
\end{align*}
We have $x_0 \leq x_1$ as $f^{-1}$ is increasing.
For all $x > x_0$ with $x \in I$, we have $f(x) \geq c$.
As $f$ is strictly increasing,
we must have $f(x) > c$ for all $x > x_0$, $x \in I$.
Therefore,
\begin{equation*}
\{ x : x > x_0, x \in I \} \subset \{ x : f(x) > c, x \in I \}.
\end{equation*}
The infimum of the left hand set is $x_0$ and the infimum of the right hand
set is $x_1$, so we obtain $x_0 \geq x_1$.
So $x_1 = x_0$, and $f^{-1}$ is continuous at $c$.

If one of the one-sided limits does not exist the argument is similar
and is left as an exercise.
\end{proof}

\begin{example}
The proposition does not require $f$ itself to be continuous.
For example, let
$f \colon \R \to \R$
\begin{equation*}
f(x) :=
\begin{cases}
x & \text{if $x < 0$}, \\
x+1 & \text{if $x \geq 0$}. \\
\end{cases}
\end{equation*}
The function $f$ is not continuous at $0$.
The image of $I = \R$ is the set 
$(-\infty,0)\cup [1,\infty)$, not an interval.
Then $f^{-1} \colon (-\infty,0)\cup [1,\infty)
\to \R$ can be written as
\begin{equation*}
f^{-1}(x) =
\begin{cases}
x & \text{if $x < 0$}, \\
x-1 & \text{if $x \geq 1$}. 
\end{cases}
\end{equation*}
It is not difficult to see that $f^{-1}$ is a continuous function.
\end{example}

Notice what happens with the proposition if $f(I)$ is an interval.
In that case we could simply
apply \corref{cor:continterval} to both $f$ and $f^{-1}$.
That is, if
$f \colon I \to J$ is an onto strictly monotone function and $I$ and $J$ are intervals,
then both $f$ and $f^{-1}$ are continuous.
Furthermore $f(I)$ is an
interval precisely when $f$ is continuous.

%\begin{cor} \label{cor:continterval}
%If $I \subset \R$ is an interval and $f \colon I \to \R$ is 
%strictly monotone, then $f(I)$ is an interval if and only if $f$
%is continuous.
%\end{cor}
%
%\begin{proof}
%If $f$ is continuous then $f(I)$ is an interval is a consequence of
%\hyperref[IVT:thm]{intermediate value theorem}.  See also
%\exerciseref{exercise:imageofinterval}.
%
%For the reverse direction, suppose that $f(I)$ is an interval.  Let
%$f^{-1} \colon f(I) \to I$ be
%its inverse.
%The function $f$ is the inverse of $f^{-1}$, which is itself strictly
%monotone, and so $f$ is continuous by \propref{prop:invcont}.  
%\end{proof}

\subsection*{Exercises}

\begin{exercise}
Suppose $f \colon [0,1] \to \R$ is monotone.
Prove $f$ is bounded.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:monotlimits}.
\end{exercise}

\begin{exercise}
Finish the proof of \corref{cor:continterval}.
\end{exercise}

\begin{exercise}
Prove the claims in \exampleref{example:countdiscont}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:invcont}.
\end{exercise}

\begin{exercise}
Suppose $S \subset \R$, and $f \colon S \to \R$ is an increasing
function.
\begin{enumerate}[a)]
 \item If $c$ is a cluster point
of $S \cap (c,\infty)$ show that 
$\lim\limits_{x\to c^+} f(x) < \infty$.
 \item If $c$ is a cluster point of $S \cap (-\infty,c)$
and $\lim\limits_{x\to c^-} f(x) = \infty$, prove that 
$S \subset (-\infty,c)$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Suppose $I \subset \R$ is an interval and $f \colon I \to \R$ is a function
such that for each $c \in I$, there exist $a, b \in \R$ with
$a > 0$ such that $f(x) \geq a x + b$ for all $x \in I$
and $f(c) = a c + b$.
Show that $f$ is strictly increasing.
\end{exercise}

\begin{exercise}
Suppose $f \colon I \to J$ is a continuous, bijective (one-to-one and onto)
function for two intervals $I$ and $J$.
Show that $f$ is strictly monotone.
\end{exercise}

\begin{exercise}
Consider a monotone function $f \colon I \to \R$ on an interval $I$.
Prove that there exists
a function $g \colon I \to \R$ such that
$\lim\limits_{x \to c^-} g(x) = g(c)$ for all $c \in I$, except the
smaller (left) endpoint of $I$, and such that
$g(x) = f(x)$ for all but countably many $x$.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Let $S \subset \R$ be any subset.
If $f \colon S \to \R$ is increasing,
then show that there exists an increasing $F \colon \R \to \R$
such that $f(x) = F(x)$ for all $x \in S$.
 \item Find an example of a strictly increasing $f \colon S \to \R$ such that
an increasing $F$ as above is never strictly increasing.
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:increasingfuncdiscatQ}
Find an example of an increasing function $f \colon [0,1] \to \R$
that has a discontinuity at each rational number.
Then show that the image
$f([0,1])$ contains no interval.
Hint: Enumerate
the rational numbers and define
the function with a series.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The Derivative} \label{der:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The derivative}
\label{sec:der}

\sectionnotes{1 lecture}

The idea of a derivative is the following.
Let us suppose a graph of a function looks locally like a straight line.
We can then talk about the slope of this line.
The slope tells us 
the rate at which 
the value of the function changing at the particular point.
Of course, we are leaving out any function that has corners or
discontinuities.
Let us be precise.

\subsection*{Definition and basic properties}

\begin{defn}
Let $I$ be an interval, let
$f \colon I \to \R$ be a function, and let $c \in I$.
If 
the limit
\begin{equation*}
L := \lim_{x \to c} \frac{f(x)-f(c)}{x-c} 
\end{equation*}
exists, then we say $f$ is \emph{\myindex{differentiable}} at
$c$, that $L$ is the \emph{\myindex{derivative}} of $f$ at $c$,
and write $f'(c) := L$.

\medskip

If $f$ is differentiable at all $c \in I$, then we simply say that
$f$ is \emph{differentiable}, and then we obtain a function
$f' \colon I \to \R$.

\medskip

The expression $\frac{f(x)-f(c)}{x-c}$ is called the
\emph{\myindex{difference quotient}}.
\end{defn}

The graphical interpretation of the derivative is  depicted in
\figureref{derivfig}.
The left-hand plot gives the line through
$\bigl(c,f(c)\bigr)$
and $\bigl(x,f(x)\bigr)$ with slope
$\frac{f(x)-f(c)}{x-c}$, that is,
the so-called \emph{\myindex{secant line}}.
When we take the limit as $x$ goes to $c$,
we get the right-hand plot, where we see
that the derivative of the function
at the point $c$ is the slope of the line tangent to the graph of $f$
at the point $\bigl(c,f(c)\bigr)$.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input derivdfig.eepic
\qquad
\input derivfig.eepic
\caption{Graphical interpretation of the derivative.\label{derivfig}}
\end{center}
\end{figure}

We allow $I$ to be a closed interval and we allow
$c$ to be an endpoint of $I$.
Some calculus books do not allow $c$ to be an
endpoint of an interval, but all the theory still works by allowing it, and
it makes our work easier.

\begin{example}
Let $f(x) := x^2$ defined on the whole real line.
We find that
\begin{equation*}
\lim_{x\to c} \frac{x^2-c^2}{x-c} =
\lim_{x\to c} \frac{(x+c)(x-c)}{x-c} =
\lim_{x\to c} (x+c) = 2c.
\end{equation*}
Therefore $f'(c) = 2c$.
\end{example}

\begin{example}
The function $f(x) := \abs{x}$ is not differentiable
at the origin.
When $x > 0$, then
\begin{equation*}
\frac{\abs{x}-\abs{0}}{x-0} =
\frac{x-0}{x-0} = 1 ,
\end{equation*}
and when $x < 0$ we have
\begin{equation*}
\frac{\abs{x}-\abs{0}}{x-0} =
\frac{-x-0}{x-0} = -1 .
\end{equation*}
\end{example}

A famous example of Weierstrass shows that there exists a continuous
function that is not differentiable at \emph{any} point.
The construction
of this function is beyond the scope of this book.
On the other hand,
a differentiable function
is always continuous.

\begin{prop}
Let $f \colon I \to \R$ be differentiable at $c \in I$,
then it is continuous at $c$.
\end{prop}

\begin{proof}
We know the limits
\begin{equation*}
\lim_{x\to c}\frac{f(x)-f(c)}{x-c} = f'(c)
\qquad
\text{and}
\qquad
\lim_{x\to c}(x-c) = 0
\end{equation*}
exist.
Furthermore,
\begin{equation*}
f(x)-f(c) = 
\left( \frac{f(x)-f(c)}{x-c} \right) (x-c) .
\end{equation*}
Therefore the limit of $f(x)-f(c)$ exists and
\begin{equation*}
\lim_{x\to c} \bigl( f(x)-f(c) \bigr) =
\left(\lim_{x\to c} \frac{f(x)-f(c)}{x-c} \right)
\left(\lim_{x\to c} (x-c) \right) =
f'(c) \cdot 0  = 0.
\end{equation*}
Hence $\lim\limits_{x\to c} f(x) = f(c)$, and $f$ is continuous at $c$.
\end{proof}

An important property of the derivative is linearity.
The
derivative is the approximation of a function by a straight line.
The slope of a line through two points changes linearly when the
$y$-coordinates are changed linearly.
By taking the limit,
it makes sense that the derivative is linear.

\begin{prop}
\index{linearity of the derivative}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be differentiable at $c \in I$,
and let $\alpha \in \R$.
\begin{enumerate}[(i)]
\item
Define $h \colon I \to \R$ by $h(x) := \alpha f(x)$.
Then
$h$ is differentiable at $c$ and
$h'(c) = \alpha f'(c)$.
\item
Define $h \colon I \to \R$ by $h(x) :=  f(x) + g(x)$.
Then
$h$ is differentiable at $c$ and
$h'(c) =  f'(c) + g'(c)$.
\end{enumerate}
\end{prop}

\begin{proof}
First, let $h(x) := \alpha f(x)$.
For $x \in I$, $x \not= c$ we have
\begin{equation*}
\frac{h(x)-h(c)}{x-c} =
\frac{\alpha f(x) - \alpha f(c)}{x-c}
=
\alpha \frac{f(x) - f(c)}{x-c} .
\end{equation*}
The limit as $x$ goes to $c$ exists on the right
by \corref{falg:cor}.
We get
\begin{equation*}
\lim_{x\to c}\frac{h(x)-h(c)}{x-c} =
\alpha \lim_{x\to c} \frac{f(x) - f(c)}{x-c} .
\end{equation*}
Therefore $h$ is differentiable at $c$,
and the derivative is computed as given.

Next, define $h(x) := f(x)+g(x)$.
For $x \in I$, $x \not= c$ we have
\begin{equation*}
\frac{h(x)-h(c)}{x-c} =
\frac{\bigl(f(x) + g(x)\bigr) - \bigl(f(c) + g(c)\bigr)}{x-c}
=
\frac{f(x) - f(c)}{x-c}
+
\frac{g(x) - g(c)}{x-c} .
\end{equation*}
The limit as $x$ goes to $c$ exists on the right
by \corref{falg:cor}.
We get
\begin{equation*}
\lim_{x\to c}\frac{h(x)-h(c)}{x-c} =
\lim_{x\to c} \frac{f(x) - f(c)}{x-c}
+
\lim_{x\to c}\frac{g(x) - g(c)}{x-c} .
\end{equation*}
Therefore $h$ is differentiable at $c$
and the derivative is computed as given.
\end{proof}

It is not true that the derivative of a multiple of two functions is
the multiple of the derivatives.
Instead we get the so-called \emph{product
rule} or the \emph{\myindex{Leibniz rule}}%
\footnote{Named for the German mathematician
\href{http://en.wikipedia.org/wiki/Leibniz}{Gottfried Wilhelm Leibniz}
(1646--1716).}.

\begin{prop}[Product rule]\index{product rule}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be 
functions differentiable at $c$.
If $h \colon I \to \R$
is defined by
\begin{equation*}
h(x) := f(x) g(x) ,
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = f(c) g'(c) + f'(c) g(c) .
\end{equation*}
\end{prop}

The proof of the product rule is left as an exercise.
The key is to
use the identity
$f(x) g(x) - f(c) g(c) = f(x)\bigl( g(x) - g(c) \bigr) + g(c) \bigl( f(x) -
f(c) \bigr)$.

\begin{prop}[Quotient Rule]\index{quotient rule}
Let $I$ be an interval, let
$f \colon I \to \R$ and $g \colon I \to \R$ be differentiable at $c$
and $g(x) \not= 0$ for all $x \in I$.
If $h \colon I \to \R$
is defined by
\begin{equation*}
h(x) := \frac{f(x)}{g(x)},
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = \frac{f'(c) g(c) - f(c) g'(c)}{{\bigl(g(c)\bigr)}^2} .
\end{equation*}
\end{prop}

Again the proof is left as an exercise.

\subsection*{Chain rule}

A useful rule for computing derivatives 
is the chain rule.

\begin{prop}[Chain Rule]
\index{chain rule}
Let $I_1, I_2$ be intervals, let
$g \colon I_1 \to I_2$ be differentiable at $c \in I_1$,
and
$f \colon I_2 \to \R$ be differentiable at $g(c)$.
If $h \colon I_1 \to \R$
is defined by
\begin{equation*}
h(x) := (f \circ g) (x) = f\bigl(g(x)\bigr) ,
\end{equation*}
then $h$ is differentiable at $c$ and
\begin{equation*}
h'(c) = f'\bigl(g(c)\bigr)g'(c) .
\end{equation*}
\end{prop}

\begin{proof}
Let $d := g(c)$.
Define
$u \colon I_2 \to \R$ and $v \colon I_1 \to \R$ by
\begin{align*}
& u(y) :=
\begin{cases}
 \frac{f(y) - f(d)}{y-d}  & \text{ if $y \not=d$,} \\
f'(d) & \text{ if $y = d$,}
\end{cases}
\\
& v(x) :=
\begin{cases}
\frac{g(x) - g(c)}{x-c} & \text{ if $x \not=c$,} \\
g'(c) & \text{ if $x = c$.}
\end{cases}
\end{align*}
We note that
\begin{equation*}
f(y)-f(d) = u(y) (y-d)
\qquad \text{and} \qquad
g(x)-g(c) = v(x) (x-c) .
\end{equation*}
We plug in to obtain
\begin{equation*}
h(x)-h(c)
=
f\bigl(g(x)\bigr)-f\bigl(g(c)\bigr)
=
u\bigl( g(x) \bigr) \bigl(g(x)-g(c)\bigr)
=
u\bigl( g(x) \bigr) \bigl(v(x) (x-c)\bigr) .
\end{equation*}
Therefore,
\begin{equation} \label{eq:chainruleeq}
\frac{h(x)-h(c)}{x-c}
=
u\bigl( g(x) \bigr) v(x) .
\end{equation}
We compute the limits $\lim_{y \to d} u(y)
= f'(d) = f'\bigl(g(c)\bigr)$ and
$\lim_{x \to c} v(x) = g'(c)$.
That is, the functions $u$ and $v$
are continuous at $d = g(c)$ and $c$ respectively.
Furthermore the function $g$ is continuous at $c$.
%We note that $\displaystyle \lim_{x\to c} v(x) = g'(c)$,
%$g$ is continuous at $c$, that is
%$\displaystyle \lim_{x\to c} g(x) = g(c)$,
%and
%finally that
%$\displaystyle \lim_{y\to g(c)} u(y) = f'\bigl(g(c)\bigr)$.
Hence the limit of
the right-hand side of \eqref{eq:chainruleeq}
as $x$ goes to $c$
exists and is equal to $f'\bigl(g(c)\bigr) g'(c)$.
Thus $h$
is differentiable at $c$ and the limit is $f'\bigl(g(c)\bigr)g'(c)$.
\end{proof}

\subsection*{Exercises}

\begin{exercise}
Prove the product rule.
Hint: Use
$f(x) g(x) - f(c) g(c) = f(x)\bigl( g(x) - g(c) \bigr) + g(c) \bigl( f(x) -
f(c) \bigr)$.
\end{exercise}

\begin{exercise}
Prove the quotient rule.
Hint: You can do this directly, but it may be
easier to find the derivative of $\nicefrac{1}{x}$ and then use
the chain rule and the product rule.
\end{exercise}

\begin{exercise} \label{exercise:diffofxn}
For $n \in \Z$,
prove that $x^n$ is differentiable and find the derivative,
unless, of course, $n < 0$ and $x=0$.
Hint: Use the product rule.
\end{exercise}

\begin{exercise}
Prove that a polynomial is differentiable and find the derivative.
Hint: Use the previous exercise.
\end{exercise}

\begin{exercise}
Define $f \colon \R \to \R$ by
\begin{equation*}
f(x) :=
\begin{cases}
x^2 & \text{ if $x \in \Q$,}\\
0 & \text{ otherwise.}
\end{cases}
\end{equation*}
Prove that $f$ is differentiable at $0$, but discontinuous at all points
except $0$.
\end{exercise}

\begin{exercise}
Assume the inequality $\abs{x-\sin(x)} \leq x^2$.
Prove that sin is
differentiable at $0$, and find the derivative at $0$.
\end{exercise}

\begin{exercise}
Using the previous exercise, prove that sin is differentiable at all $x$
and that the derivative is $\cos(x)$.
Hint: Use the sum-to-product
trigonometric identity as we did before.
\end{exercise}

\begin{exercise}
Let $f \colon I \to \R$ be differentiable.
Given $n \in \Z$, define $f^n$
be the function defined by $f^n(x) := {\bigl( f(x) \bigr)}^n$.
If
$n < 0$ assume $f(x) \not= 0$.
Prove that
$(f^n)'(x) = n {\bigl(f(x) \bigr)}^{n-1} f'(x)$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a differentiable
Lipschitz continuous function.
Prove that $f'$ is a bounded function.
\end{exercise}

\begin{exercise}
Let $I_1, I_2$ be intervals.
Let $f \colon I_1 \to I_2$ be a bijective function and $g \colon I_2 \to I_1$
be the inverse.
Suppose that both $f$ is differentiable at $c \in I_1$ and
$f'(c) \not=0$ and $g$ is differentiable at $f(c)$.
Use the chain rule
to find a formula for $g'\bigl(f(c)\bigr)$ (in terms of $f'(c)$).
\end{exercise}

\begin{exercise} \label{exercise:bndmuldiff}
Suppose $f \colon I \to \R$ is a bounded function and $g \colon I \to
\R$ is a function differentiable at $c \in I$ and $g(c) = g'(c) = 0$.
Show
that $h(x) := f(x) g(x)$ is differentiable at $c$.
Hint: Note that you
cannot apply the product rule.
\end{exercise}

\begin{exercise} \label{exercise:diffsqueeze}
Suppose $f \colon I \to \R$, 
$g \colon I \to \R$, and
$h \colon I \to \R$, are functions.
Suppose $c \in I$ is such that
$f(c) = g(c) = h(c)$, $g$ and $h$ are differentiable at $c$,
and $g'(c) = h'(c)$.
Furthermore suppose $h(x) \leq f(x) \leq g(x)$ for
all $x \in I$.
Prove $f$ is differentiable at $c$ and $f'(c) = g'(c) =
h'(c)$.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Mean value theorem}
\label{sec:mvt}

\sectionnotes{2 lectures (some applications may be skipped)}

\subsection*{Relative minima and maxima}

We talked about absolute maxima and minima.
These are the tallest peaks and
lowest valleys in the whole mountain range.
We might also want to talk
about peaks of individual mountains and valleys.

\begin{defn}
Let $S \subset \R$ be a set and
let $f \colon S \to \R$ be a function.
The function $f$ is said to have
a \emph{\myindex{relative maximum}} at $c \in S$ if there exists a $\delta>0$
such that for all $x \in S$ such that $\abs{x-c} < \delta$
we have $f(x) \leq f(c)$.
The definition of \emph{\myindex{relative
minimum}} is analogous.
\end{defn}

\begin{thm}\label{relminmax:thm}
Let $f \colon [a,b] \to \R$ be a function differentiable at $c \in (a,b)$,
and $c$
is a relative minimum or a relative maximum of $f$.
Then
$f'(c) = 0$.
\end{thm}

\begin{proof}
We prove the statement for a maximum.
For a minimum the statement
follows by considering the function $-f$.

Let $c$ be a relative maximum of $f$.
In particular as long
as $\abs{x-c} < \delta$ we have $f(x)-f(c) \leq 0$.
Then we look at the difference
quotient.
If $x > c$ we note that
\begin{equation*}
\frac{f(x)-f(c)}{x-c} \leq 0 ,
\end{equation*}
and if $y < c$ we have
\begin{equation*}
\frac{f(y)-f(c)}{y-c} \geq 0 .
\end{equation*}
See \figureref{fig:critpt} for an illustration.
\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input critpt.eepic
\caption{Slopes of secants at a relative maximum.\label{fig:critpt}}
\end{center}
\end{figure}

We now take sequences $\{ x_n\}$ and $\{ y_n \}$, such
that $x_n > c$, and
$y_n < c$ for all $n \in \N$, and such that
 $\lim\, x_n = \lim\, y_n = c$.
Since $f$
is differentiable at $c$ we know 
\begin{equation*}
0 \geq \lim_{n\to\infty} \frac{f(x_n)-f(c)}{x_n-c} 
=
f'(c)
=
\lim_{n\to\infty} \frac{f(y_n)-f(c)}{y_n-c} \geq 0.\qedhere
\end{equation*}
\end{proof}

For a differentiable function, a point where 
$f'(c) = 0$ is called a \emph{\myindex{critical point}}.
When $f$ is not
differentiable at some points,
it is common to also say $c$ is a critical point
if $f'(c)$ does not exist.
The theorem says that a relative minimum or maximum at an interior point
of an interval must be a critical point.
As you remember from calculus, finding minima and maxima of a function can
be done by finding all the critical points together with the endpoints of
the interval and simply checking where is the function biggest or smallest.

\subsection*{Rolle's theorem}

Suppose a function is zero at both endpoints of an interval.
Intuitively it ought to attain a minimum or a maximum in the interior of the
interval,
then at such a minimum or a maximum, the derivative should be zero.
See \figureref{rollefig} for the geometric idea.
This is the content of the
so-called Rolle's theorem%
\footnote{Named after the French mathematician
\href{https://en.wikipedia.org/wiki/Michel_Rolle}{Michel Rolle}
(1652--1719).}.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input rollefig.eepic
\caption{Point where tangent line is horizontal, that is $f'(c) =
0$.\label{rollefig}}
\end{center}
\end{figure}

\begin{thm}[Rolle] \label{thm:rolle}
\index{Rolle's theorem}
Let $f \colon [a,b] \to \R$ be continuous function
differentiable on $(a,b)$ such that $f(a) = f(b)$.
Then there exists a $c \in (a,b)$ such that $f'(c) = 0$.
\end{thm}

\begin{proof}
As $f$ is continuous on $[a,b]$ it attains an absolute minimum and an
absolute 
maximum in $[a,b]$.
We wish to apply \thmref{relminmax:thm} and
so we need a minimum or maximum at some $c \in (a,b)$.
Write $K := f(a) = f(b)$.
If there exists an $x$ such that $f(x) > K$, then the absolute
maximum is bigger than $K$ and hence occurs at $c \in (a,b)$, and
therefore we get $f'(c) = 0$.
On the other hand if there exists an $x$
such that $f(x) < K$, then the absolute minimum occurs at some
$c \in (a,b)$ and we have that $f'(c) = 0$.
If there is no $x$ such that
$f(x) > K$ or
$f(x) < K$, then we have that $f(x) = K$ for all $x$ and then
$f'(x) = 0$ for all $x \in [a,b]$, so any $c$ will work.
%If it attains an absolute maximum at $c \in (a,b)$, then $c$
%is also a relative maximum and we apply 
%\thmref{relminmax:thm} to find that $f'(c) = 0$.
%If the absolute maximum is at $a$ or at $b$,
%then we look for the absolute minimum.
%If the absolute minimum is at $c \in (a,b)$,
%then again we find that $f'(c) = 0$.  So suppose that the absolute
%minimum is also at $a$ or $b$.  Write $k := f(a) = f(b)$.
%Hence the absolute minimum is $k$ and
%the absolute maximum is $k$, and the function is
%identically $k$.  Thus $f'(x) = 0$ for all $x \in [a,b]$, so pick
%an arbitrary $c \in (a,b)$.
\end{proof}

Note that it is absolutely necessary for the derivative to exist for all $x
\in (a,b)$.
For example take the function $f(x) = \abs{x}$ on $[-1,1]$.
Clearly $f(-1) = f(1)$, but there is no point where $f'(c) = 0$.

\subsection*{Mean value theorem}

We extend \hyperref[thm:rolle]{Rolle's theorem}
to functions that attain different
values at the endpoints.

\begin{thm}[Mean value theorem] \label{thm:mvt}
\index{mean value theorem}
Let $f \colon [a,b] \to \R$ be a continuous function
differentiable on $(a,b)$.
Then there exists a point $c \in (a,b)$
such that
\begin{equation*}
f(b)-f(a) = f'(c)(b-a) .
\end{equation*}
\end{thm}

\begin{proof}
The theorem follows from \hyperref[thm:rolle]{Rolle's theorem}.
Define the
function $g \colon [a,b] \to \R$ by
\begin{equation*}
g(x) := f(x)-f(b)+\bigl(f(b)-f(a)\bigr)\frac{b-x}{b-a}.
\end{equation*}
The function $g$ is a differentiable on $(a,b)$,
continuous on $[a,b]$, such that $g(a) = 0$ and $g(b) = 0$.
Thus there exists
$c \in (a,b)$ such that $g'(c) = 0$.
\begin{equation*}
0 = g'(c) = f'(c) + \bigl(f(b)-f(a)\bigr)\frac{-1}{b-a} .
\end{equation*}
Or in other words
$f'(c)(b-a) = f(b)-f(a)$.
\end{proof}

For a geometric interpretation of the mean value theorem, see
\figureref{mvtfig}.
The idea is that the value $\frac{f(b)-f(a)}{b-a}$
is the slope of the line between the points $\bigl(a,f(a)\bigr)$
and $\bigl(b,f(b)\bigr)$.
Then $c$ is the point such that $f'(c) = \frac{f(b)-f(a)}{b-a}$, that 
is, the tangent line at the point $\bigl(c,f(c)\bigr)$ has the same slope as the
line between $\bigl(a,f(a)\bigr)$ and $\bigl(b,f(b)\bigr)$.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input mvtfig.eepic
\caption{Graphical interpretation of the mean value theorem.\label{mvtfig}}
\end{center}
\end{figure}

\subsection*{Applications}

We now solve our very first differential equation.

\begin{prop} \label{prop:derzeroconst}
Let $I$ be an interval and
let $f \colon I \to \R$ be a differentiable function such that $f'(x) = 0$
for all $x \in I$.
Then $f$ is constant.
\end{prop}

\begin{proof}
Take arbitrary $x,y \in I$ with $x < y$.
Then $f$ restricted to $[x,y]$ satisfies the hypotheses
of the \hyperref[thm:mvt]{mean value theorem}.
Therefore there is a $c \in (x,y)$ such that
\begin{equation*}
f(y)-f(x) = f'(c)(y-x).
\end{equation*}
as $f'(c) = 0$, we have $f(y) = f(x)$.
Therefore,
the function is constant.
\end{proof}

Now that we know what it means for the function to stay constant, let us look
at increasing and decreasing functions.
We say $f \colon I \to \R$ is \emph{\myindex{increasing}}
(resp.\  \emph{\myindex{strictly increasing}}) if
$x < y$ implies $f(x) \leq f(y)$ (resp.\ $f(x) < f(y)$).
We define
\emph{\myindex{decreasing}} and
\emph{\myindex{strictly decreasing}} in the same way by switching the
inequalities for $f$.

\begin{prop} \label{incdecdiffprop}
Let $I$ be an interval and
let $f \colon I \to \R$ be a differentiable function.
%\begin{enumerate}[(i),itemsep=0.5\itemsep,parsep=0.5\parsep,topsep=0.5\topsep,partopsep=0.5\partopsep]
\begin{enumerate}[(i)]
\item $f$ is increasing if and only if $f'(x) \geq 0$ for all $x \in I$.
\item $f$ is decreasing if and only if $f'(x) \leq 0$ for all $x \in I$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us prove the first item.
Suppose $f$ is increasing, then
for all $x$ and $c$ in $I$ we have
\begin{equation*}
\frac{f(x)-f(c)}{x-c} \geq 0 .
\end{equation*}
Taking a limit as $x$ goes to $c$ we see that $f'(c) \geq 0$.

For the other direction, suppose $f'(x) \geq 0$ for all $x \in I$.
Take any $x, y \in I$ where $x < y$.
By the \hyperref[thm:mvt]{mean value theorem}
there is some $c \in
(x,y)$ such that
\begin{equation*}
f(x)-f(y) = f'(c)(x-y) .
\end{equation*}
As $f'(c) \geq 0$, and $x-y < 0$, then $f(x) - f(y) \leq 0$ or $f(x) \leq
f(y)$ and so
$f$ is increasing.

We leave the decreasing part to the reader as exercise.
\end{proof}

\begin{example}
We can make a similar but weaker statement about strictly increasing and
decreasing functions.
If $f'(x) > 0$ for all $x \in I$, then
$f$ is strictly increasing.
The proof is left as an exercise.
The converse is not true.
For example,
$f(x) := x^3$ is a strictly increasing function, but $f'(0) = 0$.
\end{example}

Another application of the \hyperref[thm:mvt]{mean value theorem} is the following result about
location of extrema.
The theorem is stated for an absolute minimum and
maximum, but the way it is applied to find relative minima
and maxima is to restrict $f$ to an interval $(c-\delta,c+\delta)$.

\begin{prop} \label{firstderminmaxtest}
Let $f \colon (a,b) \to \R$ be continuous.
Let $c \in (a,b)$
and suppose
$f$ is differentiable on $(a,c)$ and $(c,b)$.
\begin{enumerate}[(i)]
\item If $f'(x) \leq 0$ for $x \in (a,c)$ and
 $f'(x) \geq 0$ for $x \in (c,b)$, then $f$ has an absolute minimum 
at $c$.
\item If $f'(x) \geq 0$ for $x \in (a,c)$ and
 $f'(x) \leq 0$ for $x \in (c,b)$, then $f$ has an absolute maximum
at $c$.
\end{enumerate}
\end{prop}

\begin{proof}
Let us prove the first item.
The second is left to the reader.
Let $x$ be in $(a,c)$
and $\{ y_n\}$ a sequence such that $x < y_n < c$ and $\lim\, y_n = c$.
By the previous proposition,
the function is decreasing on $(a,c)$ so $f(x) \geq f(y_n)$.
The function is continuous at $c$ so we can take the limit to get
$f(x) \geq f(c)$ for all $x \in (a,c)$.

Similarly take $x \in (c,b)$
and $\{ y_n\}$ a sequence such that $c < y_n < x$ and $\lim\, y_n = c$.
The function is increasing on $(c,b)$ so $f(x) \geq f(y_n)$.
By continuity of $f$ we get
$f(x) \geq f(c)$ for all $x \in (c,b)$.
Thus $f(x) \geq f(c)$ for all
$x \in (a,b)$.
\end{proof}

The converse of the proposition does not hold.
See
\exampleref{baddifffunc:example} below.

\subsection*{Continuity of derivatives and the intermediate value theorem}

Derivatives of functions satisfy an
intermediate value property.
The result is usually
called \myindex{Darboux's theorem}.

\begin{thm}[Darboux] \label{thm:darboux}
Let $f \colon [a,b] \to \R$ be differentiable.
Suppose that there exists
a $y \in \R$ such that $f'(a) < y < f'(b)$ or
$f'(a) > y > f'(b)$.
Then there exists a $c \in (a,b)$ such that $f'(c) =
y$.
\end{thm}

\begin{proof}
Suppose without loss of generality that
$f'(a) < y < f'(b)$.
Define
\begin{equation*}
g(x) := yx - f(x) .
\end{equation*}
As $g$ is continuous on $[a,b]$, then $g$ attains a maximum at some $c \in
[a,b]$.

Now compute $g'(x) = y-f'(x)$.
Thus $g'(a) > 0$.
As the derivative is
the limit of difference quotients and is positive, there must be some
difference quotient that is positive.
That is, there must exist
an $x > a$ such that
\begin{equation*}
\frac{g(x)-g(a)}{x-a} > 0 ,
\end{equation*}
or $g(x) > g(a)$.
Thus $a$
cannot possibly be a maximum of $g$.
Similarly as $g'(b) < 0$,
we find an $x < b$ (a different $x$) such that
$\frac{g(x)-g(b)}{x-b} < 0$ or that $g(x) > g(b)$, thus
$b$ cannot possibly be a maximum.

Therefore $c \in (a,b)$.
Then as $c$ is a maximum of $g$ we find $g'(c) = 0$
and $f'(c) = y$.
\end{proof}

We have seen already that
there exist discontinuous functions that have the
intermediate value property.
While it is hard to imagine at first, there
also
exist functions that are differentiable everywhere and the derivative is not
continuous.

\begin{example} \label{baddifffunc:example}
Let $f \colon \R \to \R$ be the function defined by
\begin{equation*}
f(x) :=
\begin{cases}
{\bigl( x \sin(\nicefrac{1}{x}) \bigr)}^2 & \text{ if $x \not= 0$,} \\
0 & \text{ if $x = 0$.}
\end{cases}
\end{equation*}
We claim that $f$ is differentiable everywhere, but
$f' \colon \R \to \R$ is not continuous at
the origin.
Furthermore, $f$ has a minimum at 0, but the derivative
changes sign infinitely often near the origin.
See \figureref{fig:nonc1diff}.
\begin{figure}[h!t]
\begin{center}
\includegraphics{nonc1difffig}
\qquad
\includegraphics{nonc1diffderfig}
\caption{A function with a discontinuous derivative. The function $f$ is on the left
and $f'$ is on the right.
Notice that $f(x) \leq x^2$ on the left graph.\label{fig:nonc1diff}}
\end{center}
\end{figure}

\begin{proof}It is easy to see from the definition that $f$ has an absolute
minimum at 0: we know $f(x) \geq 0$ for all $x$ and $f(0) = 0$.

The function $f$ is differentiable for $x\not=0$
and
the derivative 
is $2 \sin (\nicefrac{1}{x}) \bigl( x \sin (\nicefrac{1}{x}) -
\cos(\nicefrac{1}{x}) \bigr)$.
As an exercise show that for $x_n = \frac{4}{(8n+1)\pi}$
we have
$\lim\, f'(x_n) = -1$, and for
$y_n = \frac{4}{(8n+3)\pi}$  we have
$\lim\, f'(y_n) = 1$.
Hence if $f'$ exists at $0$,
then it cannot be continuous.

Let us show that $f'$ exists at 0.
We claim that the derivative is zero.
In other words $\abs{\frac{f(x)-f(0)}{x-0} - 0}$ goes to zero
as $x$ goes to zero.
For $x \not= 0$ we have
\begin{equation*}
\abs{\frac{f(x)-f(0)}{x-0} - 0}
=
\abs{\frac{x^2 \sin^2(\nicefrac{1}{x})}{x}}
=
\abs{x \sin^2(\nicefrac{1}{x})}
\leq
\abs{x} .
\end{equation*}
And, of course, as $x$ tends to zero, then $\abs{x}$ tends to zero and hence
$\abs{\frac{f(x)-f(0)}{x-0} - 0}$ goes to zero.
Therefore, $f$
is differentiable at 0 and the derivative at 0 is 0.
\end{proof}

A key point in the above calculation is that 
is that $\abs{f(x)} \leq x^2$,
see also Exercises \ref{exercise:bndmuldiff} and
\ref{exercise:diffsqueeze}.
\end{example}

It is sometimes useful to assume the derivative of a differentiable
function is continuous.
If $f \colon I \to \R$ is differentiable and
the derivative $f'$ is continuous on $I$, then we say $f$ is
\emph{\myindex{continuously differentiable}}.
It is common to
write $C^1(I)$ for the set of continuously differentiable functions on $I$.

\subsection*{Exercises}

\begin{exercise}
Finish the proof of \propref{incdecdiffprop}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{firstderminmaxtest}.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a differentiable
function such that $f'$ is a bounded function.
Prove
$f$ is a Lipschitz continuous function.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is differentiable and $c \in [a,b]$.
Then show there exists a sequence $\{ x_n \}$ converging to $c$, $x_n
\not= c$ for all $n$, such that
\begin{equation*}
f'(c) = \lim_{n\to \infty} f'(x_n).
\end{equation*}
Do note this does \emph{not} imply that $f'$ is continuous (why?).
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ is a function such that
$\abs{f(x)-f(y)} \leq \abs{x-y}^2$ for all $x$ and $y$.
Show that
$f(x) = C$ for some constant $C$.
Hint: Show that $f$ is differentiable
at all points and compute the derivative.
\end{exercise}

\begin{exercise} \label{exercise:posderincr}
Suppose $I$ is an interval and
$f \colon I \to \R$ is a differentiable function.
If $f'(x) > 0$ for all $x \in I$, show that $f$ is strictly increasing.
\end{exercise}

\begin{exercise}
Suppose $f \colon (a,b) \to \R$ is a differentiable function
such that
$f'(x) \not= 0$ for all $x \in (a,b)$.
Suppose there
exists
a point $c \in (a,b)$ such that $f'(c) > 0$.
Prove $f'(x) > 0$ for all $x \in (a,b)$.
\end{exercise}

\begin{exercise} \label{exercise:samediffconst}
Suppose $f \colon (a,b) \to \R$ and $g \colon (a,b) \to \R$ are
differentiable functions such that $f'(x) = g'(x)$ for all $x \in (a,b)$,
then show that there exists a constant $C$ such that $f(x) = g(x) + C$.
\end{exercise}

\begin{exercise}
Prove the following version of \myindex{L'Hopital's rule}.
Suppose 
$f \colon (a,b) \to \R$ and $g \colon (a,b) \to \R$ are differentiable
functions.
Suppose that at $c \in (a,b)$, $f(c) = 0$, $g(c)=0$, and
that the limit of $\nicefrac{f'(x)}{g'(x)}$ as $x$ goes to $c$ exists.
Show that
\begin{equation*}
\lim_{x \to c} \frac{f(x)}{g(x)} = 
\lim_{x \to c} \frac{f'(x)}{g'(x)} .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f \colon (a,b) \to \R$ be an unbounded differentiable function.
Show
$f' \colon (a,b) \to \R$ is unbounded.
\end{exercise}

\begin{exercise}
Prove the theorem Rolle actually proved in 1691:
\emph{If $f$ is a polynomial,
$f'(a) = f'(b) = 0$ for some $a < b$,
and there is no $c \in (a,b)$ such that $f'(c) = 0$,
then there is at most one root of $f$ in $(a,b)$,
that is at most one $x \in (a,b)$ such that $f(x) = 0$.}
In other words, between any two consecutive roots of $f'$ is at most one
root of $f$.
Hint: suppose there are two roots and see what happens.
\end{exercise}

\begin{exercise}
Suppose $a,b \in \R$ and $f \colon \R \to \R$ is differentiable,
$f'(x) = a$ for all $x$, and $f(0) = b$.
Find $f$ and prove that 
it is the unique differentiable function with this property.
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Taylor's theorem}
\label{sec:taylor}

\sectionnotes{half a lecture (optional section)}

\subsection*{Derivatives of higher orders}

When $f \colon I \to \R$ is differentiable, we obtain a function
$f' \colon I \to \R$.
The function
$f'$ is called the \emph{\myindex{first derivative}} of $f$.
If $f'$ is differentiable, we denote by
$f'' \colon I \to \R$ the derivative of $f'$.
The function $f''$
is called the \emph{\myindex{second derivative}} of $f$.
We similarly obtain
$f'''$, $f''''$, and so on.
With a larger number of derivatives
the notation would get out of hand; we denote
by $f^{(n)}$ the \emph{$n$th derivative}\index{nth derivative} of $f$.

When $f$ possesses $n$ derivatives, we say $f$ is
\emph{$n$ times differentiable}\index{n times differentiable}.

\subsection*{Taylor's theorem}

Taylor's theorem%
\footnote{Named for the English mathematician
\href{http://en.wikipedia.org/wiki/Brook_Taylor}{Brook Taylor}
(1685--1731).
It was first found by
the Scottish mathematician
\href{http://en.wikipedia.org/wiki/James_Gregory_(mathematician)}{James Gregory}
(1638 -- 1675).
The statement we give
was proved by
\href{http://en.wikipedia.org/wiki/Lagrange}{Joseph-Louis Lagrange}
(1736 -- 1813)}
is a generalization of the \hyperref[thm:mvt]{mean value theorem}.
Mean value theorem says that up to a small error $f(x)$ for $x$ near $x_0$ can be
approximated by $f(x_0)$, that is
\begin{equation*}
f(x) = f(x_0) + f'(c)(x-x_0),
\end{equation*}
where the ``error'' is measured in terms of the first derivative
at some point $c$ between $x$ and $x_0$.
Taylor's theorem generalizes this result to higher derivatives.
It tells us that up to a small error, any $n$
times differentiable function can be approximated at a point $x_0$
by a polynomial.
The
error of this approximation behaves like ${(x-x_0)}^{n}$ near the point $x_0$.
To see why this is a good approximation notice that for a big $n$, 
${(x-x_0)}^n$ is very small in a small interval around $x_0$.

\begin{defn}
For an $n$ times differentiable function $f$ defined near a point $x_0 \in \R$, define the
$n$th \emph{\myindex{Taylor polynomial}}%
\index{nth Taylor polynomial for f}
for $f$ at $x_0$ as
\begin{equation*}
\begin{split}
P_n^{x_0}(x)
& :=
\sum_{k=0}^n
\frac{f^{(k)}(x_0)}{k!}{(x-x_0)}^k
\\
& =
f(x_0)
+ f'(x_0)(x-x_0)
+ \frac{f''(x_0)}{2}{(x-x_0)}^2
+ \frac{f^{(3)}(x_0)}{6}{(x-x_0)}^3
+ \cdots
+ \frac{f^{(n)}(x_0)}{n!}{(x-x_0)}^n .
\end{split}
\end{equation*}
\end{defn}

Taylor's theorem says a function behaves like its $n$th
Taylor polynomial.
The 
\hyperref[thm:mvt]{mean value theorem} is really Taylor's theorem
for the first derivative.

\begin{thm}[Taylor]\index{Taylor's theorem} \label{thm:taylor}
Suppose $f \colon [a,b] \to \R$ is a function with $n$ continuous
derivatives on $[a,b]$ and such that $f^{(n+1)}$ exists on $(a,b)$.
Given distinct points $x_0$ and $x$ in $[a,b]$,
we can find a point $c$ between $x_0$
and $x$ such that
\begin{equation*}
f(x)=P_{n}^{x_0}(x)+\frac{f^{(n+1)}(c)}{(n+1)!}{(x-x_0)}^{n+1} .
\end{equation*}
\end{thm}

The term $R_n^{x_0}(x):=\frac{f^{(n+1)}(c)}{(n+1)!}{(x-x_0)}^{n+1}$ is called the
\emph{remainder term}\index{remainder term in Taylor's formula}.
This
form 
of the remainder term is called the
\emph{\myindex{Lagrange form}} of the remainder.
There are other ways
to write the remainder term, but we skip those.
Note that $c$ depends on
both $x$ and $x_0$.

\begin{proof}
Find a number $M_{x,x_0}$ (depending on $x$ and $x_0$) solving the equation
\begin{equation*}
f(x)=P_{n}^{x_0}(x)+M_{x,x_0}{(x-x_0)}^{n+1} .
\end{equation*}
Define a function $g(s)$ by
\begin{equation*}
g(s) := f(s)-P_n^{x_0}(s)-M_{x,x_0}{(s-x_0)}^{n+1} .
\end{equation*}
We compute
%A simple computation shows that 
the $k$th derivative at $x_0$ of the Taylor polynomial
${(P_n^{x_0})}^{(k)}(x_0) = f^{(k)}(x_0)$ for
$k=0,1,2,\ldots,n$ (the zeroth derivative corresponds to the function
itself).
Therefore,
\begin{equation*}
g(x_0) = g'(x_0) = g''(x_0) = \cdots = g^{(n)}(x_0) = 0 .
\end{equation*}
In particular $g(x_0) = 0$.
On the other hand $g(x) = 0$.
By the
\hyperref[thm:mvt]{mean value theorem}
there exists an $x_1$ between $x_0$ and $x$ such that $g'(x_1) = 0$.
Applying the \hyperref[thm:mvt]{mean value theorem}
to $g'$ we obtain that there exists
$x_2$ between $x_0$ and $x_1$ (and therefore between $x_0$ and $x$)
such that $g''(x_2) = 0$.
We repeat the
argument $n+1$ times to obtain a number $x_{n+1}$ between $x_0$ and $x_n$
(and therefore between $x_0$ and $x$) such that $g^{(n+1)}(x_{n+1}) = 0$.

Let $c:=x_{n+1}$.
We compute the $(n+1)$th derivative of $g$ to find
\begin{equation*}
g^{(n+1)}(s) = f^{(n+1)}(s)-(n+1)!\,M_{x,x_0} .
\end{equation*}
Plugging in $c$ for $s$ we obtain $M_{x,x_0} = \frac{f^{(n+1)}(c)}{(n+1)!}$, and
we are done.
\end{proof}

In the proof we have computed 
${(P_n^{x_0})}^{(k)}(x_0) = f^{(k)}(x_0)$ for $k=0,1,2,\ldots,n$.
Therefore the Taylor polynomial has the same derivatives as $f$ at $x_0$
up to the $n$th derivative.
That is why the Taylor polynomial is
a good approximation to $f$.

The definition of derivative says that
a function is
differentiable if it
is locally approximated by a line.
%, that is the definition of the derivative.
Similarly we mention in passing that there exists a converse to Taylor's
theorem,
which we will neither state nor prove,
saying that if a function is
locally approximated in a certain way by a polynomial of degree $d$, then it
has $d$ derivatives.

\subsection*{Exercises}

\begin{exercise}
Compute the $n$th Taylor Polynomial at $0$ for the exponential function.
\end{exercise}

\begin{exercise}
Suppose $p$ is a polynomial of degree $d$.
Given any $x_0 \in \R$,
show that
the $(d+1)$th Taylor polynomial for $p$ at $x_0$ is equal to $p$.
\end{exercise}

\begin{exercise}
Let $f(x) := \abs{x}^3$.
Compute $f'(x)$ and $f''(x)$ for all $x$,
but show that $f^{(3)}(0)$ does not exist.
\end{exercise}

\begin{exercise}
Suppose $f \colon \R \to \R$ has $n$ continuous derivatives.
Show
that for any $x_0 \in \R$,
there exist polynomials $P$ and $Q$ of degree $n$ and 
an $\epsilon > 0$ such that $P(x) \leq f(x) \leq Q(x)$ for all $x \in
[x_0-\epsilon,x_0+\epsilon]$  and
$Q(x)-P(x) = \lambda {(x-x_0)}^n$ for some $\lambda \geq 0$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ has $n+1$ continuous derivatives
and $x_0 \in [a,b]$,
prove
$\lim\limits_{x\to x_0} \frac{R_n^{x_0}(x)}{{(x-x_0)}^n} = 0$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ has $n+1$ continuous derivatives
and $x_0 \in (a,b)$.
Show that $f^{(k)}(x_0) = 0$ for all $k = 0, 1, 2, \ldots, n$
if and only if $g(x) := \frac{f(x)}{{(x-x_0)}^{n+1}}$ is continuous at $x_0$.
\end{exercise}

\begin{exercise}
Suppose $a,b,c \in \R$ and $f \colon \R \to \R$ is differentiable,
$f''(x) = a$ for all $x$, $f'(0) = b$, and $f(0) = c$.
Find $f$ and prove that 
it is the unique differentiable function with this property.
\end{exercise}

\begin{exercise}[Challenging]
Show that a simple converse to Taylor's theorem does not hold.
Find a function $f \colon \R \to \R$ with no second derivative at $x=0$ such that
$\abs{f(x)} \leq \abs{x^3}$, that is, $f$ goes to zero at 0 faster than $x^3$, and
while $f'(0)$ exists, $f''(0)$ does not.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Inverse function theorem}
\label{sec:ift}

\sectionnotes{less than 1 lecture (optional section, needed for
\sectionref{sec:logandexp}, requires 
\sectionref{sec:monotonefunc})}

\subsection*{Inverse function theorem}

The main idea of differentiating inverse functions is the following lemma.

\begin{lemma} \label{lemma:ift}
Let $I,J \subset \R$ be intervals.
If $f \colon I \to J$ is strictly monotone (hence one-to-one),
onto ($f(I) = J$),
differentiable at $x$, and $f'(x) \not= 0$,
then the inverse 
$f^{-1}$ is differentiable at $y = f(x)$ and
\begin{equation*}
(f^{-1})'(y) = \frac{1}{f'\bigl( f^{-1}(y) \bigr)} = \frac{1}{f'(x)} .
\end{equation*}
If $f$ is continuously differentiable and $f'$ is never zero, then $f^{-1}$
is continuously differentiable.
\end{lemma}

\begin{proof}
By \propref{prop:invcont} $f$ has a continuous inverse.
Let us
call the inverse $g \colon J \to I$ for convenience.
Let $x,y$ be as in the statement, take $t \in I$ to be arbitrary
and let $s := f(t)$.
Then
\begin{equation*}
\frac{g(s)-g(y)}{s-y} =
\frac{g\bigl(f(t)\bigr)-g\bigl(f(x)\bigr)}{f(t)-f(x)} =
\frac{t-x}{f(t)-f(x)} .
\end{equation*}
As $f$ is differentiable at $x$ and $f'(x) \not= 0$, then 
$\frac{t-x}{f(t)-f(x)} \to \nicefrac{1}{f'(x)}$ as $t \to x$.
Because $g(s) \to g(y)$ as $s \to y$, we 
can plug in $g(s)$ for $t$, and $g(y)$ for $x$ and take the limit
as $s$ goes to $y$, that is, the limit exists.
In other words,
\begin{equation*}
\lim_{s \to y}
\frac{g(s)-g(y)}{s-y} 
=
\lim_{t \to x} \frac{t-x}{f(t)-f(x)} 
=
\frac{1}{f'(x)} 
=
\frac{1}{f'\bigl(g(y)\bigr)}
\end{equation*}
See \figureref{inversefig} for the geometric idea.
\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input inversefigA.eepic
\quad
\input inversefigB.eepic
\caption{Interpretation of the derivative of the inverse
function.\label{inversefig}}
\end{center}
\end{figure}

If both $f'$ and $g$ are continuous, $f'$ is nonzero at all $x$,
then the lemma applies at all points $x \in I$ and the resulting function $g'(y) =
\frac{1}{f'\bigl(g(t)\bigr)}$ must be continuous.
\end{proof}

What is usually called the inverse function theorem is the following result.

\begin{thm}[Inverse function theorem]\index{inverse function theorem}
Let $f \colon (a,b) \to \R$ be a continuously differentiable function,
$x_0 \in (a,b)$ a point where $f'(x_0) \not= 0$.
Then there exists
an interval $I \subset (a,b)$ with $x_0 \in I$, the
restriction $f|_{I}$ is injective with an inverse
$g \colon J \to I$ defined on $J := f(I)$,
which is continuously differentiable and
\begin{equation*}
g'(y) = \frac{1}{f'\bigl( g(y) \bigr)} , \qquad \text{for all $y \in J$}.
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality, suppose $f'(x_0) > 0$.
As $f'$ is
continuous, there must exist an interval $I$ with $x_0 \in I$
such that $f'(x) > 0$ for all $x_0 \in I$.

By \exerciseref{exercise:posderincr} $f$ is strictly increasing
on $I$, and hence the restriction $f|_{I}$ bijective onto $J: = f(I)$.
As $f$ is continuous, then by the
\hyperref[IVT:thm]{intermediate value theorem}
(see also \corref{cor:continterval}), $f(I)$ is in interval.
Now apply \lemmaref{lemma:ift}.
\end{proof}

If you tried to prove the existence of roots directly as in
\exampleref{example:sqrt2} you may have seen
how difficult that endeavor is.
However, with the machinery we have built
for inverse functions it becomes
an almost trivial exercise, and with and the inverse function theorem
we prove far more than mere existence.

\begin{cor}
Given any $n \in \N$ and any $x \geq 0$ there exists a unique 
number $y \geq 0$ (denoted $x^{1/n} := y$), such that $y^n = x$.
Furthermore,
the function $g \colon (0,\infty) \to (0,\infty)$ defined by
$g(x) := x^{1/n}$ is continuously differentiable and
\begin{equation*}
g'(x) = \frac{1}{nx^{(n-1)/n}} = \frac{1}{n} \, x^{(1-n)/n} ,
\end{equation*}
using the convention $x^{n/m} := {(x^{1/m})}^{n}$.
\end{cor}

\begin{proof}
For $x=0$ the existence of a unique root is trivial.

Let $f(x) := x^n$.
Using product rule, $f$ is continuously differentiable
and $f'(x) = nx^{n-1}$, see \exerciseref{exercise:diffofxn}.
For $x > 0$ the derivative $f'$ is strictly positive
and so again by \exerciseref{exercise:posderincr}, $f$ is strictly
increasing (this can also be proved directly).
It is also easy to
see that the image of $f$ is the entire interval $(0,\infty)$.
We 
obtain a unique inverse $g$ and so the existence and uniqueness of positive
$n$th roots.
We apply \lemmaref{lemma:ift} to obtain the derivative.
\end{proof}

\begin{example}
The corollary provides a good example of where the inverse function theorem
gives us an interval smaller than $(a,b)$.
Take $f \colon \R \to \R$
defined by $f(x) := x^2$.
Then $f'(x) \not= 0$
as long as $x \not= 0$.
If $x_0 > 0$, we can take $I=(0,\infty)$, but
no larger.
\end{example}

\begin{example}
Another useful example is $f(x) := x^3$.
The function $f \colon \R \to \R$ is
one-to-one and onto, so $f^{-1}(x) = x^{1/3}$ exists on the entire real
line including zero and negative $x$.
The function $f$ has
a continuous derivative, but $f^{-1}$ has no derivative at the origin.
The
point is that $f'(0) = 0$.
See also \exerciseref{exercise:oddroot}.
\end{example}


\subsection*{Exercises}

\begin{exercise}
Suppose $f \colon \R \to \R$ is continuously differentiable such that
$f'(x) > 0$ for all $x$.
Show that $f$ is invertible on the interval $J =
f(\R)$, the inverse is continuously differentiable, and ${(f^{-1})}'(y) >
0$ for all $y \in f(\R)$.
\end{exercise}

%FIXME: we changed the lemma to be exactly this
%\begin{exercise}
%Prove the following version of the inverse function theorem:
%Let $I,J \subset \R$ be intervals.
%Let $f \colon I \to J$ be strictly monotone (hence one-to-one)
%and onto.  Suppose $f$ is differentiable at $x_0$
%and $f'(x_0) \not= 0$.  Then prove that the inverse
%$f^{-1}$ is differentiable at $y_0 = f'(x_0)$ and
%$(f^{-1})'(y_0) = \frac{1}{f'(x_0)}$.
%\end{exercise}

\begin{exercise}
Suppose $I,J$ are intervals and a monotone onto $f \colon I \to J$ has an inverse $g \colon J \to I$.
Suppose you already know that both $f$ and $g$ are differentiable
everywhere and $f'$ is never zero.
Using chain rule but not \lemmaref{lemma:ift} prove the
formula $g'(y) = \nicefrac{1}{f'\bigl(g(y)\bigr)}$.
\end{exercise}

\begin{exercise}
Let $n\in \N$ be even.
Prove that every $x > 0$ has a unique negative $n$th root.
That is, there exists a negative number $y$ such that $y^n = x$.
Compute the derivative
of the function $g(x) := y$.
\end{exercise}

\begin{exercise} \label{exercise:oddroot}
Let $n \in \N$ be odd and $n \geq 3$.
Prove that every $x$ has a unique $n$th root.
That is, there exists a number $y$ such that $y^n = x$.
Prove that
the function defined by $g(x) := y$ is differentiable except at $x=0$
and compute the derivative.
Prove that $g$ is not differentiable at $x=0$.
\end{exercise}

\begin{exercise}[requires \sectionref{sec:taylor}]
Show that if in the inverse function theorem $f$ has $k$ continuous
derivatives, then the inverse function $g$ also has $k$ continuous
derivatives.
\end{exercise}

\begin{exercise}
Let $f(x) := x + 2 x^2 \sin(\nicefrac{1}{x})$ for $x \not= 0$ and
$f(0) = 0$.
Show that $f$ is differentiable at all $x$, that $f'(0) > 0$,
but that $f$ is not invertible
on any interval containing the origin.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Let $f \colon \R \to \R$ be a continuously differentiable function
and $k > 0$ be a number such that $f'(x) \geq k$ for all $x \in \R$.
Show $f$ is one-to-one and onto, and has a continuously differentiable
inverse $f^{-1} \colon \R \to \R$. 
 \item Find an example $f \colon \R \to \R$
where $f'(x) > 0$
for all $x$, but $f$ is not onto.
\end{enumerate}
\end{exercise}

\begin{exercise}
Suppose $I,J$ are intervals and a monotone onto $f \colon I \to J$ has an inverse $g \colon J \to I$.
Suppose $x \in I$ and $y := f(x) \in J$, and that $g$ is differentiable at
$y$.
Prove:
\begin{enumerate}[a)]
 \item If $g'(y) \not= 0$, then $f$ is differentiable at $x$.
  \item If $g'(y) = 0$, then $f$ is not differentiable at $x$.
\end{enumerate}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The Riemann Integral} \label{int:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Riemann integral}
\label{sec:rint}

\sectionnotes{1.5 lectures}

We now get to the fundamental concept of integration.
There is
often confusion among students of
calculus between \emph{integral} and \emph{antiderivative}.
The integral is (informally) the area under the curve, nothing else.
That we can compute an antiderivative using the integral is a nontrivial
result we have to prove.

In this chapter we define the \emph{Riemann integral}%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Riemann}{Georg Friedrich Bernhard Riemann}
(1826--1866).}
using the Darboux integral%
\footnote{Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Darboux}{Jean-Gaston Darboux} (1842--1917).},
which is technically simpler than (but equivalent to) the traditional
definition as done by Riemann.

\subsection*{Partitions and lower and upper integrals}

We want to integrate a bounded function defined on an interval $[a,b]$.
We first define two auxiliary integrals that can be defined for all
bounded functions.
Only then can we talk about the Riemann integral and
the Riemann integrable functions.

\begin{defn}
A \emph{\myindex{partition}} $P$ of the interval $[a,b]$ is
a finite set of numbers $\{ x_0,x_1,x_2,\ldots,x_n \}$ such that
\begin{equation*}
a = x_0 < x_1 < x_2 < \cdots < x_{n-1} < x_n = b .
\end{equation*}
We write
\begin{equation*}
\Delta x_i := x_i - x_{i-1} .
\end{equation*}
%"size" was never used
%We say $P$ is of size $n$.

\medskip

Let $f \colon [a,b] \to \R$ be a bounded function.
Let $P$ be a partition of
$[a,b]$.
Define
\begin{align*}
& m_i := \inf \{ f(x) : x_{i-1} \leq x \leq x_i \} , \\
& M_i := \sup \{ f(x) : x_{i-1} \leq x \leq x_i \} , \\
& L(P,f) :=
\sum_{i=1}^n m_i \Delta x_i , \\
& U(P,f) :=
\sum_{i=1}^n M_i \Delta x_i .
\end{align*}
We call $L(P,f)$ the \emph{\myindex{lower Darboux sum}} and
$U(P,f)$ the \emph{\myindex{upper Darboux sum}}\index{Darboux sum}.
\end{defn}

The geometric idea of Darboux sums is indicated in
\figureref{darbouxfig}.
The lower sum is the area of the shaded
rectangles, and the upper sum is the area of the entire
rectangles, shaded plus unshaded parts.
The width of the $i$th rectangle is $\Delta x_i$,
the height of the shaded rectangle is $m_i$ and the height
of the entire rectangle is $M_i$.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input darbouxfig.eepic
\caption{Sample Darboux sums.\label{darbouxfig}}
\end{center}
\end{figure}

\begin{prop} \label{sumulbound:prop}
Let $f \colon [a,b] \to \R$ be a bounded function.
Let $m, M \in \R$ be 
such that for all $x$ we have $m \leq f(x) \leq M$.
For any partition $P$ of $[a,b]$
we have
\begin{equation}
\label{sumulbound:eq}
m(b-a) \leq
L(P,f) \leq U(P,f)
\leq M(b-a) .
\end{equation}
\end{prop}

\begin{proof}
Let $P$ be a partition.
Then note that $m \leq m_i$ for all $i$ and
$M_i \leq M$ for all $i$.
Also $m_i \leq M_i$ for all $i$.
Finally
$\sum_{i=1}^n \Delta x_i = (b-a)$.
Therefore,
\begin{multline*}
m(b-a) =
m \left( \sum_{i=1}^n \Delta x_i \right)
=
\sum_{i=1}^n m \Delta x_i
\leq
\sum_{i=1}^n m_i \Delta x_i 
\leq
\\
\leq
\sum_{i=1}^n M_i \Delta x_i
\leq
\sum_{i=1}^n M \Delta x_i 
=
M \left( \sum_{i=1}^n \Delta x_i \right)
=
M(b-a) .
\end{multline*}
Hence we get \eqref{sumulbound:eq}.
In other words, the set of lower and
upper sums are bounded sets.
\end{proof}

%FIXME: use the following snippet idea for upper and lower integrals
%& \displaystyle{\int_{\mspace{-12mu} \underline{\mspace{10mu}} % \medspace
%a}^{b} f(x) \, dx} \\

\begin{defn}
As the sets of lower and upper Darboux sums are bounded, we define
\begin{align*}
& \underline{\int_a^b} f(x)~dx := \sup \{ L(P,f) : P \text{ a
partition of $[a,b]$} \} , \\
& \overline{\int_a^b} f(x)~dx := \inf \{ U(P,f) : P \text{ a
partition of $[a,b]$} \} .
\end{align*}
We call $\underline{\int}$ the \emph{\myindex{lower Darboux integral}} and
$\overline{\int}$ the \emph{\myindex{upper Darboux integral}}.
To avoid worrying about the variable of integration, 
we often simply write
\begin{equation*}
\underline{\int_a^b} f :=
\underline{\int_a^b} f(x)~dx 
\qquad \text{and} \qquad
\overline{\int_a^b} f :=
\overline{\int_a^b} f(x)~dx  .
\end{equation*}
\end{defn}

If integration is to make sense, then the lower and upper Darboux
integrals should be the same number, as we want a single number to call
\emph{the integral}.
However, these two integrals may in fact differ for
some functions.

\begin{example}
Take the Dirichlet function $f \colon [0,1] \to \R$, where $f(x) := 1$ if
$x \in \Q$ and $f(x) := 0$ if $x \notin \Q$.
Then
\begin{equation*}
\underline{\int_0^1} f = 0 \qquad \text{and} \qquad
\overline{\int_0^1} f = 1 .
\end{equation*}
The reason is that for every $i$ we have 
$m_i = \inf \{ f(x) : x \in [x_{i-1},x_i] \} = 0$  and
$M_i = \sup \{ f(x) : x \in [x_{i-1},x_i] \} = 1$.
Thus
\begin{align*}
& L(P,f) = \sum_{i=1}^n 0 \cdot \Delta x_i = 0 , \\
& U(P,f) = \sum_{i=1}^n 1 \cdot \Delta x_i = \sum_{i=1}^n \Delta x_i = 1  .
\end{align*}
\end{example}

\begin{remark}
The same definition of $\underline{\int_a^b} f$ and
$\overline{\int_a^b} f$
is used when $f$ is defined on a larger set $S$ such that
$[a,b] \subset S$.
In that case, we use the restriction of $f$ to $[a,b]$
and we must ensure that the restriction is bounded on $[a,b]$.
\end{remark}

To compute the integral we often take a partition $P$ and make it finer.
That is, we cut intervals in the partition into yet smaller pieces.

\begin{defn}
Let $P = \{ x_0, x_1, \ldots, x_n \}$ and
$\widetilde{P} = \{ \widetilde{x}_0, \widetilde{x}_1, \ldots, \widetilde{x}_m \}$ be
partitions of $[a,b]$.
We say $\widetilde{P}$ is a
\emph{refinement}\index{refinement of a partition} of $P$
if as sets $P \subset \widetilde{P}$.
\end{defn}

That is, $\widetilde{P}$ is a refinement of a partition if it contains all the
numbers in $P$ and perhaps some other numbers in between.
For example,
$\{ 0, 0.5, 1, 2 \}$ is a partition of $[0,2]$ and
$\{ 0, 0.2, 0.5, 1, 1.5, 1.75, 2 \}$ is a refinement.
The main reason for introducing refinements is the following proposition.

\begin{prop} \label{prop:refinement}
Let $f \colon [a,b] \to \R$ be a bounded function, and let $P$
be a partition of $[a,b]$.
Let $\widetilde{P}$ be a refinement of $P$.
Then
\begin{equation*}
L(P,f) \leq L(\widetilde{P},f) 
\qquad \text{and} \qquad
U(\widetilde{P},f) \leq U(P,f) .
\end{equation*}
\end{prop}

\begin{proof}
The tricky part of this proof is to get the notation correct.
Let $\widetilde{P} := \{ \widetilde{x}_0, \widetilde{x}_1, \ldots,
\widetilde{x}_m \}$ be
a refinement of 
$P := \{ x_0, x_1, \ldots, x_n \}$.
Then
$x_0 = \widetilde{x}_0$ and 
$x_n = \widetilde{x}_m$.
In fact, we can find integers
$k_0 < k_1 < \cdots < k_n$ such that $x_j = \widetilde{x}_{k_j}$ for
$j=0,1,2,\ldots,n$.

Let $\Delta \widetilde{x}_j = \widetilde{x}_{j-1} - \widetilde{x}_j$.
We get 
\begin{equation*}
\Delta x_j
=
\sum_{p=k_{j-1}+1}^{k_j} \Delta \widetilde{x}_p .
\end{equation*}

Let $m_j$ be as before and correspond to the partition $P$.
Let $\widetilde{m}_j := \inf \{ f(x) : \widetilde{x}_{j-1} \leq x \leq
\widetilde{x}_j \}$.
Now, $m_j \leq \widetilde{m}_p$ for $k_{j-1} < p \leq k_j$.
Therefore,
\begin{equation*}
m_j \Delta x_j
=
m_j \sum_{p=k_{j-1}+1}^{k_j} \Delta \widetilde{x}_p
=
\sum_{p=k_{j-1}+1}^{k_j} m_j \Delta \widetilde{x}_p
\leq
\sum_{p=k_{j-1}+1}^{k_j} \widetilde{m}_p \Delta \widetilde{x}_p .
\end{equation*}
So
\begin{equation*}
L(P,f) =
\sum_{j=1}^n m_j \Delta x_j
\leq
\sum_{j=1}^n
\sum_{p=k_{j-1}+1}^{k_j} \widetilde{m}_p \Delta \widetilde{x}_p
=
\sum_{j=1}^m
\widetilde{m}_j \Delta \widetilde{x}_j = L(\widetilde{P},f).
\end{equation*}

The proof of $U(\widetilde{P},f) \leq U(P,f)$ is left as an exercise.
\end{proof}

Armed with refinements we prove the following.
The key point of this next proposition is that
the lower Darboux integral is less than or equal to the upper Darboux
integral.

\begin{prop} \label{intulbound:prop}
Let $f \colon [a,b] \to \R$ be a bounded function.
Let $m, M \in \R$ be 
such that for all $x \in [a,b]$ we have $m \leq f(x) \leq M$.
Then
\begin{equation}
\label{intulbound:eq}
m(b-a) \leq
\underline{\int_a^b} f \leq \overline{\int_a^b} f
\leq M(b-a) .
\end{equation}
\end{prop}

\begin{proof}
By \propref{sumulbound:prop} we have for any partition $P$
\begin{equation*}
m(b-a) \leq L(P,f) \leq U(P,f) \leq M(b-a).
\end{equation*}
The inequality
$m(b-a) \leq L(P,f)$ implies $m(b-a) \leq \underline{\int_a^b} f$.
Also
$U(P,f) \leq M(b-a)$ implies $\overline{\int_a^b} f \leq M(b-a)$.

The key point of this proposition is the middle inequality in
\eqref{intulbound:eq}.
Let $P_1, P_2$ be partitions of $[a,b]$.
Define 
$\widetilde{P} := P_1 \cup P_2$.
The set $\widetilde{P}$ is a partition of $[a,b]$.
Furthermore,
$\widetilde{P}$ is a refinement of $P_1$ and it is also a refinement of $P_2$.
By \propref{prop:refinement}
we have $L(P_1,f) \leq L(\widetilde{P},f)$ and
$U(\widetilde{P},f) \leq U(P_2,f)$.
Putting it all together we have
\begin{equation*}
L(P_1,f) \leq L(\widetilde{P},f) \leq U(\widetilde{P},f) \leq U(P_2,f) .
\end{equation*}
In other words, for two arbitrary partitions $P_1$ and $P_2$ we have
$L(P_1,f) \leq U(P_2,f)$.
Now we recall \propref{infsupineq:prop}.
Taking the supremum and
infimum over all partitions we get
\begin{equation*}
\sup \{ L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\leq
\inf \{ U(P,f) : \text{$P$ a partition of $[a,b]$} \} .
\end{equation*}
In other words $\underline{\int_a^b} f \leq \overline{\int_a^b} f$.
\end{proof}

\subsection*{Riemann integral}

We can finally define the Riemann integral.
However, the Riemann
integral is only defined on a certain class of functions, called the
Riemann integrable functions.

\begin{defn}
Let $f \colon [a,b] \to \R$ be a bounded function such that
\begin{equation*}
\underline{\int_a^b} f(x)~dx = \overline{\int_a^b} f(x)~dx .
\end{equation*}
Then $f$ is said to be \emph{\myindex{Riemann integrable}}.
The set of Riemann integrable functions on $[a,b]$ is denoted
by $\sR[a,b]$.
When $f \in \sR[a,b]$ we define
\begin{equation*}
\int_a^b f(x)~dx := 
\underline{\int_a^b} f(x)~dx = \overline{\int_a^b} f(x)~dx .
\end{equation*}
As before, we often simply write
\begin{equation*}
\int_a^b f := \int_a^b f(x)~dx.
\end{equation*}
The number $\int_a^b f$ is called the \emph{\myindex{Riemann integral}}
of $f$, or sometimes simply the \emph{integral} of $f$.
\end{defn}

By definition, any Riemann integrable function is bounded.
By appealing to \propref{intulbound:prop} we immediately obtain
the following proposition.

\begin{prop} \label{intbound:prop}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.
Let $m, M \in \R$ be 
such that $m \leq f(x) \leq M$ for all $x \in [a,b]$.
Then
\begin{equation*}
m(b-a) \leq
\int_a^b f
\leq M(b-a) .
\end{equation*}
\end{prop}

Often we use a weaker form of this proposition.
That is, if
$\abs{f(x)} \leq M$ for all $x \in [a,b]$, then
\begin{equation*}
\abs{\int_a^b f} \leq M(b-a) .
\end{equation*}

\begin{example}
We integrate constant functions using
\propref{intulbound:prop}.
If $f(x) := c$ for some constant $c$, then we take $m = M = c$.
In inequality \eqref{intulbound:eq}
all the inequalities must be equalities.
Thus $f$ is integrable on $[a,b]$ and $\int_a^b f = c(b-a)$.
\end{example}

\begin{example}
Let $f \colon [0,2] \to \R$ be defined by
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x < 1$,}\\
\nicefrac{1}{2} & \text{ if $x = 1$,}\\
0 & \text{ if $x > 1$.}
\end{cases}
\end{equation*}
We claim $f$ is Riemann integrable and $\int_0^2 f = 1$.

\begin{proof} Let $0 < \epsilon < 1$ be arbitrary.
Let $P := \{0, 1-\epsilon, 1+\epsilon, 2\}$ be a partition.
We use the notation from
the definition of the Darboux sums.
Then
\begin{align*}
m_1 &= \inf \{ f(x) : x \in [0,1-\epsilon] \} = 1 , & 
M_1 &= \sup \{ f(x) : x \in [0,1-\epsilon] \} = 1 , \\
m_2 &= \inf \{ f(x) : x \in [1-\epsilon,1+\epsilon] \} = 0 , & 
M_2 &= \sup \{ f(x) : x \in [1-\epsilon,1+\epsilon] \} = 1 , \\
m_3 &= \inf \{ f(x) : x \in [1+\epsilon,2] \} = 0 , & 
M_3 &= \sup \{ f(x) : x \in [1+\epsilon,2] \} = 0 .
\end{align*}
Furthermore, $\Delta x_1 = 1-\epsilon$, $\Delta x_2 = 2\epsilon$ and
$\Delta x_3 = 1-\epsilon$.
We compute
\begin{align*}
& L(P,f) = \sum_{i=1}^3 m_i \Delta x_i =
1 \cdot (1-\epsilon) + 0 \cdot 2\epsilon + 0 \cdot (1-\epsilon)
= 1-\epsilon , \\
& U(P,f) = \sum_{i=1}^3 M_i \Delta x_i =
1 \cdot (1-\epsilon) + 1 \cdot 2\epsilon + 0 \cdot (1-\epsilon)
= 1+\epsilon .
\end{align*}
Thus,
\begin{equation*}
\overline{\int_0^2} f - 
\underline{\int_0^2} f
\leq
U(P,f) - L(P,f)
=
(1+\epsilon)
- (1-\epsilon) = 2 \epsilon .
\end{equation*}
By \propref{intulbound:prop} we have $\underline{\int_0^2} f \leq \overline{\int_0^2} f$.
As $\epsilon$ was arbitrary we see 
$\overline{\int_0^2} f = \underline{\int_0^2} f$.
So $f$ is Riemann
integrable.
Finally,
\begin{equation*}
1-\epsilon = L(P,f) \leq \int_0^2 f \leq U(P,f) =
1+\epsilon.
\end{equation*}
Hence, $\bigl\lvert \int_0^2 f - 1 \bigr\rvert \leq \epsilon$.
As $\epsilon$ was arbitrary,
we have $\int_0^2 f = 1$.
\end{proof}
\end{example}

It may be worthwhile to extract part of the technique of the example into a
proposition.

\begin{prop}
Let $f \colon [a,b] \to \R$ be a bounded function.
Then $f$ is Riemann
integrable if for every $\epsilon > 0$, there exists a partition $P$ such that
\begin{equation*}
U(P,f) - L(P,f) < \epsilon .
\end{equation*}
\end{prop}

\begin{proof}
If for every $\epsilon > 0$, a $P$ exists we have:
\begin{equation*}
0 \leq
\overline{\int_a^b} f - 
\underline{\int_a^b} f
\leq
U(P,f) - L(P,f) < \epsilon .
\end{equation*}
Therefore, 
$\overline{\int_a^b} f = \underline{\int_a^b} f$, and $f$ is integrable.
\end{proof}

\begin{example}
Let us show $\frac{1}{1+x}$ is integrable on $[0,b]$ for any $b > 0$.
We will see later that all continuous functions are integrable, but let us
demonstrate how we do it directly.

Let $\epsilon > 0$ be given.
Take $n \in \N$ and
pick $x_j := \nicefrac{jb}{n}$, to form the 
partition $P := \{ x_0,x_1,\ldots,x_n \}$ of $[0,b]$.
We have $\Delta x_j = \nicefrac{b}{n}$ for all $j$.

As $f$ is decreasing, for any subinterval $[x_{j-1},x_j]$ we obtain
\begin{equation*}
m_j = \inf \left\{ \frac{1}{1+x} : x \in [x_{j-1},x_j] \right\} = \frac{1}{1+x_j} ,
\qquad
M_j = \sup \left\{ \frac{1}{1+x} : x \in [x_{j-1},x_j] \right\} =
\frac{1}{1+x_{j-1}} .
\end{equation*}
Then we have
\begin{multline*}
U(P,f)-L(P,f)
=
\sum_{j=1}^n
\Delta x_j
(M_j-m_j)
=
\\
=
\frac{b}{n}
\sum_{j=1}^n 
\left( \frac{1}{1+\nicefrac{(j-1)b}{n}} - \frac{1}{1+\nicefrac{jb}{n}} \right) 
=
\frac{b}{n}
\left( \frac{1}{1+\nicefrac{0b}{n}} - \frac{1}{1+\nicefrac{nb}{n}} \right) 
=
\frac{b^2}{n(b+1)} .
\end{multline*}
The sum telescopes, the terms successively cancel each other, something
we have seen before.
Picking $n$ to be such that
$\frac{b^2}{n(b+1)} < \epsilon$ the proposition is satisfied and the
function is integrable.
\end{example}

\subsection*{More notation}

When $f \colon S \to \R$ is defined on a larger set $S$ and
$[a,b] \subset S$,
we say $f$ is Riemann integrable on $[a,b]$ if the restriction of $f$
to $[a,b]$ is Riemann integrable. 
In this case,
we say $f \in \sR[a,b]$,
and
we write $\int_a^b f$ to mean the Riemann integral
of the restriction of $f$ to $[a,b]$.

It is useful to define the integral $\int_a^b f$ even if
$a \not< b$.
Suppose $b < a$ and $f \in \sR[b,a]$,
then define
\begin{equation*}
\int_a^b f := - \int_b^a f .
\end{equation*}
For any function $f$ we define
\begin{equation*}
\int_a^a f := 0 .
\end{equation*}

At times, the variable $x$ may already have some other meaning.
When
we need to write down the variable of integration, we may simply
use a different letter.
For example,
\begin{equation*}
\int_a^b f(s)~ds := \int_a^b f(x)~dx .
\end{equation*}

\subsection*{Exercises}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be defined by $f(x) := x^3$
and let $P := \{ 0, 0.1, 0.4, 1 \}$.
Compute $L(P,f)$ and $U(P,f)$.
\end{exercise}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be defined by $f(x) := x$.
Show that $f \in \sR[0,1]$ and
compute $\int_0^1 f$ using the definition of the integral
(but
feel free to use the propositions of this section).%\propref{intulbound:prop}).
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be a bounded function.
Suppose there exists a sequence of partitions $\{ P_k \}$ of $[a,b]$
such that
\begin{equation*}
\lim_{k \to \infty} \bigl( U(P_k,f) - L(P_k,f) \bigr) = 0 .
\end{equation*}
Show that $f$ is Riemann integrable and that
\begin{equation*}
\int_a^b f = 
\lim_{k \to \infty} U(P_k,f)
=
\lim_{k \to \infty} L(P_k,f) .
\end{equation*}
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:refinement}.
\end{exercise}

\begin{exercise}
Suppose $f \colon [-1,1] \to \R$ is defined as
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{ if $x > 0$,} \\
0 & \text{ if $x \leq 0$.}
\end{cases}
\end{equation*}
Prove that $f \in \sR[-1,1]$ and
compute $\int_{-1}^1 f$ using the definition of the integral
(but
feel free to use the propositions of this section).
%(feel free to use \propref{intulbound:prop}).
\end{exercise}

\begin{exercise}
Let $c \in (a,b)$ and let $d \in \R$.
Define $f \colon [a,b] \to \R$ as
\begin{equation*}
f(x) :=
\begin{cases}
d & \text{ if $x = c$,} \\
0 & \text{ if $x \not= c$.}
\end{cases}
\end{equation*}
Prove that $f \in \sR[a,b]$ and
compute
$\int_a^b f$ using the definition of the integral
%(feel free to use \propref{intulbound:prop}).
(but
feel free to use the propositions of this section).
\end{exercise}

\begin{exercise} \label{exercise:taggedpartition}
Suppose $f \colon [a,b] \to \R$ is Riemann integrable.
Let $\epsilon
> 0$ be given.
Then show that there exists a partition $P = \{ x_0, x_1,
\ldots, x_n \}$
such that if we
pick any set of numbers $\{ c_1, c_2, \ldots, c_n \}$ with
$c_k \in [x_{k-1},x_k]$ for all $k$, then
\begin{equation*}
\abs{\int_a^b f - \sum_{k=1}^n f(c_k) \Delta x_k} < \epsilon .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.
Let $\alpha > 0$ and $\beta \in \R$.
Then define $g(x) := f(\alpha x + \beta)$ on the interval
$I = [\frac{a-\beta}{\alpha}, \frac{b-\beta}{\alpha}]$.
Show
that $g$ is Riemann integrable on $I$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,1] \to \R$ and $g \colon [0,1] \to \R$
are such that for all $x \in (0,1]$
we have $f(x) = g(x)$.
Suppose $f$ is Riemann integrable. 
Prove $g$ is Riemann integrable and $\int_{0}^1 f = \int_{0}^1 g$.
\end{exercise}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be a bounded function.
Let $P_n = \{ x_0,x_1,\ldots,x_n \}$ be a uniform partition of $[0,1]$,
that is, $x_j := \nicefrac{j}{n}$.
Is $\{ L(P_n,f) \}_{n=1}^\infty$
always monotone?
  Yes/No: Prove or find a counterexample.
\end{exercise}

\begin{exercise}[Challenging]
For a bounded function $f \colon [0,1] \to \R$ let
$R_n := (\nicefrac{1}{n})\sum_{j=1}^n f(\nicefrac{j}{n})$ (the
uniform right hand rule).
\begin{enumerate}[a)]
 \item If $f$ is Riemann integrable show $\int_0^1 f = \lim \, R_n$.
 \item Find an $f$ that is not Riemann integrable, but $\lim \, R_n$ exists.
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:riemannintdarboux}
Generalize the previous exercise.
Show that $f \in \sR[a,b]$ if and only if there exists an $I \in \R$,
such that for every $\epsilon > 0$ there exists
a $\delta > 0$ such that if $P$ is a partition with $\Delta x_i < \delta$
for all $i$, then
$\abs{L(P,f) - I} < \epsilon$ and
$\abs{U(P,f) - I} < \epsilon$.
If $f \in \sR[a,b]$, then $I = \int_a^b f$.
\end{exercise}

\begin{exercise} \label{exercise:riemannintdarboux2}
Using \exerciseref{exercise:riemannintdarboux} and the idea of
the proof in \exerciseref{exercise:taggedpartition}, show that 
Darboux integral is the same as the standard definition
of Riemann integral, which you have most likely seen in calculus.
That is,
show that
$f \in \sR[a,b]$ if and only if there exists an $I \in \R$,
such that for every $\epsilon > 0$ there exists
a $\delta > 0$ such that if $P = \{ x_0,x_1,\ldots,x_n \}$
is a partition with $\Delta x_i < \delta$
for all $i$, then
$\abs{\sum_{i=1}^n f(c_i) \Delta x_i - I} < \epsilon$ for any set
$\{ c_1,c_2,\ldots,c_n \}$ with $c_i \in [x_{i-1},x_i]$.
If $f \in \sR[a,b]$, then $I = \int_a^b f$.
\end{exercise}


\begin{exercise}[Challenging]
Find an example of functions
$f \colon [0,1] \to \R$ which is Riemann integrable,
and $g \colon [0,1] \to [0,1]$ which is one-to-one and onto,
such that the composition $f \circ g$ is not Riemann integrable.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Properties of the integral}
\label{sec:rintprop}

\sectionnotes{2 lectures, integrability of functions with 
discontinuities can safely be skipped}

\subsection*{Additivity}

The next result we prove is usually referred to as the
\myindex{additive property of the integral}.
First we prove the additivity
property for the lower and upper Darboux integrals.

\begin{lemma} \label{lemma:darbouxadd}
Suppose $a < b < c$ and $f \colon [a,c] \to \R$ is a bounded function.
Then
\begin{equation*}
\underline{\int_a^c} f
=
\underline{\int_a^b} f
+
\underline{\int_b^c} f
\end{equation*}
and
\begin{equation*}
\overline{\int_a^c} f
=
\overline{\int_a^b} f
+
\overline{\int_b^c} f .
\end{equation*}
\end{lemma}

\begin{proof}
If we have partitions $P_1 = \{ x_0,x_1,\ldots,x_k \}$
of $[a,b]$ and $P_2 = \{ x_k, x_{k+1}, \ldots, x_n \}$ of $[b,c]$,
then the set $P := P_1 \cup P_2 = \{ x_0, x_1, \ldots, x_n \}$ is
a partition of $[a,c]$.
Then
\begin{equation*}
L(P,f) =
\sum_{j=1}^n m_j \Delta x_j
=
\sum_{j=1}^k m_j \Delta x_j
+
\sum_{j=k+1}^n m_j \Delta x_j
=
L(P_1,f) + L(P_2,f) .
\end{equation*}
When we take the supremum of the right hand side over all $P_1$ and $P_2$,
we are taking a supremum of the left hand side
over all partitions $P$ of $[a,c]$ that contain $b$.
If $Q$ is any partition
of $[a,c]$ and $P = Q \cup \{ b \}$, then $P$ is a refinement of $Q$
and so $L(Q,f) \leq L(P,f)$.
Therefore, taking a supremum only over the $P$
that contain $b$ is sufficient to find the supremum of $L(P,f)$
over all partitions $P$, see \exerciseref{exercise:dominatingb}.
Finally recall \exerciseref{exercise:supofsum}
to compute
\begin{equation*}
\begin{split}
\underline{\int_a^c} f
& =
\sup \{ L(P,f) : \text{$P$ a partition of $[a,c]$} \}
\\
& =
\sup \{ L(P,f) : \text{$P$ a partition of $[a,c]$, $b \in P$} \}
\\
& =
\sup \{ L(P_1,f) + L(P_2,f) :
\text{$P_1$ a partition of $[a,b]$, $P_2$ a partition of $[b,c]$} \}
\\
& =
\sup \{ L(P_1,f) : \text{$P_1$ a partition of $[a,b]$} \}
+
\sup \{ L(P_2,f) : \text{$P_2$ a partition of $[b,c]$} \}
\\
&=
\underline{\int_a^b} f + \underline{\int_b^c} f .
\end{split}
\end{equation*}

Similarly, for $P$, $P_1$, and $P_2$ as above we obtain
\begin{equation*}
U(P,f) =
\sum_{j=1}^n M_j \Delta x_j
=
\sum_{j=1}^k M_j \Delta x_j
+
\sum_{j=k+1}^n M_j \Delta x_j
=
U(P_1,f) + U(P_2,f) .
\end{equation*}
We wish to take the infimum on the right
over all $P_1$ and $P_2$, and so we are taking the infimum
over all partitions $P$ of $[a,c]$ that contain $b$.
If $Q$ is any partition
of $[a,c]$ and $P = Q \cup \{ b \}$, then $P$ is a refinement of $Q$
and so $U(Q,f) \geq U(P,f)$.
Therefore, taking an infimum only over the $P$
that contain $b$ is sufficient to find the infimum of $U(P,f)$ for
all $P$.
We obtain
\begin{equation*}
\overline{\int_a^c} f
=
\overline{\int_a^b} f + \overline{\int_b^c} f .  \qedhere
\end{equation*}
\end{proof}

\begin{thm}
Let $a < b < c$.
A function $f \colon [a,c] \to \R$ is Riemann integrable
if and only if $f$ is Riemann integrable on $[a,b]$ and $[b,c]$.
If
$f$ is Riemann integrable, then
\begin{equation*}
\int_a^c f
=
\int_a^b f
+
\int_b^c f .
\end{equation*}
\end{thm}

\begin{proof}
Suppose $f \in \sR[a,c]$, then 
$\overline{\int_a^c} f = 
\underline{\int_a^c} f = 
\int_a^c f$.
We apply the lemma to get
\begin{equation*}
\int_a^c f
=
\underline{\int_a^c} f
 =
\underline{\int_a^b} f + \underline{\int_b^c} f
 \leq
\overline{\int_a^b} f + \overline{\int_b^c} f
 =
\overline{\int_a^c} f
 =
\int_a^c f .
\end{equation*}
Thus the inequality is an equality and
\begin{equation*}
\underline{\int_a^b} f + \underline{\int_b^c} f
=
\overline{\int_a^b} f + \overline{\int_b^c} f .
\end{equation*}
As we also know 
$\underline{\int_a^b} f \leq \overline{\int_a^b} f$
and
$\underline{\int_b^c} f \leq \overline{\int_b^c} f$, we 
conclude 
\begin{equation*}
\underline{\int_a^b} f
=
\overline{\int_a^b} f
\qquad \text{and} \qquad
\underline{\int_b^c} f
=
\overline{\int_b^c} f .
\end{equation*}
Thus $f$ is Riemann integrable on $[a,b]$ and $[b,c]$ and the desired formula
holds.

Now assume the restrictions of $f$ to $[a,b]$ and to $[b,c]$
are Riemann integrable.
We again apply the lemma to get
\begin{equation*}
\underline{\int_a^c} f
=
\underline{\int_a^b} f + \underline{\int_b^c} f
=
\int_a^b f + \int_b^c f
=
\overline{\int_a^b} f + \overline{\int_b^c} f
=
\overline{\int_a^c} f .
\end{equation*}
Therefore $f$ is Riemann integrable on $[a,c]$, and the integral is computed
as indicated.
\end{proof}

An easy consequence of the additivity is the following corollary.
We
leave the details to the reader as an exercise.

\begin{cor} \label{intsubcor}
If $f \in \sR[a,b]$ and
$[c,d] \subset [a,b]$, then
the restriction $f|_{[c,d]}$ is in $\sR[c,d]$.
\end{cor}

\subsection*{Linearity and monotonicity}

\begin{prop}[Linearity]
\index{linearity of the integral}
Let $f$ and $g$ be in $\sR[a,b]$ and $\alpha \in \R$.
\begin{enumerate}[(i)]
\item $\alpha f$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \alpha f(x) ~dx = \alpha \int_a^b f(x) ~dx .
\end{equation*}
\item $f+g$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \bigl( f(x)+g(x) \bigr) ~dx = 
\int_a^b f(x) ~dx 
+
\int_a^b g(x) ~dx .
\end{equation*}
\end{enumerate}
\end{prop}

\begin{proof}
Let us prove the first item.
First suppose $\alpha \geq 0$.
Let $P$ be a partition of $[a,b]$.
Let $m_i := \inf \{ f(x) : x \in [x_{i-1},x_i] \}$ as usual.
Since $\alpha$ is nonnegative, we can move the multiplication by $\alpha$
past the infimum,
\begin{equation*}
\inf \{ \alpha f(x) : x \in [x_{i-1},x_i] \}
=
\alpha \inf \{ f(x) : x \in [x_{i-1},x_i] \} = \alpha m_i .
\end{equation*}
Therefore
\begin{equation*}
L(P,\alpha f) =
\sum_{i=1}^n \alpha m_i \Delta_i = \alpha \sum_{i=1}^n m_i \Delta_i = \alpha
L(P,f).
\end{equation*}
Similarly 
\begin{equation*}
U(P,\alpha f) = \alpha U(P,f) .
\end{equation*}
Again, as $\alpha \geq 0$ we
may move multiplication by $\alpha$ past the supremum.
Hence,
\begin{equation*}
\begin{split}
\underline{\int_a^b} \alpha f(x)~dx & =
\sup \{ L(P,\alpha f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\sup \{ \alpha L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\alpha
\sup \{ L(P,f) : \text{$P$ a partition of $[a,b]$} \}
\\
& =
\alpha
\underline{\int_a^b} f(x)~dx .
\end{split}
\end{equation*}
Similarly we show 
\begin{equation*}
\overline{\int_a^b} \alpha f(x)~dx
=
\alpha
\overline{\int_a^b} f(x)~dx .
\end{equation*}
The conclusion now follows for $\alpha \geq 0$.

To finish the proof of the first item, we need to show 
that $-f$ is Riemann integrable and
$\int_a^b - f(x)~dx =
-
\int_a^b f(x)~dx$.
The proof of this fact is left as an exercise.

The proof of the second item in the proposition is also left as an exercise.
It is not as
trivial as it may appear at first glance.
\end{proof}

We should note that the second item in the proposition does not hold with
equality for the Darboux integrals.
For arbitrary bounded functions $f$ and $g$ we
only obtain
\begin{equation*}
%\overline{\int_a^b} \bigl(f(x)+g(x)\bigr)~dx \leq
%\overline{\int_a^b}f(x)~dx+\overline{\int_a^b}g(x)~dx
\overline{\int_a^b} (f+g) \leq \overline{\int_a^b}f+\overline{\int_a^b}g
,
\qquad
\text{and}
\qquad
\underline{\int_a^b} (f+g) \geq \underline{\int_a^b}f+\underline{\int_a^b}g
%\underline{\int_a^b} \bigl(f(x)+g(x)\bigr)~dx \geq
%\underline{\int_a^b}f(x)~dx+\underline{\int_a^b}g(x)~dx
.
\end{equation*}
See \exerciseref{exercise:upperlowerlinineq}.

\begin{prop}[Monotonicity]
\index{monotonicity of the integral}
Let $f$ and $g$ be in $\sR[a,b]$ and let $f(x) \leq g(x)$
for all $x \in [a,b]$.
Then
\begin{equation*}
\int_a^b f 
\leq
\int_a^b g .
\end{equation*}
\end{prop}

\begin{proof}
Let $P = \{ x_0, x_1, \ldots, x_n \}$ be a partition of $[a,b]$.
Then
let
\begin{equation*}
m_i := \inf \{ f(x) : x \in [x_{i-1},x_i] \}
\qquad \text{and} \qquad
\widetilde{m}_i := \inf \{ g(x) : x \in [x_{i-1},x_i] \} .
\end{equation*}
As $f(x) \leq g(x)$, then $m_i \leq \widetilde{m}_i$.
Therefore,
\begin{equation*}
L(P,f)
=
\sum_{i=1}^n m_i \Delta x_i
\leq
\sum_{i=1}^n \widetilde{m}_i \Delta x_i
=
L(P,g) .
\end{equation*}
We take the supremum over all $P$ (see \propref{prop:funcsupinf}) to obtain 
\begin{equation*}
\underline{\int_a^b} f 
\leq
\underline{\int_a^b} g .
\end{equation*}
As $f$ and $g$ are Riemann integrable, the conclusion follows.
\end{proof}

\subsection*{Continuous functions}

Let us show that continuous functions are Riemann integrable.
In fact we
will show we can even allow some discontinuities.
We start with a
function continuous on the whole closed interval $[a,b]$.

\begin{lemma} \label{lemma:contint}
If $f \colon [a,b] \to \R$ is a continuous function,
then $f \in \sR[a,b]$.
\end{lemma}

\begin{proof}
As $f$ is continuous on a closed bounded interval, it is
uniformly continuous.
Let $\epsilon > 0$ be given.
Find a $\delta > 0$ such that
$\abs{x-y} < \delta$ implies $\abs{f(x)-f(y)} < \frac{\epsilon}{b-a}$.

Let $P = \{ x_0, x_1, \ldots, x_n \}$
be a partition of $[a,b]$ such that $\Delta x_i < \delta$ for all $i = 1,2,
\ldots, n$.
For example,
take $n$ such that $\frac{b-a}{n} < \delta$ and
let $x_i := \frac{i}{n}(b-a) + a$.
Then for all $x, y \in [x_{i-1},x_i]$ we have 
$\abs{x-y} \leq \Delta x_i < \delta$ and so
\begin{equation*}
f(x)-f(y) \leq \abs{f(x)-f(y)} < \frac{\epsilon}{b-a} .
\end{equation*}
As $f$ is continuous on $[x_{i-1},x_i]$, it attains a maximum and a minimum
on this interval.
Let $x$ be a point where $f$ attains the maximum and $y$ be a point
where $f$ attains the minimum.
Then $f(x) = M_i$
and $f(y) = m_i$ in the notation from the definition of the integral.
Therefore,
\begin{equation*}
M_i-m_i = f(x)-f(y) < 
\frac{\epsilon}{b-a} .
\end{equation*}
And so
\begin{equation*}
\begin{split}
\overline{\int_a^b} f - 
\underline{\int_a^b} f 
& \leq
U(P,f) - L(P,f)
\\
& =
\left(
\sum_{i=1}^n
M_i \Delta x_i
\right)
-
\left(
\sum_{i=1}^n
m_i \Delta x_i
\right)
\\
& =
\sum_{i=1}^n
(M_i-m_i) \Delta x_i
\\
& <
\frac{\epsilon}{b-a}
\sum_{i=1}^n
\Delta x_i
\\
& =
\frac{\epsilon}{b-a} (b-a)
= \epsilon .
\end{split}
\end{equation*}
As $\epsilon > 0$ was arbitrary,
\begin{equation*}
\overline{\int_a^b} f = \underline{\int_a^b} f ,
\end{equation*}
and $f$ is Riemann integrable on $[a,b]$.
\end{proof}

The second lemma says that we need the function to only be ``Riemann integrable
inside the interval,'' as long as it is bounded.
It also tells us how to
compute the integral.

\begin{lemma} \label{lemma:boundedimpriemann}
Let $f \colon [a,b] \to \R$ be a bounded function that is Riemann
integrable on $[a',b']$ for all $a',b'$ such that $a < a' < b' < b$.
Then $f \in \sR[a,b]$.
Furthermore, if $a < a_n < b_n < b$ are such that
$\lim \, a_n = a$ and $\lim \, b_n = b$, then
\begin{equation*}
\int_a^b f = 
\lim_{n \to \infty} \int_{a_n}^{b_n} f .
\end{equation*}
\end{lemma}

\begin{proof}
Let $M > 0$ be a real number such that $\abs{f(x)} \leq M$.
Pick two
sequences of numbers $a < a_n < b_n < b$ such that $\lim\, a_n = a$
and $\lim\, b_n = b$.
Note $M > 0$ and $(b-a) \geq (b_n-a_n)$.
Thus
\begin{equation*}
-M(b-a) \leq
-M(b_n-a_n) \leq
\int_{a_n}^{b_n} f
\leq
M(b_n-a_n) \leq
M(b-a) .
\end{equation*}
Therefore the sequence of numbers
$\{ \int_{a_n}^{b_n} f \}_{n=1}^\infty$ is bounded and by
\hyperref[thm:bwseq]{Bolzano--Weierstrass}
has a convergent subsequence indexed by $n_k$.
Let us call
$L$ the limit of the subsequence
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}_{k=1}^\infty$.

\lemmaref{lemma:darbouxadd} says that
the lower and upper integral are additive
and the hypothesis says that
$f$ is integrable on $[a_{n_k},b_{n_k}]$.
Therefore
\begin{equation*}
\underline{\int_a^b} f
=
\underline{\int_a^{a_{n_k}}} f
+
\int_{a_{n_k}}^{b_{n_k}} f
+
\underline{\int_{b_{n_k}}^b} f
\geq
-M(a_{n_k}-a)
+
\int_{a_{n_k}}^{b_{n_k}} f
-
M(b-b_{n_k}) .
\end{equation*}
We take the limit as $k$ goes to $\infty$ on the right-hand side,
\begin{equation*}
\underline{\int_a^b} f
\geq
-M\cdot 0
+
L
-
M\cdot 0
= L .
\end{equation*}

Next we use additivity of the upper integral,
\begin{equation*}
\overline{\int_a^b} f
=
\overline{\int_a^{a_{n_k}}} f
+
\int_{a_{n_k}}^{b_{n_k}} f
+
\overline{\int_{b_{n_k}}^b} f
\leq
M(a_{n_k}-a)
+
\int_{a_{n_k}}^{b_{n_k}} f
+
M(b-b_{n_k}) .
\end{equation*}
We take the same subsequence 
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}_{k=1}^\infty$ and take the limit 
to obtain
\begin{equation*}
\overline{\int_a^b} f
\leq
M\cdot 0
+
L
+
M\cdot 0
= L .
\end{equation*}
Thus $\overline{\int_a^b} f = \underline{\int_a^b} f = L$
and hence $f$ is Riemann integrable and $\int_a^b f = L$.
In particular, no matter what sequences $\{ a_n \}$ and
$\{b_n\}$ we started with and what subsequence we chose,
the $L$ is the same number.

To prove the final statement of the lemma we use 
\thmref{seqconvsubseqconv:thm}.
We have shown that every convergent
subsequence
$\{ \int_{a_{n_k}}^{b_{n_k}} f \}$ converges to $L = \int_a^b f$.
Therefore, the sequence
$\{ \int_{a_n}^{b_n} f \}$ is convergent and converges to $L$.
\end{proof}

We say a function $f \colon [a,b] \to \R$ has \emph{\myindex{finitely many
discontinuities}} if there exists a finite set $S := \{ x_1, x_2, \ldots, x_n \}
\subset [a,b]$, and $f$ is continuous
at all points of $[a,b] \setminus S$.

\begin{thm}
Let $f \colon [a,b] \to \R$ be a bounded function with finitely
many discontinuities.
Then $f \in \sR[a,b]$.
\end{thm}

\begin{proof}
We divide the interval into finitely many intervals $[a_i,b_i]$
so that $f$ is continuous
on the interior $(a_i,b_i)$.
If $f$ is continuous on $(a_i,b_i)$,
then it is continuous and hence integrable on $[c_i,d_i]$ whenever $a_i < c_i < d_i < b_i$.  
By \lemmaref{lemma:boundedimpriemann}
the restriction
of $f$ to $[a_i,b_i]$ is integrable.
By additivity of the integral (and
\hyperref[induction:thm]{induction}) $f$ is integrable on the union of the intervals.
\end{proof}

Sometimes it is convenient (or necessary)
to change certain values of a function and
then integrate.
The next result says
that if we change the values only at finitely
many points, the integral does not change.

\begin{prop}
Let $f \colon [a,b] \to \R$ be Riemann integrable.
Let $g \colon [a,b] \to
\R$ be a function such that $f(x) = g(x)$ for all $x \in [a,b] \setminus S$,
where $S$ is a finite set.
Then $g$ is a Riemann integrable function
and
\begin{equation*}
\int_a^b g = \int_a^b f.
\end{equation*}
\end{prop}

\begin{proof}[Sketch of proof]
Using additivity of the integral, we split up the interval $[a,b]$ into
smaller intervals such that $f(x) = g(x)$ holds for all $x$ except at the
endpoints (details are left to the reader).

Therefore, without loss of generality suppose $f(x) = g(x)$ for
all $x \in (a,b)$.
The proof follows by \lemmaref{lemma:boundedimpriemann},
and is left as an exercise.
\end{proof}


\subsection*{Exercises}

\begin{exercise}
Let $f$ be in $\sR[a,b]$.
Prove that
$-f$ is in $\sR[a,b]$ and 
\begin{equation*}
\int_a^b - f(x) ~dx = - \int_a^b f(x) ~dx .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f$ and $g$ be in $\sR[a,b]$.
Prove that $f+g$ is in $\sR[a,b]$ and
\begin{equation*}
\int_a^b \bigl( f(x)+g(x) \bigr) ~dx = 
\int_a^b f(x) ~dx 
+
\int_a^b g(x) ~dx .
\end{equation*}
Hint: Use \propref{prop:refinement} to find a single partition $P$
such that $U(P,f)-L(P,f) < \nicefrac{\epsilon}{2}$ and
$U(P,g)-L(P,g) < \nicefrac{\epsilon}{2}$.
\end{exercise}

\begin{exercise}
Let $f \colon [a,b] \to \R$ be Riemann integrable.
Let $g \colon [a,b] \to
\R$ be a function such that $f(x) = g(x)$ for all $x \in (a,b)$.
Prove that $g$ is Riemann integrable and that
\begin{equation*}
\int_a^b g = \int_a^b f.
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{mean value theorem for integrals}}.
That is,
prove that if $f \colon [a,b] \to \R$ is continuous, then there exists
a $c \in [a,b]$ such that $\int_a^b f = f(c)(b-a)$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ is a continuous function such that $f(x) \geq 0$
for all $x \in [a,b]$ and $\int_a^b f = 0$.
Prove that $f(x) = 0$
for all $x$.
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ is a continuous function
for all $x \in [a,b]$ and $\int_a^b f = 0$.
Prove that
there exists a $c \in [a,b]$ such that $f(c) = 0$ (Compare with the
previous exercise).
\end{exercise}

\begin{exercise}
If $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$
are continuous functions such that $\int_a^b f = \int_a^b g$.
Then show that there exists a $c \in [a,b]$ such that $f(c) = g(c)$.
\end{exercise}

\begin{exercise}
Let $f \in \sR[a,b]$.
Let $\alpha, \beta, \gamma$ be arbitrary numbers in
$[a,b]$ (not necessarily ordered in any way).
Prove 
\begin{equation*}
\int_\alpha^\gamma f =
\int_\alpha^\beta f +
\int_\beta^\gamma f .
\end{equation*}
Recall what $\int_a^b f$ means if $b \leq a$.
\end{exercise}

\begin{exercise}
Prove \corref{intsubcor}.
\end{exercise}

\begin{exercise} \label{exercise:easyabsint}
Suppose $f \colon [a,b] \to \R$ is bounded and
has finitely many discontinuities.
Show that as a function of $x$ the expression $\abs{f(x)}$ is bounded with finitely many
discontinuities and is thus Riemann integrable.
Then show 
\begin{equation*}
\abs{\int_a^b f(x)~dx} \leq \int_a^b \abs{f(x)}~dx .
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Show that the
Thomae\index{Thomae function} or \myindex{popcorn function}
(see \exampleref{popcornfunction:example})
is Riemann integrable.
Therefore, there exists a
function discontinuous at all rational numbers (a dense set)
that is Riemann integrable.

In particular,
define $f \colon [0,1] \to \R$ by
\begin{equation*}
f(x) := 
\begin{cases}
\nicefrac{1}{k} & \text{ if $x=\nicefrac{m}{k}$ where $m,k \in \N$
and $m$ and $k$ have no common divisors,} \\
0 & \text{ if $x$ is irrational}.
\end{cases}
\end{equation*}
Show $\int_0^1 f = 0$.
\end{exercise}


\begin{exnote}
If $I \subset \R$ is a bounded interval, then
the function
\begin{equation*}
\varphi_I(x) :=
\begin{cases}
1 & \text{if $x \in I$,} \\
0 & \text{otherwise,}
\end{cases}
\end{equation*}
is called an \emph{\myindex{elementary step function}}.
\end{exnote}

\begin{exercise}
Let $I$ be an arbitrary bounded interval (you should consider all types
of intervals: closed, open, half-open) and $a < b$, then
using only the definition of the integral
show that
the elementary step function $\varphi_I$ is integrable
on $[a,b]$, and find the integral in terms of $a$, $b$, and the
endpoints of $I$.
\end{exercise}

\begin{exnote}
When a function $f$ can be written as
\begin{equation*}
f(x) = \sum_{k=1}^n \alpha_k \varphi_{I_k} (x)
\end{equation*}
for some real numbers $\alpha_1,\alpha_2, \ldots, \alpha_n$
and some bounded intervals $I_1,I_2,\ldots,I_n$, then 
$f$ is called a \emph{\myindex{step function}}.
\end{exnote}

\begin{exercise}
Using the previous exercise, show that a step function is integrable
on any interval $[a,b]$.
Furthermore, find the integral in terms of
$a$, $b$, the endpoints of $I_k$ and the $\alpha_k$.
\end{exercise}

\begin{exercise} \label{exercise:boundedvariationintegrable}
Let $f \colon [a,b] \to \R$ be increasing.
\begin{enumerate}[a)]
 \item Show that $f$ is Riemann
integrable.
Hint: Use a uniform partition; each subinterval of same length.
  \item Use part a to show that a decreasing function is Riemann
integrable.
   \item Suppose $h = f-g$ where $f$ and $g$ are increasing
functions on $[a,b]$.
Show that $h$ is Riemann integrable%
\footnote{Such an $h$ is said to be of \emph{\myindex{bounded variation}}.}.
\end{enumerate}
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:hardabsint}
Suppose $f \in \sR[a,b]$, then the function that takes $x$ to
$\abs{f(x)}$ is also Riemann integrable on $[a,b]$.
Then show the same inequality as \exerciseref{exercise:easyabsint}.
\end{exercise}

\begin{exercise}  \label{exercise:upperlowerlinineq}
Suppose $f \colon [a,b] \to \R$ and $g \colon [a,b] \to \R$
are bounded. 
\begin{enumerate}[a)]
 \item Show
$\overline{\int_a^b} (f+g) \leq \overline{\int_a^b}f+\overline{\int_a^b}g$ and
$\underline{\int_a^b} (f+g) \geq
\underline{\int_a^b}f+\underline{\int_a^b}g$.
 \item Find example $f$ and $g$ where
the inequality is strict.
Hint: $f$ and $g$ should not be Riemann
integrable.
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Fundamental theorem of calculus}
\label{sec:ftc}

\sectionnotes{1.5 lectures}

In this chapter we discuss and prove the
\emph{\myindex{fundamental theorem of calculus}}.
The entirety of integral calculus is built upon this theorem,
ergo the name.
The theorem relates the seemingly unrelated concepts of integral and
derivative.
It tells us how to compute the antiderivative of a function
using the integral and vice-versa.

\subsection*{First form of the theorem}

\begin{thm}
Let $F \colon [a,b] \to \R$ be a continuous function, differentiable
on $(a,b)$.
Let $f \in \sR[a,b]$ be such that $f(x) = F'(x)$ for $x \in
(a,b)$.
Then
\begin{equation*}
\int_a^b f = F(b)-F(a) .
\end{equation*}
\end{thm}

It is not hard to generalize the theorem to allow a finite number of points
in $[a,b]$ where $F$ is not differentiable, as long as it is continuous.
This generalization is left as an exercise.

\begin{proof}
Let $P = \{ x_0, x_1, \ldots, x_n \}$ be a partition of $[a,b]$.
For each interval $[x_{i-1},x_i]$, use the
\hyperref[thm:mvt]{mean value theorem} to find a
$c_i \in (x_{i-1},x_i)$ such that
\begin{equation*}
f(c_i) \Delta x_i = F'(c_i) (x_i - x_{i-1}) = F(x_i) - F(x_{i-1}) .
\end{equation*}
Using the notation from the definition of the integral, we have
$m_i \leq f(c_i) \leq M_i$ and so
\begin{equation*}
m_i \Delta x_i \leq F(x_i) - F(x_{i-1}) \leq M_i \Delta x_i .
\end{equation*}
We sum over $i = 1,2, \ldots, n$ to get
\begin{equation*}
\sum_{i=1}^n m_i \Delta x_i
\leq \sum_{i=1}^n \bigl(F(x_i) - F(x_{i-1}) \bigr)
\leq \sum_{i=1}^n M_i \Delta x_i .
\end{equation*}
In the middle sum, all the terms except the first and last cancel 
and we end up with $F(x_n)-F(x_0) = F(b)-F(a)$.
The sums on the left
and on the right are the lower and the upper sum respectively.
So
\begin{equation*}
L(P,f) \leq F(b)-F(a) \leq U(P,f) .
\end{equation*}
We take the supremum of $L(P,f)$ over all $P$ and the left inequality
yields 
\begin{equation*}
\underline{\int_a^b} f \leq F(b)-F(a) .
\end{equation*}
Similarly, taking
the infimum of $U(P,f)$ over all partitions $P$ yields
\begin{equation*}
F(b)-F(a) \leq \overline{\int_a^b} f .
\end{equation*}
As $f$ is Riemann integrable, we have
\begin{equation*}
\int_a^b f =
\underline{\int_a^b} f \leq F(b)-F(a) \leq \overline{\int_a^b} f
= \int_a^b f .
\end{equation*}
The inequalities must be equalities and we are done.
\end{proof}

The theorem is used to compute integrals.
Suppose we know that
the function $f(x)$ is a derivative of some other function $F(x)$,
then we can find an explicit expression for $\int_a^b f$. 

\begin{example}
Suppose we are trying to compute
\begin{equation*}
\int_0^1 x^2 ~dx .
\end{equation*}
We notice $x^2$ is the derivative of $\frac{x^3}{3}$.
We
use the fundamental theorem to write 
\begin{equation*}
\int_0^1 x^2 ~dx =
\frac{1^3}{3}
-
\frac{0^3}{3}
= \frac{1}{3}.
\end{equation*}
\end{example}

\subsection*{Second form of the theorem}

The second form of the fundamental theorem gives us a way to solve
the differential equation $F'(x) = f(x)$, where $f$ is a known
function and we are trying to find an $F$ that satisfies the equation.

\begin{thm}
Let $f \colon [a,b] \to \R$ be a Riemann integrable function.
Define
\begin{equation*}
F(x) := \int_a^x f .
\end{equation*}
First, $F$ is continuous on $[a,b]$.
Second,
if $f$ is continuous at $c \in [a,b]$, then $F$ is differentiable at $c$
and $F'(c) = f(c)$.
\end{thm}

\begin{proof}
As $f$ is bounded, there is an $M > 0$
such that $\abs{f(x)} \leq M$ for all $x \in [a,b]$.
Suppose $x,y \in [a,b]$
with $x > y$.
Then
%Then using
%an exercise from an earlier section we note
\begin{equation*}
\abs{F(x)-F(y)} =
\abs{\int_a^x f - \int_a^y f}
=
\abs{\int_y^x f}
\leq
M\abs{x-y} .
\end{equation*}
By symmetry, the same also holds if $x < y$.
So $F$ is Lipschitz continuous and hence continuous.

Now suppose $f$ is continuous at $c$.
Let $\epsilon > 0$ be given.
Let $\delta > 0$ be such that
for $x \in [a,b]$
$\abs{x-c} < \delta$ implies $\abs{f(x)-f(c)} < \epsilon$.
In particular,
for such $x$ we have
\begin{equation*}
f(c)-\epsilon \leq f(x) \leq f(c) + \epsilon.
\end{equation*}
Thus if $x > c$, then
\begin{equation*}
\bigl(f(c)-\epsilon\bigr) (x-c) \leq \int_c^x f \leq
\bigl(f(c) + \epsilon\bigr)(x-c).
\end{equation*}
When $c > x$, then the inequalities are reversed.
Therefore,
assuming $c \not= x$ we get
\begin{equation*}
f(c)-\epsilon
\leq
\frac{\int_c^{x} f}{x-c}
\leq
f(c)+\epsilon .
\end{equation*}
As 
\begin{equation*}
\frac{F(x)-F(c)}{x-c}
=
\frac{\int_a^{x} f - \int_a^{c} f}{x-c}
=
\frac{\int_c^{x} f}{x-c} ,
\end{equation*}
we have 
\begin{equation*}
\abs{\frac{F(x)-F(c)}{x-c} - f(c)} \leq \epsilon .
\end{equation*}
The result follows.
It is left to the reader to see why is it OK that we
just have a non-strict inequality.
\end{proof}

Of course, if $f$ is continuous on $[a,b]$, then it is automatically Riemann
integrable, $F$ is differentiable on all of $[a,b]$ and $F'(x) = f(x)$ for
all $x \in [a,b]$.

\begin{remark}
The second form of the fundamental theorem of calculus still holds if
we let $d \in [a,b]$ and define
\begin{equation*}
F(x) := \int_d^x f .
\end{equation*}
That is, we can use any point of $[a,b]$ as our base point.
The proof is
left as an exercise.
\end{remark}

Let us look at what a simple discontinuity can do.
Take $f(x) := -1$ if $x
< 0$, and $f(x) := 1$ if $x \geq 0$.
Let $F(x) := \int_0^x f$.
It is not
difficult to see that $F(x) = \abs{x}$.
Notice that $f$ is discontinuous at
$0$ and $F$ is not differentiable at $0$.
However, the converse does not
hold.
Let us do another quick example.
Let $g(x) := 0$ if $x \not= 0$, and $g(0) = 1$.
Letting $G(x) :=
\int_0^x g$, we find that $G(x) = 0$ for all $x$.
So $g$ is discontinuous
at $0$, but $G'(0)$ exists and is equal to 0.

A common misunderstanding of the integral for calculus students is to
think of integrals whose solution cannot be given in closed-form as somehow
deficient.
This is not the case.
Most integrals we write down are not
computable in closed-form.
Even some integrals that we consider
in closed-form are not really such.
For example, how does a computer find
the value of $\ln x$?
  One way to do it is to note that
we define the natural log as the antiderivative of $\nicefrac{1}{x}$
such that $\ln 1 = 0$.
Therefore,
\begin{equation*}
\ln x := \int_1^x \nicefrac{1}{s}~ds .
\end{equation*}
Then we can numerically approximate the integral.
Morally,
we did not really ``simplify'' $\int_1^x \nicefrac{1}{s}~ds$ by
writing down $\ln x$.
We simply gave the integral a name.
If we require numerical answers,
it is possible we end up doing
the calculation by approximating an integral anyway.

Another common function defined by an integral that cannot
be evaluated symbolically
is the erf function, defined as
\begin{equation*}
\operatorname{erf}(x) := \frac{2}{\sqrt{\pi}} \int_0^x e^{-s^2} ~ds .
\end{equation*}
This function comes up often in applied mathematics.
It is simply 
the antiderivative of $\left(\nicefrac{2}{\sqrt{\pi}}\right) e^{-x^2}$
that is zero at zero.
The second form of the fundamental theorem tells us that we can write the function
as an integral.
If we wish to compute any particular value, we 
numerically approximate the integral.

\subsection*{Change of variables}

A theorem often used in calculus to solve integrals is the change of
variables theorem.
Let us prove it now.
Recall 
a function is continuously differentiable if
it is differentiable and the derivative is continuous.

\begin{thm}[Change of variables]
\index{change of variables theorem}
Let $g \colon [a,b] \to \R$ be a continuously differentiable function.
If $g([a,b]) \subset [c,d]$ and 
$f \colon [c,d] \to \R$ is continuous, then
\begin{equation*}
\int_a^b f\bigl(g(x)\bigr)\, g'(x)~ dx =
\int_{g(a)}^{g(b)} f(s)~ ds .
\end{equation*}
\end{thm}

\begin{proof}
As $g$, $g'$, and $f$ are continuous, we know $f\bigl(g(x)\bigr)\,g'(x)$
is a continuous function on $[a,b]$, therefore it is Riemann integrable.

Define 
\begin{equation*}
F(y) := \int_{g(a)}^{y} f(s)~ds .
\end{equation*}
By the second form of the fundamental
theorem of calculus (using \exerciseref{secondftc:exercise} below)
$F$ is a differentiable function and $F'(y) = f(y)$.
We apply the chain
rule and write
\begin{equation*}
\bigl( F \circ g \bigr)' (x) =
F'\bigl(g(x)\bigr) g'(x)
=
f\bigl(g(x)\bigr) g'(x) .
\end{equation*}
We note that $F\bigl(g(a)\bigr) = 0$ and we
use the first form of the fundamental theorem
to obtain
\begin{equation*}
\int_{g(a)}^{g(b)} f(s)~ds = F\bigl(g(b)\bigr) = F\bigl(g(b)\bigr)-F\bigl(g(a)\bigr)
=
\int_a^b 
\bigl( F \circ g \bigr)' (x) ~dx
=
\int_a^b 
f\bigl(g(x)\bigr) g'(x)
~dx .%FIXME \qedhere
\end{equation*}
\end{proof}

The change of variables theorem is often used to solve integrals by changing them
to integrals that we know or that we can solve using the fundamental theorem of
calculus.

\begin{example}
From an exercise, we know that the derivative of $\sin(x)$ is $\cos(x)$.
Therefore we solve
\begin{equation*}
\int_0^{\sqrt{\pi}} x \cos(x^2) ~ dx = \int_0^\pi \frac{\cos(s)}{2} ~ ds
=
\frac{1}{2}
\int_0^\pi \cos(s) ~ ds
=
\frac{
\sin(\pi) - \sin(0)
}{2}
=
0 .
\end{equation*}
\end{example}

However, beware that we must satisfy the hypotheses of the theorem.
The
following example demonstrates why we should not just 
move symbols around mindlessly.
We must be careful that those symbols really make sense.

\begin{example}
Suppose we write down
\begin{equation*}
\int_{-1}^{1} \frac{\ln \abs{x}}{x} ~dx .
\end{equation*}
It may be tempting to take $g(x) := \ln \abs{x}$.
Then take $g'(x) =
\frac{1}{x}$ and try to write
\begin{equation*}
\int_{g(-1)}^{g(1)} s ~ds = 
\int_{0}^{0} s ~ds = 0. 
\end{equation*}
This ``solution'' is incorrect, and it does not say
that we can solve the given integral.
First problem is that
$\frac{\ln \abs{x}}{x}$ is not continuous on $[-1,1]$.
Second, $\frac{\ln \abs{x}}{x}$ is not even Riemann integrable on $[-1,1]$
(it is unbounded).
The integral we wrote down simply does not make sense.
Finally, $g$ is not continuous on $[-1,1]$ either.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Compute
$\displaystyle
\frac{d}{dx} \biggl( \int_{-x}^x e^{s^2}~ds \biggr)$.
\end{exercise}

\begin{exercise}
Compute
$\displaystyle
\frac{d}{dx} \biggl( \int_{0}^{x^2} \sin(s^2)~ds \biggr)$.
\end{exercise}

\begin{exercise}
Suppose $F \colon [a,b] \to \R$ is continuous and differentiable
on $[a,b] \setminus S$, where $S$ is a finite set.
Suppose there
exists an $f \in \sR[a,b]$ such that $f(x) = F'(x)$ for $x \in [a,b]
\setminus S$.
Show that
$\int_a^b f = F(b)-F(a)$.
\end{exercise}

\begin{exercise} \label{secondftc:exercise}
Let $f \colon [a,b] \to \R$ be a continuous function.
Let $c \in [a,b]$
be arbitrary.
Define
\begin{equation*}
F(x) := \int_c^x f .
\end{equation*}
Prove that $F$ is differentiable and that $F'(x) = f(x)$ for all $x \in
[a,b]$.
\end{exercise}

\begin{exercise}
Prove \emph{\myindex{integration by parts}}.
That is, suppose $F$ and
$G$ are continuously differentiable functions on $[a,b]$.
Then prove
\begin{equation*}
\int_a^b F(x)G'(x)~dx
=
F(b)G(b)-F(a)G(a)
-
\int_a^b F'(x)G(x)~dx .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $F$ and $G$ are
continuously\footnote{
Compare this hypothesis to \exerciseref{exercise:samediffconst}.}
differentiable
functions defined on $[a,b]$
such that $F'(x) = G'(x)$ for all $x \in [a,b]$.
Using the fundamental theorem of calculus,
show that $F$ and $G$ differ by a constant.
That is, show that
there exists a $C \in \R$ such that
$F(x)-G(x) = C$.
\end{exercise}

\begin{exnote}
The next exercise shows how we can use the integral to ``smooth out'' a
non-differentiable function.
\end{exnote}

\begin{exercise} \label{exercise:smoothingout}
Let $f \colon [a,b] \to \R$ be a continuous function.
Let $\epsilon > 0$
be a constant.
For $x \in [a+\epsilon,b-\epsilon]$, define
\begin{equation*}
g(x) := \frac{1}{2\epsilon} \int_{x-\epsilon}^{x+\epsilon} f .
\end{equation*}
\begin{enumerate}[a)]
 \item Show that $g$ is differentiable and find the derivative.
  \item Let $f$ be differentiable and fix $x \in (a,b)$ (let $\epsilon$
be small enough).
What happens to $g'(x)$ as $\epsilon$ gets smaller?
   \item Find $g$ for $f(x) := \abs{x}$, $\epsilon = 1$ (you can assume 
$[a,b]$ is large enough).
\end{enumerate}
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is continuous and
$\int_a^x f = \int_x^b f$ for all $x \in [a,b]$.
Show that $f(x) = 0$
for all $x \in [a,b]$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [a,b] \to \R$ is continuous and
$\int_a^x f = 0$ for all rational $x$ in $[a,b]$.
Show that $f(x) = 0$
for all $x \in [a,b]$.
\end{exercise}

\begin{exercise}
A function $f$ is an \emph{\myindex{odd function}} if $f(x) = -f(-x)$,
and $f$ is an \emph{\myindex{even function}} if $f(x) = f(-x)$.
Let $a >
0$.  
Assume $f$ is continuous.  
Prove: 
\begin{enumerate}[a)]
 \item If $f$ is odd, then $\int_{-a}^a f
= 0$.
 \item If $f$ is even, then $\int_{-a}^a f = 2 \int_0^a f$.
\end{enumerate}
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Show that $f(x) := \sin(\nicefrac{1}{x})$
is integrable on any interval (you can define $f(0)$ to be anything).
  \item Compute $\int_{-1}^1 \sin(\nicefrac{1}{x})\,dx$.
(Mind the discontinuity.)
\end{enumerate}
\end{exercise}

\begin{exercise}[uses \sectionref{sec:monotonefunc}]
\begin{enumerate}[a)]
 \item Suppose $f \colon [a,b] \to \R$ is increasing, by
\exerciseref{exercise:boundedvariationintegrable},
$f$ is Riemann integrable.
Suppose $f$ has a discontinuity at $c \in
(a,b)$, show that $F(x) := \int_a^x f$ is not differentiable at $c$.
  \item In \exerciseref{exercise:increasingfuncdiscatQ}, you have constructed an increasing
function $f \colon [0,1] \to \R$ that is discontinuous at every
$x \in [0,1] \cap \Q$.
Use this $f$ to construct a function $F(x)$ that is
continuous on $[0,1]$, but not differentiable at every $x \in [0,1] \cap \Q$.
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{The logarithm and the exponential}
\label{sec:logandexp}

\sectionnotes{1 lecture (optional, requires the optional sections 
\sectionref{sec:limitatinf},
\sectionref{sec:monotonefunc},
\sectionref{sec:ift})}

We now have all that is required to finally properly define the exponential
and the
logarithm that you know from calculus so well.
First recall that we have a good idea of what $x^n$ means as long as
$n$ is a positive integer.
Simply,
\begin{equation*}
x^n := \underbrace{x \cdot x \cdot \cdots \cdot x}_{\text{$n$ times}} .
\end{equation*}
It makes sense to define $x^0 := 1$.
For negative integers we define $x^{-n} := \nicefrac{1}{x^n}$.
If $x > 0$,
we mentioned before that $x^{1/n}$ is defined as
the unique positive $n$th root.
Finally for any rational
number $\nicefrac{n}{m}$, we define
\begin{equation*}
x^{n/m} := {\bigl(x^{1/m}\bigr)}^n .
\end{equation*}
However, what do we mean by $\sqrt{2}^{\sqrt{2}}$?
  Or
$x^y$ in general?
  In particular, what is $e^x$ for all $x$?
And how do we solve $y=e^x$ for $x$?
This section answers these questions and more.

\subsection*{The logarithm}
\index{logarithm}

It is convenient to start with the logarithm.  
Let us show that
a unique function with the right properties exists, and only then will
we call it \emph{the} logarithm.

\begin{prop}
There exists a unique function $L \colon (0,\infty) \to \R$ such that
\begin{enumerate}[(i)]
\item \label{it:log:i}
$L(1) = 0$.
\item \label{it:log:ii}
$L$ is differentiable and $L'(x) = \nicefrac{1}{x}$.
\item \label{it:log:iii}
$L$ is strictly increasing, bijective, and
\begin{equation*}
\lim_{x\to 0} L(x) = -\infty , \qquad \text{and} \qquad
\lim_{x\to \infty} L(x) = \infty .
\end{equation*}
\item \label{it:log:iv}
$L(xy) = L(x)+L(y)$ for all $x,y \in (0,\infty)$.
\item \label{it:log:v}
If $q$ is a rational number and $x > 0$, then
$L(x^q) = q L(x)$.
\end{enumerate}
\end{prop}

\begin{proof}
To prove existence, let us define a candidate and show it satisfies
all the properties.
Define
\begin{equation*}
L(x) := \int_1^x \frac{1}{t}~dt .
\end{equation*}

Obviously \ref{it:log:i} holds.
Property \ref{it:log:ii} holds
via the fundamental theorem of calculus.

To prove property \ref{it:log:iv},
we change variables $u=yt$ to obtain
\begin{equation*}
L(x) =
\int_1^{x} \frac{1}{t}~dt
=
\int_y^{xy} \frac{1}{u}~du
=
\int_1^{xy} \frac{1}{u}~du
-
\int_1^{y} \frac{1}{u}~du
=
L(xy)-L(y) .
\end{equation*}

Property \ref{it:log:ii} together with the fact that $L'(x) = \nicefrac{1}{x} > 0$ 
for $x > 0$, implies that $L$
is strictly increasing and hence one-to-one.
Let us show $L$ is onto.

As $\nicefrac{1}{t} \geq \nicefrac{1}{2}$ when $t \in [1,2]$,
\begin{equation*}
L(2) = \int_1^2 \frac{1}{t} ~dt \geq \nicefrac{1}{2} .
\end{equation*}
By induction, \ref{it:log:iv} implies that for $n \in \N$
\begin{equation*}
L(2^n) = L(2) + L(2) + \cdots + L(2) = n L(2) .
\end{equation*}
Given any $y > 0$, 
by the \hyperref[thm:arch:i]{Archimedean property} of the real numbers
(notice $L(2) > 0$), there is an $n \in \N$ such that
$L(2^n) > y$.
By the
\hyperref[IVT:thm]{intermediate value theorem}
there is an $x_1 \in (1,2^n)$ such that $L(x_1) = y$.
We get
$(0,\infty)$ is in the image of $L$.
As $L$ is increasing, $L(x) > y$ for all $x > 2^n$, and so
\begin{equation*}
\lim_{x\to\infty} L(x) = \infty .
\end{equation*}
Next
$0 = L(\nicefrac{x}{x}) = L(x) + L(\nicefrac{1}{x})$, and
so $L(x) = - L(\nicefrac{1}{x})$.
Using $x=2^{-n}$, we obtain
as above that $L$ achieves all negative numbers.
And
\begin{equation*}
\lim_{x \to 0} L(x) = 
\lim_{x \to 0} -L(\nicefrac{1}{x})
=
\lim_{x \to \infty} -L(x)
=  - \infty .
\end{equation*}
In the limits, note that only $x > 0$ are in the domain of $L$.
%Once the two limits are proved, the fact that $L$ is onto follows by
%\hyperref[IVT:thm]{intermediate value theorem}.

Let us now prove \ref{it:log:v}.
As above, \ref{it:log:iv} implies for $n \in \N$ we have
$L(x^n) = n L(x)$.
We already saw that
$L(x) = - L(\nicefrac{1}{x})$
so $L(x^{-n}) = - L(x^n) = -n L(x)$.
Then for $m \in \N$
\begin{equation*}
L(x) = L\Bigl({(x^{1/m})}^m\Bigr) = m L(x^{1/m}) .
\end{equation*}
Putting everything together for $n \in \Z$ and $m \in \N$ we have
$L(x^{n/m}) = n L(x^{1/m}) = (\nicefrac{n}{m}) L(x)$.

Finally for uniqueness, let us use properties \ref{it:log:i} and
\ref{it:log:ii}.
Via the fundamental theorem of calculus
\begin{equation*}
L(x) = \int_1^x \frac{1}{t}~dt
\end{equation*}
is the unique function such that $L(1) = 0$ and $L'(x) = \nicefrac{1}{x}$.
\end{proof}

Having proved
that there is a unique function with these properties
we simply define the \emph{\myindex{logarithm}} or sometimes called the
\emph{\myindex{natural logarithm}}:
\begin{equation*}
\ln(x) := L(x) .
\end{equation*}
Often mathematicians write $\log(x)$ instead of $\ln(x)$, which is
more familiar to calculus students.

\subsection*{The exponential}
\index{exponential}

Just as with the logarithm we define the exponential via a list of
properties.

\begin{prop}
There exists a unique function $E \colon \R \to (0,\infty)$ such that
\begin{enumerate}[(i)]
\item \label{it:exp:i}
$E(0) = 1$.
\item \label{it:exp:ii}
$E$ is differentiable and $E'(x) = E(x)$.
\item \label{it:exp:iii}
$E$ is strictly increasing, bijective, and
\begin{equation*}
\lim_{x\to -\infty} E(x) = 0 , \qquad \text{and} \qquad
\lim_{x\to \infty} E(x) = \infty .
\end{equation*}
\item \label{it:exp:iv}
$E(x+y) = E(x)E(y)$ for all $x,y \in \R$.
\item \label{it:exp:v}
If $q \in \Q$, then
$E(qx) = {E(x)}^q$.
\end{enumerate}
\end{prop}

\begin{proof}
Again, let us prove existence of such a function by defining a candidate,
and prove that it satisfies all the properties.
The $L$ defined above is invertible.
Let $E$ be the
inverse function of $L$.
Property \ref{it:exp:i} is immediate.

Property \ref{it:exp:ii} follows
via the inverse function theorem, in particular
\lemmaref{lemma:ift}:  $L$ satisfies
all the hypotheses of the lemma, and hence
\begin{equation*}
E'(x) = \frac{1}{L'\bigl(E(x)\bigr)} = E(x) .
\end{equation*}

Let us look at property \ref{it:exp:iii}.
The function $E$ is strictly increasing since $E(x) > 0$ and
$E'(x) = E(x) > 0$.
As $E$ is the inverse of $L$, it must also
be bijective.

To find the limits, we use that 
$E$ is strictly increasing and onto $(0,\infty)$.
For every $M > 0$, there is an $x_0$ such that
$E(x_0) = M$ and $E(x) \geq M$ for all $x \geq x_0$.
Similarly for every $\epsilon > 0$, there is
an $x_0$ such that $E(x_0) = \epsilon$ and
$E(x) < \epsilon$ for all $x < x_0$.
Therefore,
\begin{equation*}
\lim_{n\to -\infty} E(x) = 0 , \qquad \text{and} \qquad
\lim_{n\to \infty} E(x) = \infty .
\end{equation*}

To prove property \ref{it:exp:iv} we use the corresponding
property for the logarithm.
Take $x, y \in \R$.
As $L$ is bijective, find $a$ and $b$ such that $x = L(a)$ and $y = L(b)$.
Then
\begin{equation*}
E(x+y) =
E\bigl(L(a)+L(b)\bigr) = 
E\bigl(L(ab)\bigr) = ab = E(x)E(y)  .
\end{equation*}

Property \ref{it:exp:v} also follows from the corresponding property of $L$.
Given $x \in \R$, let $a$ be such that $x = L(a)$ and
\begin{equation*}
E(qx) = E\bigl(qL(a)\bigr)
E\bigl(L(a^q)\bigr) = a^q = {E(x)}^q .
\end{equation*}

Finally, uniqueness follows from
\ref{it:exp:i} and
\ref{it:exp:ii}.
Let $E$ and $F$
be two functions satisfying
\ref{it:exp:i} and \ref{it:exp:ii}.  
\begin{equation*}
\frac{d}{dx} \Bigl( F(x)E(-x) \Bigr)
=
F'(x)E(-x) - E'(-x)F(x)
=
F(x)E(-x) - E(-x)F(x) = 0 .
\end{equation*}
Therefore by \propref{prop:derzeroconst},
$F(x)E(-x) = F(0)E(-0) = 1$ for all $x \in \R$.
Doing the computation with $F = E$,
we obtain $E(x)E(-x) = 1$.
Then
\begin{equation*}
0 = 1-1 = F(x)E(-x) - E(x)E(-x) = \bigl(F(x)-E(x)\bigr) E(-x) .
\end{equation*}
Since $E(x)E(-x) = 1$, then $E(-x) \not= 0$ for
all $x$.
So
$F(x)-E(x) = 0$ for all $x$, and we are done.
\end{proof}

Having proved $E$ is unique, we define the
\emph{\myindex{exponential}} function as
\begin{equation*}
\exp(x) := E(x) .
\end{equation*}

We can now make sense of exponentiation $x^y$ for arbitrary numbers
when $x > 0$.
First suppose $y \in \Q$.
Then
\begin{equation*}
x^y = \exp\bigl(\ln(x^y)\bigr) = \exp\bigl(y\ln(x)\bigr) .
\end{equation*}
Therefore when $x > 0$ and $y$ is irrational let us define
\begin{equation*}
x^y := \exp\bigl(y\ln(x)\bigr) .
\end{equation*}
As $\exp$ is continuous then $x^y$ is a continuous function of $y$.
Therefore, we would
obtain the same result had we taken a sequence of rational numbers $\{ y_n \}$
approaching $y$ and defined $x^y = \lim\, x^{y_n}$.

Define the number $e$ as
\begin{equation*}
e := \exp(1) .
\end{equation*}
The number $e$ is sometimes called \emph{\myindex{Euler's number}} or
the \emph{\myindex{base of the natural logarithm}}.
We notice 
\begin{equation*}
e^x = \exp\bigl(x \ln(e) \bigr) = \exp(x) .
\end{equation*}
We have justified the notation $e^x$ for $\exp(x)$.

Finally, let us extend properties of logarithm and exponential to
irrational powers.
The proof is immediate.

\begin{prop}
Let $x, y \in \R$.
\begin{enumerate}[(i)]
\item
$\exp(xy) = {\bigl(\exp(x)\bigr)}^y$.
\item
If $x > 0$ then $\ln(x^y) = y \ln (x)$.
\end{enumerate}
\end{prop}

\subsection*{Exercises}

\begin{exercise}
Let $y$ be any real number and $b > 0$.
Define $f \colon (0,\infty) \to \R$
and $g \colon \R \to \R$ as, $f(x) := x^y$ and $g(x) := b^x$.
Show that $f$
and $g$ are differentiable and find their derivative.
\end{exercise}

\begin{exercise}
Let $b > 0$ be given.
\begin{enumerate}[a)]
 \item Show that for every $y > 0$, there exists a unique number $x$
such that $y = b^x$.
Define
the \emph{\myindex{logarithm base $b$}},
$\log_b \colon (0,\infty) \to \R$, by
$\log_b(y) := x$.
 \item Show that $\log_b(x) = \frac{\ln(x)}{\ln(b)}$.
  \item Prove that if $c > 0$, then
$\log_b(x) = \frac{\log_c(x)}{\log_c(b)}$.
   \item Prove $\log_b(xy) =
\log_b(x)+\log_b(y)$, and $\log_b(x^y) = y \log_b(x)$.
\end{enumerate}
\end{exercise}

\begin{exercise}[requires \sectionref{sec:taylor}]
Use \hyperref[thm:taylor]{Taylor's theorem} to study the remainder term and show that for
all $x \in \R$
\begin{equation*}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} .
\end{equation*}
Hint: Do not differentiate the series term by term (unless you would prove that it
works).
\end{exercise}

\begin{exercise}
Use the geometric sum formula to show (for $t\not= -1$)
\begin{equation*}
1-t+t^2-\cdots+{(-1)}^n t^n = \frac{1}{1+t} - \frac{{(-1)}^{n+1}t^{n+1}}{1+t}.
\end{equation*}
Using this fact show
\begin{equation*}
\ln (1+x) = \sum_{n=1}^\infty \frac{{(-1)}^{n+1}x^n}{n} 
\end{equation*}
for all $x \in (-1,1]$ (note that $x=1$ is included).
Finally,
find the limit of the alternating harmonic series
\begin{equation*}
\sum_{n=1}^\infty \frac{{(-1)}^{n+1}}{n} = 1 - \nicefrac{1}{2} +
\nicefrac{1}{3} - \nicefrac{1}{4} + \cdots
% = \ln 2 .
\end{equation*}

\begin{exercise}
Show 
\begin{equation*}
e^x = \lim_{n\to\infty} {\left( 1 + \frac{x}{n} \right)}^n .
\end{equation*}
Hint: Take the logarithm.\\
Note: The expression 
${\left( 1 + \frac{x}{n} \right)}^n$ arises in compound interest
calculations.
It is the amount of money in a bank account after 1 year
if 1 dollar was deposited initially at interest $x$
and the interest was compounded $n$
times during the year.
Therefore $e^x$ is the result of continuous
compounding.
%Oftentimes this limit with $x=1$ is used to define the number $e$.
\end{exercise}

\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Prove that for $n \in \N$ we have
\begin{equation*}
\sum_{k=2}^{n}
\frac{1}{k}
\leq
\ln (n)
\leq
\sum_{k=1}^{n-1}
\frac{1}{k} .
\end{equation*}
  \item Prove that the limit
\begin{equation*}
\gamma := \lim_{n\to\infty}
\left( \sum_{k=1}^{n}
\frac{1}{k} - \ln (n) \right)
\end{equation*}
exists.
This constant is known as the
\emph{\myindex{Euler--Mascheroni constant}}%
\footnote{Named for the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Leonhard_Euler}{Leonhard Paul Euler} (1707 -- 1783)
and the Italian mathematician
\href{http://en.wikipedia.org/wiki/Lorenzo_Mascheroni}{Lorenzo Mascheroni}
(1750 -- 1800).}.
It is not known if this constant is rational or not,
it is approximately $\gamma \approx 0.5772$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Show
\begin{equation*}
\lim_{x\to\infty} \frac{\ln(x)}{x} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $e^x$ is \emph{\myindex{convex}}, in other words, show that 
if $a \leq x \leq b$ then
$e^x \leq e^a \frac{b-x}{b-a} + e^b \frac{x-a}{b-a}$.
\end{exercise}

\begin{exercise}
Using the logarithm find
\begin{equation*}
%\lim_{n\to\infty} {\left( 1 + \nicefrac{1}{n} \right)}^n = e .
\lim_{n\to\infty} n^{1/n} .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $E(x) = e^x$ is the unique continuous function such that
$E(x+y) = E(x)E(y)$ and $E(1) = e$.
 Similarly prove that $L(x) = \ln(x)$
is the unique continuous
function defined on positive $x$ such that $L(xy) = L(x)+L(y)$
and $L(e) = 1$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Improper integrals}
\label{sec:impropriemann}

\sectionnotes{2--3 lectures (optional section, can safely be skipped, 
requires the optional \sectionref{sec:limitatinf})}

Often it is necessary to integrate over the
entire real line, or a infinite interval of the form $[a,\infty)$ or
$(\infty,b]$.
Also, we may wish to integrate functions defined on a finite
interval $(a,b)$ but not bounded.
Such functions are not Riemann integrable, but we may want to write down
the integral anyway in the spirit of \lemmaref{lemma:boundedimpriemann}.
These integrals are called \emph{\myindex{improper integrals}},
and are limits
of integrals rather than integrals themselves.

\begin{defn}
Suppose $f \colon [a,b) \to \R$ is a function (not necessarily bounded)
that is Riemann integrable on $[a,c]$ for all $c < b$.
We define
\begin{equation*}
\int_a^b f := \lim_{c \to b^-} \int_a^{c} f ,
\end{equation*}
if the limit exists.%  Of course in the limit we are ``going up to'' $b$
%as the integral is only defined when $c < b$.

Suppose $f \colon [a,\infty) \to \R$ is a function such that
$f$ is Riemann integrable on $[a,c]$ for all $c < \infty$.  
We define
\begin{equation*}
\int_a^\infty f := \lim_{c \to \infty} \int_a^c f ,
\end{equation*}
if the limit exists.

If the limit exists, we say the improper integral
\emph{\myindex{converges}}\index{convergent improper integral}.
If the limit does not exist, we say the improper integral
\emph{\myindex{diverges}}\index{divergent improper integral}.

We similarly define improper integrals for the left hand endpoint, we leave
this to the reader.
\end{defn}

For a finite endpoint $b$,
using \lemmaref{lemma:boundedimpriemann} we see that if
$f$ is bounded, then we have defined nothing new.
What is new is that
we can apply this definition to unbounded functions.
The following set of examples is
so useful that we state it as a proposition.

\begin{prop}[$p$-test for integrals]\index{p-test for integrals}
\label{impropriemann:ptest}
The improper integral
\begin{equation*}
\int_1^\infty \frac{1}{x^p} ~dx
\end{equation*}
converges to $\frac{1}{p-1}$ if $p > 1$ and diverges if $0 < p \leq 1$.

The improper integral
\begin{equation*}
\int_0^1 \frac{1}{x^p} ~dx
\end{equation*}
converges to $\frac{1}{1-p}$ if $0 < p < 1$ and diverges if $p \geq 1$.
\end{prop}

\begin{proof}
The proof follows by application of the fundamental theorem of calculus.
Let us do the proof for $p > 1$ for the infinite right endpoint, and
we leave the rest to the reader.
Hint: You should handle $p=1$
separately.

Suppose $p > 1$.
Then
\begin{equation*}
\int_1^b \frac{1}{x^p} ~dx
=
\int_1^b x^{-p} ~dx
=
\frac{b^{-p+1}}{-p+1}
-
\frac{1^{-p+1}}{-p+1}
=
-
\frac{1}{(p-1)b^{p-1}}
+
\frac{1}{p-1} .
\end{equation*}
As $p > 1$, then $p-1 > 0$.
Taking the limit as $b \to \infty$
we obtain that $\frac{1}{b^{p-1}}$ goes to 0, and the result follows.
\end{proof}

We state the following proposition for just one type
of improper integral, though the proof is straight
forward and the same for other types of improper integrals.

\begin{prop} \label{impropriemann:tail}
Let $f \colon [a,\infty) \to \R$ be a function
that is Riemann integrable on $[a,b]$ for all $b > a$.
Given any $b > a$,
$\int_b^\infty f$ converges if and only if $\int_a^\infty f$
converges, in which case
\begin{equation*}
\int_a^\infty f
=
\int_a^b f +
\int_b^\infty f .
\end{equation*}
\end{prop}

\begin{proof}
Let $c > b$.
Then
\begin{equation*}
\int_a^c f
=
\int_a^b f +
\int_b^c f .
\end{equation*}
Taking the limit $c \to \infty$ finishes the proof.
\end{proof}

Nonnegative functions are easier to work with
as the following proposition demonstrates.
The exercises will show that this proposition
holds only for nonnegative functions.
Analogues of this proposition
exist for all the other types of improper limits are left to the
student.

\begin{prop} \label{impropriemann:possimp}
Suppose $f \colon [a,\infty) \to \R$ is nonnegative ($f(x)
\geq 0$ for all $x$) and such that
$f$ is Riemann integrable on $[a,b]$ for all $b > a$.
\begin{enumerate}[(i)]
\item 
\begin{equation*}
\int_a^\infty f = \sup \left\{ \int_a^x f : x \geq a \right\} .
\end{equation*}
\item
Suppose $\{ x_n \}$
is a sequence with $\lim\, x_n = \infty$.
Then
$\int_a^\infty f$ converges if and only if $\lim\, \int_a^{x_n} f$ exists, in
which case
\begin{equation*}
\int_a^\infty f = \lim_{n\to\infty} \int_a^{x_n} f .
\end{equation*}
\end{enumerate}
\end{prop}

In the first item we allow for the value of $\infty$ in the
supremum indicating that the integral diverges to infinity.

\begin{proof}
Let us start with the first item.
Notice that as $f$ is nonnegative,
then $\int_a^x f$ is increasing as a function of $x$.
If the supremum is infinite, then for every $M \in \R$
we find $N$ such that $\int_a^N f \geq M$.
As $\int_a^x f$
is increasing then $\int_a^x f \geq M$ for all $x \geq N$.
So
$\int_a^\infty f$ diverges to infinity.
%  Similarly if
%$\int_a^\infty f$ diverges to infinity, then we can find
%$x$ with $\int_a^x f$ arbitrarily large, so the supremum
%is infinite.

Next suppose the supremum is finite, say
$A = \sup \left\{ \int_a^x f : x \geq a \right\}$.
For every $\epsilon > 0$, we find an $N$ such that
$A - \int_a^N f < \epsilon$.
As $\int_a^x f$ is increasing,
then
$A - \int_a^x f < \epsilon$ for all $x \geq N$ and hence
$\int_a^\infty f$ converges to $A$.

Let us look at the second item.
If $\int_a^\infty f$ converges then every sequence $\{ x_n \}$ going to
infinity works.
The trick is
proving the other direction.
Suppose $\{ x_n \}$ is such that $\lim\, x_n =
\infty$ and
\begin{equation*}
\lim_{n\to\infty} \int_a^{x_n} f = A
\end{equation*}
converges.
Given $\epsilon > 0$, pick $N$ such that for
all $n \geq N$ we have
%\begin{equation*}
%\abs{\int_a^{x_n} f - A} < \epsilon .
%\end{equation*}
%\begin{equation*}
$A - \epsilon < \int_a^{x_n} f < A + \epsilon$.
%\end{equation*}
Because $\int_a^x f$ is increasing as a function of $x$, we have that for all
$x \geq x_N$
\begin{equation*}
A - \epsilon < \int_a^{x_N} \leq \int_a^x f .
\end{equation*}
As $\{ x_n \}$ goes to $\infty$, then for any given
$x$, there is an $x_m$ such that $m \geq N$ and $x \leq x_m$.
Then
\begin{equation*}
\int_a^{x} f \leq \int_a^{x_m} f < A + \epsilon .
\end{equation*}
In particular, for all $x \geq x_N$ we have
$\abs{\int_a^{x} f - A} < \epsilon$.
%As $\{ x_n \}$ goes to infinity, we can pick
%a subsequence $\{ x_{n_k} \}$ that is monotone increasing (why?).
%Therefore
%$\{ \int_a^{x_{n_k}} f \}$ is monotone increasing.
%\begin{equation*}
%A = \lim_{n\to\infty} \int_a^{x_n} f =
%\lim_{k\to\infty} \int_a^{x_{n_k}} f
%=
%\sup_{k \in \N} \int_a^{x_{n_k}} f .
%\end{equation*}
%So $A \leq \sup \left\{ \int_a^x f : x \geq a \right\}$.  As for any $x$
%there exists an $x_{n_k}$ such that $x \leq x_{n_k}$ and so
%$\int_a^x f \leq \int_a^{x_{n_k}}$, we see that
%So $A \geq \sup \left\{ \int_a^x f : x \geq a \right\}$.  We finish
%by applying the first item.
\end{proof}

\begin{prop}[\myindex{Comparison test for improper integrals}]
Let
$f \colon [a,\infty) \to \R$ and
$g \colon [a,\infty) \to \R$ be functions
that are Riemann integrable on $[a,b]$ for all $b > a$.
 Suppose
that for all $x \geq a$ we have
\begin{equation*}
\abs{f(x)} \leq g(x) .
\end{equation*}
\begin{enumerate}[(i)]
\item If $\int_a^\infty g$ converges, then $\int_a^\infty f$ converges,
and in this case 
$\abs{\int_a^\infty f} \leq \int_a^\infty g$.
\item If $\int_a^\infty f$ diverges, then $\int_a^\infty g$ diverges.
\end{enumerate}
\end{prop}

\begin{proof}
Let us start with the first item.
For any $b$ and $c$, such that $a \leq b \leq c$, we have 
$-g(x) \leq f(x) \leq g(x)$, and so
\begin{equation*}
\int_b^c -g \leq \int_b^c f \leq \int_b^c g  .
\end{equation*}
In other words, $\abs{\int_b^c f} \leq \int_b^c g$.

Let $\epsilon > 0$ be given.
Because
of \propref{impropriemann:tail} we have
\begin{equation*}
\int_a^\infty g =
\int_a^b g +
\int_b^\infty g .
\end{equation*}
As $\int_a^b g$ goes to
$\int_a^\infty g$ as $b$ goes to infinity, then
$\int_b^\infty g$ goes to 0 as $b$ goes to infinity.
Choose $B$
such that
\begin{equation*}
\int_B^\infty g < \epsilon .
\end{equation*}
As $g$ is nonnegative, then if $B \leq b < c$, then
$\int_b^c g < \epsilon$ as well.
Let $\{ x_n \}$ be a sequence going to infinity.
Let $M$ be such that
$x_n \geq B$ for all $n \geq M$.
Take $n, m \geq M$,
with $x_n \leq x_m$,
\begin{equation*}
\abs{\int_a^{x_m} f - \int_a^{x_n} f} 
=
\abs{\int_{x_n}^{x_m} f} 
\leq \int_{x_n}^{x_m} g < \epsilon .
\end{equation*}
Therefore the sequence $\{ \int_a^{x_n} f \}_{n=1}^\infty$ is Cauchy and hence converges.

We need to show that the limit is unique.
Suppose $\{ x_n \}$ is a sequence
converging to infinity such that
$\{ \int_a^{x_n} f \}$ converges to $L_1$, and $\{ y_n \}$ is a sequence
converging to infinity is such that
$\{ \int_a^{y_n} f \}$ converges to $L_2$.
Then there must be some $n$ such
that
$\abs{\int_a^{x_n} f - L_1} < \epsilon$ and 
$\abs{\int_a^{y_n} f - L_2} < \epsilon$.
We can also suppose $x_n \geq B$
and $y_n \geq B$.
Then
\begin{equation*}
\abs{L_1 - L_2} \leq
\abs{L_1 - \int_a^{x_n} f}
+
\abs{\int_a^{x_n} f- \int_a^{y_n} f}
+
\abs{\int_a^{y_n} f - L_2}
<
\epsilon
+
\abs{\int_{x_n}^{y_n} f}
+
\epsilon
<
3 \epsilon.
\end{equation*}
As $\epsilon > 0$ was arbitrary, $L_1 = L_2$, and hence
$\int_a^\infty f$ converges.
Above we have shown that $\abs{\int_a^c f} \leq \int_a^c g$ for all $c > a$.
By taking the limit $c \to \infty$, the first item is proved.

The second item is simply a contrapositive of the first item.
%Now suppose that $\int_a^\infty f$ diverges.  Applying the contrapositive
%of the first item with $g(x) = \abs{f(x)}$ we conclude that $\int_a^\infty \abs{f(x)}$
%must diverge.
%Because $\abs{f(x)} \geq 0$, the sequence
%$\{ \int_a^n \abs{f(x)}~dx \}_{n=1}^\infty$ is monotone increasing.  Since it diverges it must go to
%infinity:  For any $M \in \R$, there exists an $N \in \N$
%such that for all $n \geq N$, we have $\int_a^n \abs{f(x)}~dx > M$.
%For such $n$ we have
%\begin{equation*}
%M < \int_a^n \abs{f(x)}~dx \leq \int_a^n g .
%\end{equation*}
%The sequence $\{ \int_a^n g \}_{n=1}^\infty$ diverges (to infinity), and therefore
%the improper integral $\int_a^\infty g$ diverges.
\end{proof}

\begin{example}
The improper integral
\begin{equation*}
\int_0^\infty \frac{\sin(x^2)(x+2)}{x^3+1} ~dx
\end{equation*}
converges.

\begin{proof}  First observe we simply need to show
that the integral converges when going from 1 to infinity.
For $x \geq 1$ we obtain
\begin{equation*}
\abs{\frac{\sin(x^2)(x+2)}{x^3+1}}
\leq
\frac{x+2}{x^3+1}
\leq \frac{x+2}{x^3} \leq
\frac{x+2x}{x^3} \leq \frac{3}{x^2} .
\end{equation*}
Then
\begin{equation*}
3 \int_1^\infty \frac{1}{x^2}~dx
=
\lim_{c\to\infty} \int_1^c \frac{3}{x^2} ~dx.
\end{equation*}
So the integral converges.
\end{proof}
\end{example}

\begin{example}
You should be careful when doing formal manipulations with improper
integrals.
For example,
\begin{equation*}
\int_2^\infty \frac{2}{x^2-1}~dx
\end{equation*}
converges via the comparison test again using $\frac{1}{x^2}$.
However, if you
succumb to the temptation to write
\begin{equation*}
\frac{2}{x^2-1} = 
\frac{1}{x-1}
-
\frac{1}{x+1} 
\end{equation*}
and try to integrate each part separately, you will not succeed.
It is \emph{not} true that you can split the improper
integral in two; you cannot split the limit.
\begin{equation*}
\begin{split}
\int_2^\infty \frac{2}{x^2-1} ~dx &=
\lim_{b\to \infty} \int_2^b \frac{2}{x^2-1} ~dx
\\
&=
\lim_{b\to \infty}
\left(
\int_2^b \frac{1}{x-1}~dx
-
\int_2^b \frac{1}{x+1}~dx
\right)
\\
&\not=
\int_2^\infty \frac{1}{x-1}~dx
-
\int_2^\infty \frac{1}{x+1}~dx .
\end{split}
\end{equation*}
The last line in the computation does not even make sense.
Both of the
integrals there diverge to infinity since we can
apply the comparison test appropriately with
$\nicefrac{1}{x}$.
We get $\infty - \infty$.
\end{example}

Now let us suppose that we need to take limits at both endpoints.

\begin{defn}
Suppose $f \colon (a,b) \to \R$ is a function
that is Riemann integrable on $[c,d]$ for all $c$, $d$
such that $a < c < d < b$, then we define
\begin{equation*}
\int_a^b f := \lim_{c \to a^+} \, \lim_{d \to b^-} \, \int_{c}^{d} f ,
\end{equation*}
if the limits exist.

Suppose $f \colon \R \to \R$ is a function such that
$f$ is Riemann integrable on all finite intervals $[a,b]$.
Then
we define
\begin{equation*}
\int_{-\infty}^\infty f := \lim_{c \to -\infty} \, \lim_{d \to \infty} \, \int_c^d f ,
\end{equation*}
if the limits exist.

We similarly define improper integrals with one infinite and one finite
improper endpoint, we leave this to the reader.
\end{defn}

One ought to always be careful about double limits.
The definition
given above says that we first take the limit as $d$ goes to $b$ or
$\infty$ for a fixed $c$, and then we take the limit in $c$.
We will have to prove that in this case it does not matter which limit
we compute first.

\begin{example}
Let us see an example:
\begin{equation*}
\int_{-\infty}^\infty \frac{1}{1+x^2} ~ dx
=
\lim_{a \to -\infty} \, \lim_{b \to \infty} \,
\int_{a}^b \frac{1}{1+x^2} ~ dx
=
\lim_{a \to -\infty} \, \lim_{b \to \infty}
\bigl( \arctan(b) - \arctan(a) \bigr)
=
\pi .
\end{equation*}
\end{example}

In the definition the order of the limits can always be switched if they
exist.
Let us prove this fact only for the infinite limits.

\begin{prop}
If $f \colon \R \to \R$ is a function integrable on every interval.
Then 
\begin{equation*}
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
\quad \text{converges if and only if} \qquad
\lim_{b \to \infty}
\,
\lim_{a \to -\infty}
\,
\int_a^b f
\quad
\text{converges,}
\end{equation*}
in which case the two
expressions are equal.
If either of the
expressions converges then the improper integral converges and
\begin{equation*}
\lim_{a\to\infty}
\int_{-a}^a f
=
\int_{-\infty}^\infty f .
\end{equation*}
\end{prop}

\begin{proof}
Without loss of generality assume $a < 0$ and $b > 0$.
Suppose
the first expression converges.
Then
\begin{equation*}
\begin{split}
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
& =
\lim_{a \to -\infty} \, \lim_{b \to \infty}
\left(
\int_a^0 f
+
\int_0^b f
\right)
=
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right)
+
\left(
 \lim_{b \to \infty}
\int_0^b f
\right) \\
& = 
 \lim_{b \to \infty}
\left(
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right) 
+
\int_0^b f
\right)
=
 \lim_{b \to \infty} \,
\lim_{a \to -\infty}
\left(
\int_a^0 f
+
\int_0^b f
\right)  .
\end{split}
\end{equation*}
Similar computation shows the other direction.
Therefore, if
either expression converges then the improper integral converges
and
\begin{equation*}
\begin{split}
\int_{-\infty}^\infty f
=
\lim_{a \to -\infty} \, \lim_{b \to \infty} \, \int_a^b f
& =
\left(
\lim_{a \to -\infty}
\int_a^0 f
\right)
+
\left(
 \lim_{b \to \infty}
\int_0^b f
\right)
\\
& =
\left(
\lim_{a \to \infty}
\int_{-a}^0 f
\right)
+
\left(
 \lim_{a \to \infty}
\int_0^a f
\right)
=
\lim_{a \to \infty}
\left(
\int_{-a}^0 f
+
\int_0^a f
\right)
=
\lim_{a \to \infty}
\int_{-a}^a f .
\end{split}
\end{equation*}
\end{proof}

\begin{example}
On the other hand, you must be careful to
take the limits independently before you know convergence.
Let
$f(x) = \frac{x}{\abs{x}}$ for $x \not= 0$ and $f(0) = 0$.
If $a < 0$ and $b > 0$, then
\begin{equation*}
\int_{a}^b f
=
\int_{a}^0 f
+
\int_{0}^b f
=
a+b .
\end{equation*}
For any fixed $a < 0$ the limit as $b \to \infty$ is infinite, so even
the first limit does not exist, and hence the improper integral
$\int_{-\infty}^\infty f$
%of $f$ from $-\infty$ to $\infty$
does not converge.
On the other hand if $a > 0$, then
\begin{equation*}
\int_{-a}^{a} f
=
(-a)+a = 0 .
\end{equation*}
Therefore,
\begin{equation*}
\lim_{a\to\infty}
\int_{-a}^{a} f
= 0 .
\end{equation*}
\end{example}

\begin{example}
An example to keep in mind for improper integrals
is the so-called \emph{\myindex{sinc function}}%
\footnote{Shortened from Latin: \emph{sinus cardinalis}}.
This function comes up quite often
in both pure and applied mathematics.
Define
\begin{equation*}
\operatorname{sinc}(x) =
\begin{cases}
\frac{\sin(x)}{x} & \text{if $x \not= 0$} , \\
0 & \text{if $x = 0$} .
\end{cases}
\end{equation*}
\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input sincfig.eepic
%\includegraphics{sincfig}
\caption{The sinc function.
%The $x$ axis runs
%from -15 to 15 and the $y$ axis from $\nicefrac{-1}{4}$ to 1.
\label{figsinc}}
\end{center}
\end{figure}

It is not difficult to show that
the sinc function is continuous at zero, but that is
not important right now.
What is important is that
\begin{equation*}
\int_{-\infty}^\infty \operatorname{sinc}(x) ~dx = \pi ,
\qquad \text{while} \qquad
\int_{-\infty}^\infty \abs{\operatorname{sinc}(x)} ~dx = \infty .
\end{equation*}
The integral of the sinc function is a continuous analogue of the
alternating harmonic series $\sum \nicefrac{{(-1)}^n}{n}$, while the
absolute value is like the regular harmonic series $\sum \nicefrac{1}{n}$.
In particular, the fact that the integral converges must be done directly
rather than using comparison test.

We will not prove the first statement exactly.
Let us simply prove
that the integral of the sinc function converges, but we will not worry
about the exact limit.
Because $\frac{\sin(-x)}{-x} = \frac{\sin(x)}{x}$, it is
enough to show that
\begin{equation*}
\int_{2\pi}^\infty \frac{\sin(x)}{x}~dx
\end{equation*}
converges.
We
also avoid $x=0$ this way to make our life simpler.

For any $n \in \N$, we have that for $x \in [\pi 2n, \pi (2n+1)]$
\begin{equation*}
\frac{\sin(x)}{\pi (2n+1)}
\leq
\frac{\sin(x)}{x}
\leq
\frac{\sin(x)}{\pi 2n} ,
\end{equation*}
as $\sin(x) \geq 0$.
On $x \in [\pi (2n+1), \pi (2n+2)]$
\begin{equation*}
\frac{\sin(x)}{\pi (2n+1)}
\leq
\frac{\sin(x)}{x}
\leq
\frac{\sin(x)}{\pi (2n+2)} ,
\end{equation*}
as $\sin(x) \leq 0$.

Via the fundamental theorem of calculus,
\begin{equation*}
\frac{2}{\pi (2n+1)}
=
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{\pi (2n+1)}
~dx
\leq
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{x}
~dx
\leq
\int_{\pi 2n}^{\pi (2n+1)}
\frac{\sin(x)}{\pi 2n}
~dx
=
\frac{1}{\pi n} .
\end{equation*}
Similarly
\begin{equation*}
\frac{-2}{\pi (2n+1)}
\leq
\int_{\pi (2n+1)}^{\pi (2n+2)}
\frac{\sin(x)}{x}
~dx
\leq
\frac{-1}{\pi (n+1)} .
\end{equation*}
Putting the two together we have
\begin{equation*}
0
=
\frac{2}{\pi (2n+1)}
-
\frac{2}{\pi (2n+1)}
+
\leq
\int_{2\pi n}^{2\pi (n+1)}
\frac{\sin(x)}{x}
~dx
\leq
\frac{1}{\pi n} 
-
\frac{1}{\pi (n+1)} 
=
\frac{1}{\pi n(n+1)} .
\end{equation*}
Let $M > 2\pi$ be arbitrary, and let $k \in \N$
be the largest integer such that $2k\pi \leq M$.
Then
\begin{equation*}
\int_{2\pi}^M \frac{\sin(x)}{x}~dx
=
\int_{2\pi}^{2k\pi} \frac{\sin(x)}{x} ~dx
+
\int_{2k\pi}^{M} \frac{\sin(x)}{x} ~dx .
\end{equation*}
For $x \in [2k\pi,M]$ we have 
$\frac{-1}{2k\pi} \leq \frac{\sin(x)}{x} \leq \frac{1}{2k\pi}$, and so
\begin{equation*}
\abs{\int_{2k\pi}^{M} \frac{\sin(x)}{x} ~dx }  \leq
\frac{M-2k\pi}{2k\pi} \leq \frac{1}{k} .
\end{equation*}
As $k$ is the largest $k$ such that $2k\pi \leq M$,
this term goes to zero as $M$ goes to infinity.

Next
\begin{equation*}
0 \leq
\int_{2\pi}^{2k\pi} \frac{\sin(x)}{x}
\leq
\sum_{n=1}^{k-1}
\frac{1}{\pi n(n+1)} ,
\end{equation*}
and this series converges as $k \to \infty$.

Putting the two statements together we obtain
\begin{equation*}
\int_{2\pi}^\infty \frac{\sin(x)}{x} ~dx \leq \sum_{n=1}^{\infty}
\frac{1}{\pi n(n+1)} < \infty .
\end{equation*}

The double sided integral of sinc also exists as noted above.
We leave the other statement---that the integral
of the absolute value of the sinc function diverges---as an exercise.
\end{example}

\subsection*{Integral test for series}

It can be very useful to apply the fundamental theorem 
of calculus in proving a series is summable and to estimate its sum.

\begin{prop}
Suppose $f \colon [k,\infty) \to \R$ is a decreasing nonnegative
function where $k \in \Z$.
Then
\begin{equation*}
\sum_{n=k}^\infty f(n)
\quad \text{converges if and only if}
\qquad
\int_k^\infty f
\quad \text{converges}.
\end{equation*}
In this case 
\begin{equation*}
\int_k^\infty f
\leq
\sum_{n=k}^\infty f(n)
\leq
f(k)+
\int_k^\infty f .
\end{equation*}
\end{prop}

By \exerciseref{exercise:boundedvariationintegrable}, $f$ is integrable
on every interval $[k,b]$ for all $b > k$, so the statement of the theorem
makes sense without additional hypotheses of integrability.

\begin{proof}
Let $\epsilon > 0$ be given.
And suppose $\int_k^\infty f$ converges.
Let $\ell, m \in \Z$ be such that $m > \ell \geq k$.
Because $f$ is decreasing we have
$\int_{n}^{n+1} f \leq f(n) \leq \int_{n-1}^{n} f$.
Therefore
\begin{equation} \label{impropriemann:eqseries}
\int_\ell^m f
=
\sum_{n=\ell}^{m-1} \int_{n}^{n+1} f
\leq
\sum_{n=\ell}^{m-1} f(n)
\leq
f(\ell) +
\sum_{n=\ell+1}^{m-1} \int_{n-1}^{n} f
\leq
f(\ell)+
\int_\ell^{m-1} f .
\end{equation}
As before, since $f$ is positive then there exists
an $L \in \N$ such that if $\ell \geq L$, then
$\int_\ell^{m} f < \nicefrac{\epsilon}{2}$ for all $m \geq \ell$.
We note 
$f$ must decrease to zero (why?).
So let us also suppose
that for $\ell \geq L$ we have $f(\ell) < \nicefrac{\epsilon}{2}$.
For such $\ell$ and $m$ we have via \eqref{impropriemann:eqseries}
\begin{equation*}
\sum_{n=\ell}^{m} f(n)
\leq
f(\ell)+
\int_\ell^{m} f < \nicefrac{\epsilon}{2} + \nicefrac{\epsilon}{2} = \epsilon .
\end{equation*}
The series is therefore Cauchy and thus converges.
The estimate in the
proposition is obtained by letting $m$ go to infinity in
\eqref{impropriemann:eqseries} with $\ell = k$.

Conversely suppose $\int_k^\infty f$ diverges.
As $f$ is positive then by
\propref{impropriemann:possimp},
the sequence $\{ \int_k^m f \}_{m=k}^\infty$ diverges to infinity.
Using
\eqref{impropriemann:eqseries} with $\ell = k$ we find
\begin{equation*}
\int_k^m f
\leq
\sum_{n=k}^{m-1} f(n) .
\end{equation*}
As the left hand side goes to infinity as $m \to \infty$, so does the right
hand side.
\end{proof}

\begin{example}
Let us show $\sum_{n=1}^\infty \frac{1}{n^2}$ exists and let us
estimate its sum to within 0.01.
As this series is the $p$-series for
$p=2$, we already know it converges, but we have only very roughly
estimated its sum.

Using fundamental theorem of calculus we find that for $k \in \N$
we have
\begin{equation*}
\int_{k}^\infty \frac{1}{x^2}~dx = \frac{1}{k} .
\end{equation*}
In particular, the series must converge.
But we also have that
\begin{equation*}
\frac{1}{k} = \int_k^\infty \frac{1}{x^2}~dx
\leq
\sum_{n=k}^\infty \frac{1}{n^2}
\leq
\frac{1}{k^2}
+
\int_k^\infty \frac{1}{x^2}~dx
=
\frac{1}{k^2}
+
\frac{1}{k} .
\end{equation*}
Adding the partial sum up to $k-1$ we get
\begin{equation*}
\frac{1}{k} + \sum_{n=1}^{k-1} \frac{1}{n^2}
\leq
\sum_{n=1}^\infty \frac{1}{n^2}
\leq
\frac{1}{k^2}
+
\frac{1}{k} + \sum_{n=1}^{k-1} \frac{1}{n^2} .
\end{equation*}
In other words,
$\nicefrac{1}{k} + \sum_{n=1}^{k-1} \nicefrac{1}{n^2}$ is an estimate for
the sum to within $\nicefrac{1}{k^2}$.
Therefore, if we wish to
find the sum to within 0.01, we note $\nicefrac{1}{{10}^2} = 0.01$.
We
obtain
\begin{equation*}
1.6397\ldots
\approx
\frac{1}{10} + \sum_{n=1}^{9} \frac{1}{n^2}
\leq
\sum_{n=1}^\infty \frac{1}{n^2}
\leq
\frac{1}{100}
+
\frac{1}{10} + \sum_{n=1}^{9} \frac{1}{n^2}
\approx
1.6497\ldots .
\end{equation*}
The actual sum is $\nicefrac{\pi^2}{6} \approx 1.6449\ldots$. 
\end{example}

\subsection*{Exercises}

\begin{exercise}
Finish the proof of \propref{impropriemann:ptest}.
\end{exercise}

\begin{exercise}
Find out for which $a \in \R$ does $\sum\limits_{n=1}^\infty e^{an}$ converge.
When the series converges, find an upper bound for the sum.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Estimate $\sum\limits_{n=1}^\infty \frac{1}{n(n+1)}$ correct to within 0.01
using the integral test.
  \item Compute the limit of the series exactly
and compare.
Hint: the sum telescopes.
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove 
\begin{equation*}
\int_{-\infty}^\infty \abs{\operatorname{sinc}(x)}~dx = \infty .
\end{equation*}
Hint: again, it is enough to show this on just one side.
\end{exercise}

\begin{exercise}
Can you interpret
\begin{equation*}
\int_{-1}^1 \frac{1}{\sqrt{\abs{x}}}~dx
\end{equation*}
as an improper integral?
  If so, compute its value.
\end{exercise}

\begin{exercise}
Take $f \colon [0,\infty) \to \R$, Riemann integrable on
every interval $[0,b]$, and such that there exist $M$, $a$, and $T$,
such that $\abs{f(t)} \leq M e^{at}$ for all $t \geq T$.
Show that the
\emph{\myindex{Laplace transform}} of $f$ exists.
That is, for
every $s > a$ the following integral converges:
\begin{equation*}
F(s) := \int_{0}^\infty f(t) e^{-st} ~dt .
\end{equation*}
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ be a Riemann integrable function
on every interval $[a,b]$, and such
that $\int_{-\infty}^\infty \abs{f(x)}~dx < \infty$.
Show that the
\emph{\myindex{Fourier sine and cosine transforms}}
exist.
That is, for every $\omega \geq 0$ the
following integrals converge
\begin{equation*}
F^s(\omega) := \frac{1}{\pi} \int_{-\infty}^\infty f(t) \sin(\omega t) ~dt ,
\qquad
F^c(\omega) := \frac{1}{\pi} \int_{-\infty}^\infty f(t) \cos(\omega t) ~dt .
\end{equation*}
Furthermore, show that $F^s$ and $F^c$ are bounded functions.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,\infty) \to \R$ is Riemann integrable on every interval
$[0,b]$.
Show that  $\int_0^\infty f$ converges if and only if
for every $\epsilon > 0$ there exists an $M$ such that if $M \leq a < b$
then $\abs{\int_a^b f} < \epsilon$.
\end{exercise}

\begin{exercise}
Suppose $f \colon [0,\infty) \to \R$ is nonnegative and
\emph{decreasing}.
\begin{enumerate}[a)]
 \item Show that if $\int_0^\infty f < \infty$, then $\lim\limits_{x\to\infty} f(x) = 0$.
  \item Show that the converse does not hold.
\end{enumerate}
\end{exercise}

\begin{exercise}
Find an example of an \emph{unbounded} continuous function $f \colon
[0,\infty) \to \R$ that is nonnegative and such that $\int_0^\infty f < \infty$.
Note that this means that $\lim_{x\to\infty} f(x)$ does not exist; compare
previous exercise.
Hint: on each interval $[k,k+1]$, $k \in \N$, define a function whose
integral over this interval is less than say $2^{-k}$.
\end{exercise}

\begin{exercise}[More challenging]
Find an example of a function $f \colon [0,\infty) \to \R$ integrable on all
intervals such that $\lim_{n\to\infty} \int_0^n f$ converges as a
limit of a sequence, but such that
$\int_0^\infty f$ does not exist.
Hint: for all $n\in \N$, divide $[n,n+1]$ into two halves.
In one half
make the function negative, on the other make the function positive.
\end{exercise}

\begin{exercise}
Show that if $f \colon [1,\infty) \to \R$ is such that
$g(x) := x^2 f(x)$ is a bounded function, then
$\int_1^\infty f$ converges.
\end{exercise}

\begin{exnote}
It is sometimes desirable to assign a value to integrals that normally
cannot be interpreted as even improper integrals,
e.g.\ $\int_{-1}^1 \nicefrac{1}{x}~dx$.
Suppose $f \colon [a,b] \to \R$ is a function and $a < c < b$,
where $f$ is Riemann integrable on all intervals
$[a,c-\epsilon]$ and $[c+\epsilon,b]$ for all $\epsilon > 0$.
Define
the \emph{\myindex{Cauchy principal value}} of $\int_a^b f$ as
\begin{equation*}
p.v.\!\int_a^b f := \lim_{\epsilon\to 0^+}
\left(
\int_a^{c-\epsilon} f + 
\int_{c+\epsilon}^b f
\right) ,
\end{equation*}
if the limit exists.
\end{exnote}

\begin{exercise}
\begin{enumerate}[a)]
 \item Compute $p.v.\!\int_{-1}^1 \nicefrac{1}{x}~dx$.
 \item Compute
$\lim_{\epsilon\to 0^+}
( \int_{-1}^{-\epsilon} \nicefrac{1}{x}~dx + 
\int_{2\epsilon}^1 \nicefrac{1}{x}~dx )$ and show it is not equal
to the principal value.
 \item Show that if $f$ is integrable on $[a,b]$, then
$p.v.\!\int_a^b f = \int_a^b f$.
 \item Find an example of an $f$ with a singularity at $c$ as above
such that 
$p.v.\!\int_a^b f$ exists, but the improper integrals
$\int_a^c f$ and $\int_c^b f$ diverge.
 \item Suppose 
$f \colon [-1,1] \to \R$ is continuous.
Show that
$p.v.\!\int_{-1}^1 \frac{f(x)}{x}~dx$ exists.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $f \colon \R \to \R$ and 
$g \colon \R \to \R$ be continuous functions, where
$g(x) = 0$ for all $x \notin [a,b]$ for some interval $[a,b]$.
\begin{enumerate}[a)]
 \item Show that the
\emph{\myindex{convolution}}
\begin{equation*}
(g * f)(x) := \int_{-\infty}^\infty f(t)g(x-t)~dt 
\end{equation*}
is well-defined for all $x \in \R$.
  \item Suppose $\int_{-\infty}^\infty \abs{f(x)}~dx < \infty$.
Prove that
\begin{equation*}
\lim_{x \to -\infty} (g * f)(x) = 0, \qquad \text{and} \qquad
\lim_{x \to \infty} (g * f)(x) = 0 .
\end{equation*}
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Sequences of Functions} \label{fs:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pointwise and uniform convergence}
\label{sec:puconv}

\sectionnotes{1--1.5 lecture}

Up till now when we talked about sequences we always talked about
sequences of numbers.
However, a very useful concept in analysis is to use a
sequence of functions.
For example, a solution to some
differential equation
might be found by finding only approximate solutions.
Then the real solution is
some sort of limit of those approximate solutions.

When talking about sequences of functions, the 
tricky part is that there are multiple notions of a limit.
Let us describe two common
notions of a limit of a sequence of functions.

\subsection*{Pointwise convergence}

\begin{defn}
\index{pointwise convergence}
For every $n \in \N$
let $f_n \colon S \to \R$ be a function.
We say the sequence
$\{ f_n \}_{n=1}^\infty$
\emph{\myindex{converges pointwise}} to $f \colon S \to \R$, if for every $x
\in S$
we have
\begin{equation*}
f(x) =
\lim_{n\to\infty} f_n(x) .
\end{equation*}
\end{defn}

It is common to say that $f_n \colon S \to \R$
\emph{converges to $f$ on $T \subset \R$}
for some $f \colon T \to \R$.
In that case we, of course, mean 
$f(x) = \lim\, f_n(x)$ for every $x \in T$.
We simply mean that the
restrictions of $f_n$ to $T$ converge pointwise to $f$.

\begin{example}
The sequence of functions defined by $f_n(x) := x^{2n}$ converges to $f \colon [-1,1] \to
\R$ on $[-1,1]$, where
\begin{equation*}
f(x) =
\begin{cases}
1 & \text{if $x=-1$ or $x=1$,} \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
See \figureref{x2nfig}.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input x2nfig.eepic
\caption{Graphs of $f_1$, $f_2$, $f_3$, and $f_8$ for $f_n(x) :=
x^{2n}$.\label{x2nfig}}
\end{center}
\end{figure}

To see this is so, first take $x \in (-1,1)$.
Then 
$0 \leq x^2 < 1$.
We have seen before that
\begin{equation*}
\abs{x^{2n} - 0} = {(x^2)}^n \to 0 \quad \text{as} \quad n \to \infty .
\end{equation*}
Therefore $\lim\,f_n(x) = 0$.

When $x = 1$ or $x=-1$, then $x^{2n} = 1$ for all $n$ and hence
$\lim\,f_n(x) = 1$.
We also note that $\{ f_n(x) \}$ does not converge for all other $x$.
\end{example}

Often, functions are given as a series.
In this case, we use
the notion of pointwise convergence to find the values of the function.

\begin{example}
We write
\begin{equation*}
\sum_{k=0}^\infty x^k
\end{equation*}
to denote the limit of the functions
\begin{equation*}
f_n(x) := \sum_{k=0}^n x^k .
\end{equation*}
When studying series, 
we have seen that on $x \in (-1,1)$ the $f_n$ converge pointwise to
\begin{equation*}
\frac{1}{1-x} .
\end{equation*}

The subtle point here is that while
$\frac{1}{1-x}$ is defined for all $x \not=1$, and $f_n$ are 
defined for all $x$ (even at $x=1$), convergence only happens on $(-1,1)$.

Therefore, when we write
\begin{equation*}
f(x) := \sum_{k=0}^\infty x^k
\end{equation*}
we mean that $f$ is defined on $(-1,1)$ and is the pointwise limit
of the partial sums.
\end{example}

\begin{example}
Let $f_n(x) := \sin(xn)$.
Then $f_n$ does not converge pointwise
to any function on any interval.
It may converge at certain points, such
as when $x=0$ or $x=\pi$.
It is left as an exercise that in any interval
$[a,b]$, there exists an $x$ such that $\sin(xn)$ does not have a limit
as $n$ goes to infinity.
\end{example}

Before we move to uniform convergence, let us reformulate pointwise
convergence in a different way.
We leave the proof to the reader, it is a simple application of the
definition of convergence of a sequence of real numbers.

\begin{prop} \label{ptwsconv:prop}
Let $f_n \colon S \to \R$ and $f \colon S \to \R$ be functions.
Then $\{ f_n \}$ converges pointwise to $f$ if and only if
for every $x \in S$, and every $\epsilon > 0$, there exists
an $N \in \N$ such that
\begin{equation*}
\abs{f_n(x)-f(x)} < \epsilon
\end{equation*}
for all $n \geq N$.
\end{prop}

The key point here is that $N$ can depend on $x$, not just on
$\epsilon$.
That is, for each $x$ we can pick a different $N$.
If we can pick one $N$ for all $x$, we have what is called
uniform convergence.

\subsection*{Uniform convergence}

\begin{defn}
\index{uniform convergence}
Let $f_n \colon S \to \R$ be functions.
We say the sequence $\{ f_n \}$
\emph{\myindex{converges uniformly}} to $f \colon S \to \R$, if for
every $\epsilon > 0$ there exists an $N \in \N$ such that 
for all $n \geq N$ we have
\begin{equation*}
\abs{f_n(x) - f(x)} < \epsilon \qquad \text{for all $x \in S$.}
\end{equation*}
\end{defn}

Note that $N$ now cannot depend on $x$.
Given $\epsilon > 0$
we must find an $N$ that works for all $x \in S$.
Because of
\propref{ptwsconv:prop}, we see that uniform convergence
implies pointwise convergence.

\begin{prop}
Let $\{ f_n \}$ be a sequence of functions $f_n \colon S \to \R$.
If $\{ f_n \}$ converges
uniformly to $f \colon S \to \R$, then $\{ f_n \}$ converges pointwise to $f$.
\end{prop}

The converse does not hold.

\begin{example}
The functions $f_n(x) := x^{2n}$ do not converge uniformly on $[-1,1]$,
even though they converge pointwise.
To see this, suppose for contradiction
that the convergence is uniform.
For $\epsilon := \nicefrac{1}{2}$, there would have
to exist an $N$ such that $x^{2N} = \abs{x^{2N} - 0} < \nicefrac{1}{2}$ for all $x \in
(-1,1)$ (as $f_n(x)$ converges to 0 on $(-1,1)$).
But that means that
for any sequence $\{ x_k \}$ in $(-1,1)$ such that $\lim\, x_k = 1$
we have $x_k^{2N} < \nicefrac{1}{2}$ for all $k$.
On the other hand
$x^{2N}$ is a continuous function of $x$ (it is a polynomial), therefore
we obtain a contradiction
\begin{equation*}
1 = 1^{2N}  = \lim_{k\to\infty} x_k^{2N} \leq \nicefrac{1}{2} .
\end{equation*}

However, if we restrict our domain to $[-a,a]$ where $0 < a < 1$, then
$\{ f_n \}$ converges uniformly to 0 on $[-a,a]$.
First note
that $a^{2n} \to 0$ as $n \to \infty$.
Thus given $\epsilon > 0$,
pick $N \in \N$ such that
$a^{2n} < \epsilon$ for all $n \geq N$.
Then for any $x \in [-a,a]$
we have $\abs{x} \leq a$.
Therefore, for $n \geq N$
\begin{equation*}
\abs{x^{2N}} = \abs{x}^{2N} \leq a^{2N} < \epsilon .
\end{equation*}
\end{example}

\subsection*{Convergence in uniform norm}

For bounded functions there is another more abstract way to 
think of uniform convergence.
To every bounded function we assign
a certain nonnegative number (called the uniform norm).
This number
measures the ``distance'' of the function from 0.
We can then ``measure''
how far two functions are from each other.
We simply translate
a statement about uniform convergence into a statement about a certain
sequence of real numbers converging to zero.

\begin{defn} \label{def:unifnorm}
Let $f \colon S \to \R$ be a bounded function.
Define
\begin{equation*}
\norm{f}_u :=
\sup \bigl\{ \abs{f(x)} : x \in S \bigr\} .
\end{equation*}
$\norm{\cdot}_u$ is called the \emph{\myindex{uniform norm}}.
\end{defn}

To use this notation%
\footnote{The notation nor terminology is not completely standardized.
The norm is
also called the
\emph{\myindex{sup norm}} or
\emph{\myindex{infinity norm}}, and in addition
to $\norm{f}_u$ and $\norm{f}_S$ it is sometimes written
as $\norm{f}_{\infty}$ or $\norm{f}_{\infty,S}$.}
and this concept, the domain $S$ must be fixed.
Some authors
use
the notation
$\norm{f}_S$ to emphasize the dependence on $S$.

\begin{prop}
A sequence of bounded functions $f_n \colon S \to \R$ converges
uniformly to $f \colon S \to \R$, if and only if
\begin{equation*}
\lim_{n\to\infty} \norm{f_n - f}_u = 0 .
\end{equation*}
\end{prop}

\begin{proof}
First suppose 
$\lim \norm{f_n - f}_u = 0$.
Let $\epsilon > 0$ be
given.
Then there exists an $N$ such that
for $n \geq N$ we have $\norm{f_n - f}_u < \epsilon$.
As $\norm{f_n-f}_u$
is the supremum of $\abs{f_n(x)-f(x)}$, we see that for all $x$
we have $\abs{f_n(x)-f(x)} < \epsilon$.

On the other hand, suppose $\{ f_n \}$ converges uniformly to $f$.
Let $\epsilon > 0$ be given.
Then find $N$ such that 
$\abs{f_n(x)-f(x)} < \epsilon$ for all $x \in S$.
Taking the supremum we see that
$\norm{f_n - f}_u < \epsilon$.
Hence $\lim \norm{f_n-f}_u = 0$.
\end{proof}

Sometimes it is said that \emph{$\{ f_n \}$ converges to $f$ in uniform norm}
\index{converges in uniform norm}
\index{uniform norm convergence}
instead of \emph{converges uniformly}.
The proposition
says that the two notions are the same thing.

\begin{example}
Let $f_n \colon [0,1] \to \R$ be defined by $f_n(x) := \frac{nx+ \sin(nx^2)}{n}$.
Then we claim $\{ f_n \}$ converges uniformly to $f(x) := x$.
Let us compute:
\begin{equation*}
\begin{split}
\norm{f_n-f}_u
& =
\sup \left\{ \abs{\frac{nx+ \sin(nx^2)}{n} - x} : x \in [0,1] \right\}
\\
& =
\sup \left\{ \frac{\abs{\sin(nx^2)}}{n} : x \in [0,1] \right\}
\\
& \leq
\sup \{ \nicefrac{1}{n} : x \in [0,1] \}
\\
& = \nicefrac{1}{n}.
\end{split}
\end{equation*}
\end{example}

Using uniform norm, we define Cauchy sequences in a similar way
as we define Cauchy sequences of real numbers.

\begin{defn}
Let $f_n \colon S \to \R$ be bounded functions.
The sequence is \emph{\myindex{Cauchy in the uniform norm}}
or \emph{\myindex{uniformly Cauchy}}
if for every $\epsilon > 0$, there exists an $N \in \N$ such
that for $m,k \geq N$ we have
\begin{equation*}
\norm{f_m-f_k}_u < \epsilon .
\end{equation*}
\end{defn}

\begin{prop} \label{prop:uniformcauchy}
Let $f_n \colon S \to \R$ be bounded functions.
Then $\{ f_n \}$ is Cauchy in the uniform norm if and only if
there exists an $f \colon S \to \R$ and $\{ f_n \}$ converges
uniformly to $f$.
\end{prop}

\begin{proof}
Let us first suppose $\{ f_n \}$ is Cauchy in the uniform norm.
Let us define $f$.
Fix $x$, then
the sequence $\{ f_n(x) \}$ is Cauchy because
\begin{equation*}
\abs{f_m(x)-f_k(x)}
\leq
\norm{f_m-f_k}_u .
\end{equation*}
Thus $\{ f_n(x) \}$ converges to some real number.
Define $f \colon S
\to \R$ by
\begin{equation*}
f(x) := \lim_{n \to \infty} f_n(x) .
\end{equation*}
The sequence
$\{ f_n \}$ converges pointwise to $f$.
To show that the convergence
is uniform, let $\epsilon > 0$ be given.
Find an $N$ such that
for $m, k \geq N$ we have
$\norm{f_m-f_k}_u < \nicefrac{\epsilon}{2}$.
In other words for
all $x$ we have
$\abs{f_m(x)-f_k(x)} < \nicefrac{\epsilon}{2}$.
We take the limit
as $k$ goes to infinity.
Then $\abs{f_m(x)-f_k(x)}$
goes to $\abs{f_m(x)-f(x)}$.
Consequently for all $x$ we get
\begin{equation*}
\abs{f_m(x)-f(x)} \leq \nicefrac{\epsilon}{2} < \epsilon .
\end{equation*}
And hence $\{ f_n \}$ converges uniformly.

For the other direction, suppose $\{ f_n \}$ converges uniformly to
$f$.
Given $\epsilon > 0$, find $N$ such that for all $n \geq N$
we have $\abs{f_n(x)-f(x)} < \nicefrac{\epsilon}{4}$ for all $x \in S$.
Therefore for all $m, k \geq N$ we have
\begin{equation*}
\abs{f_m(x)-f_k(x)} = 
\abs{f_m(x)-f(x)+f(x)-f_k(x)} \leq
\abs{f_m(x)-f(x)}+\abs{f(x)-f_k(x)} < \nicefrac{\epsilon}{4} +
\nicefrac{\epsilon}{4} .
\end{equation*}
Take supremum over all $x$ to obtain
\begin{equation*}
\norm{f_m-f_k}_u \leq \nicefrac{\epsilon}{2} < \epsilon .  \qedhere
\end{equation*}
\end{proof}

\subsection*{Exercises}

\begin{exercise}
Let $f$ and $g$ be bounded functions on $[a,b]$.
Prove 
\begin{equation*}
\norm{f+g}_u \leq \norm{f}_u + \norm{g}_u .
\end{equation*}
\end{exercise}

%\pagebreak[2]

\begin{exercise}
\begin{enumerate}[a)]
 \item Find the pointwise limit $\dfrac{e^{x/n}}{n}$ for $x \in \R$.
  \item Is the limit uniform on $\R$?
   \item Is the limit uniform on $[0,1]$?
\end{enumerate}
\end{exercise}

\begin{exercise}
Suppose $f_n \colon S \to \R$ are functions that converge uniformly
to $f \colon S \to \R$.
Suppose $A \subset S$.
Show that
the sequence of restrictions $\{ f_n|_A \}$ converges uniformly to $f|_A$.
\end{exercise}

\begin{exercise}
Suppose $\{ f_n \}$ and $\{ g_n \}$ defined on some set $A$ converge to
$f$ and $g$ respectively pointwise.
Show that $\{ f_n+g_n \}$ converges
pointwise to $f+g$.
\end{exercise}

\begin{exercise}
Suppose $\{ f_n \}$ and $\{ g_n \}$ defined on some set $A$ converge to
$f$ and $g$ respectively uniformly on $A$.
Show that $\{ f_n+g_n \}$
converges uniformly to $f+g$ on $A$.
\end{exercise}

\begin{exercise}
Find an example of a sequence of functions $\{ f_n \}$ and $\{ g_n \}$
that converge uniformly to some $f$ and $g$ on some set $A$, but such that
$\{ f_ng_n \}$ (the multiple) does not converge uniformly to $fg$ on $A$.
Hint: Let $A := \R$, let $f(x):=g(x) := x$.
You can even pick $f_n = g_n$.
\end{exercise}

\begin{exercise}
Suppose there exists a sequence of functions $\{ g_n \}$ uniformly
converging to $0$ on $A$.
Now suppose we have a sequence of functions
$\{ f_n \}$ and a function $f$ on $A$ such that
\begin{equation*}
\abs{f_n(x) - f(x)} \leq g_n(x) 
\end{equation*}
for all $x \in A$.
Show that $\{ f_n \}$ converges uniformly to $f$ on $A$.
\end{exercise}

\begin{exercise}
Let $\{ f_n \}$, $\{ g_n \}$ and $\{ h_n \}$ be sequences of functions on
$[a,b]$.
Suppose $\{ f_n \}$ and $\{ h_n \}$ converge uniformly to some function
$f \colon [a,b] \to \R$ and suppose $f_n(x) \leq g_n(x) \leq h_n(x)$
for all $x \in [a,b]$.
Show that $\{ g_n \}$ converges uniformly to $f$.
\end{exercise}

\begin{exercise}
Let $f_n \colon [0,1] \to \R$ be a sequence of increasing functions (that
is, $f_n(x) \geq f_n(y)$ whenever $x \geq y$).
Suppose $f_n(0) = 0$
and $\lim\limits_{n \to \infty} f_n(1) = 0$.
Show that
$\{ f_n \}$
converges uniformly to $0$.
\end{exercise}

\begin{exercise}
Let $\{f_n\}$ be a sequence of functions defined on $[0,1]$.
Suppose there exists a sequence of distinct numbers $x_n \in [0,1]$ such that
\begin{equation*}
f_n(x_n) = 1 .
\end{equation*}
Prove or disprove the following statements:
\begin{enumerate}[a)]
 \item True or false: There exists $\{ f_n \}$ as above that converges to $0$
pointwise.
  \item True or false: There exists $\{ f_n \}$ as above that converges to $0$
uniformly on $[0,1]$.
\end{enumerate}
\end{exercise}

\begin{exercise}
Fix a continuous $h \colon [a,b] \to \R$.
Let $f(x) := h(x)$ for $x \in [a,b]$,
$f(x) := h(a)$ for $x < a$ and $f(x) := h(b)$ for all $x > b$.
First show
that $f \colon \R \to \R$ is continuous.
Now let $f_n$ be
the function $g$ from \exerciseref{exercise:smoothingout} with
$\epsilon = \nicefrac{1}{n}$, defined on the interval $[a,b]$.
Show that $\{
f_n \}$ converges uniformly to $h$ on $[a,b]$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Interchange of limits}
\label{sec:liminter}

\sectionnotes{1--1.5 lectures}

Large parts of modern analysis deal mainly with the question of the
interchange of two limiting operations.
When
we have a chain of two limits, we cannot always just swap the limits.
For example,
\begin{equation*}
0 = 
\lim_{n\to\infty}
\left(
\lim_{k\to\infty}
\frac{\nicefrac{n}{k}}{\nicefrac{n}{k} + 1}
\right)
\not=
\lim_{k\to\infty}
\left(
\lim_{n\to\infty}
\frac{\nicefrac{n}{k}}{\nicefrac{n}{k} + 1}
\right)
= 1 .
\end{equation*}

When talking about sequences of functions, interchange of limits comes up
quite often.
We treat two cases.
First we look at continuity of
the limit, and second we look at the integral of the limit.

\subsection*{Continuity of the limit}

If we have a sequence $\{ f_n \}$ of continuous functions, is the limit continuous?
Suppose $f$ is the (pointwise) limit of $\{ f_n \}$.
If $\lim\, x_k = x$
we are interested in the following
interchange of limits.
The equality we have to prove (it is not always true)
is marked with a question mark.
In fact the limits to the left
of the question mark might not even exist.
\begin{equation*}
\lim_{k \to \infty} 
f(x_k)
=
\lim_{k \to \infty} 
\Bigl(
\lim_{n \to \infty} f_n(x_k)
\Bigr)
\overset{\text{\textbf{?}}}{=}
\lim_{n \to \infty}
\Bigl(
\lim_{k \to \infty} 
f_n(x_k)
\Bigr)
=
\lim_{n \to \infty}
f_n(x)
=
f(x) .
\end{equation*}
In particular, we wish to find conditions on the sequence $\{ f_n \}$
so that the above equation holds.
It turns out that if we only require pointwise convergence, then the limit
of a sequence of functions need not be continuous, and the above equation
need not hold.

\begin{example}
Let $f_n \colon [0,1] \to \R$
be defined as
\begin{equation*}
f_n(x) :=
\begin{cases}
1-nx &  \text{if $x < \nicefrac{1}{n}$,}\\
0 &  \text{if $x \geq \nicefrac{1}{n}$.}
\end{cases}
\end{equation*}
See \figureref{contconvcntr:fig}.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input contconvcntr.eepic
\caption{Graph of $f_n(x)$.%
\label{contconvcntr:fig}}
\end{center}
\end{figure}

Each function $f_n$ is continuous.
Fix an $x \in (0,1]$.
If $n \geq \nicefrac{1}{x}$,
then $x \geq \nicefrac{1}{n}$.
Therefore for $n \geq \nicefrac{1}{x}$
we have $f_n(x) = 0$, and so
\begin{equation*}
\lim_{n \to \infty} f_n(x) = 0.
\end{equation*}
On the other hand if $x=0$, then
\begin{equation*}
\lim_{n \to \infty} f_n(0) = 
\lim_{n \to \infty} 1 = 1.
\end{equation*}
Thus the pointwise limit of $f_n$ is the function
$f \colon [0,1] \to \R$ defined by
\begin{equation*}
f(x) :=
\begin{cases}
1 &  \text{if $x = 0$,}\\
0 &  \text{if $x > 0$.}
\end{cases}
\end{equation*}
The function $f$ is not continuous at 0.
\end{example}

If we, however, require the convergence to be uniform, the limits can
be interchanged.

\begin{thm}
Let $\{ f_n \}$ be 
a sequence of continuous functions $f_n \colon S \to \R$ converging
uniformly to  $f \colon S \to \R$.
Then $f$ is continuous.
\end{thm}

\begin{proof}
Let $x \in S$ be fixed.
Let $\{ x_n \}$ be a sequence in $S$
converging to $x$.

Let $\epsilon > 0$ be given.
As $\{ f_k \}$ converges uniformly to $f$, we find a $k \in \N$ such that
\begin{equation*}
\abs{f_k(y)-f(y)} < \nicefrac{\epsilon}{3}
\end{equation*}
for all $y \in S$.
As $f_k$ is continuous at $x$,
we find an $N \in \N$ such that for $m \geq N$
we have 
\begin{equation*}
\abs{f_k(x_m)-f_k(x)} < \nicefrac{\epsilon}{3} .
\end{equation*}
Thus for
$m \geq N$ we have
\begin{equation*}
\begin{split}
\abs{f(x_m)-f(x)}
& =
\abs{f(x_m)-f_k(x_m)+f_k(x_m)-f_k(x)+f_k(x)-f(x)}
\\
& \leq
\abs{f(x_m)-f_k(x_m)}+
\abs{f_k(x_m)-f_k(x)}+
\abs{f_k(x)-f(x)}
\\
& <
\nicefrac{\epsilon}{3} +
\nicefrac{\epsilon}{3} +
\nicefrac{\epsilon}{3} = \epsilon .
\end{split}
\end{equation*}
Therefore $\{ f(x_m) \}$ converges to $f(x)$ and hence $f$ is continuous at
$x$.
As $x$ was arbitrary, $f$ is continuous everywhere.
\end{proof}

\subsection*{Integral of the limit}

Again, if we simply require pointwise convergence, then the integral
of a limit of a sequence of functions need not be equal to the limit
of the integrals.

\begin{example}
Let $f_n \colon [0,1] \to \R$
be defined as
\begin{equation*}
f_n(x) :=
\begin{cases}
0 &  \text{if $x = 0$,}\\
n-n^2x &  \text{if $0 < x < \nicefrac{1}{n}$,}\\
0 &  \text{if $x \geq \nicefrac{1}{n}$.}
\end{cases}
\end{equation*}
See \figureref{intconvcntr:fig}.

\begin{figure}[h!t]
\begin{center}
% To get fonts right we have to use .eepic not .pdf
\input intconvcntr.eepic
\caption{Graph of $f_n(x)$.%
\label{intconvcntr:fig}}
\end{center}
\end{figure}

Each $f_n$ is Riemann integrable (it is continuous on $(0,1]$ and bounded),
and it is easy to see
\begin{equation*}
\int_0^1 f_n =
\int_0^{\nicefrac{1}{n}} (n-n^2x)~dx = \nicefrac{1}{2} .
\end{equation*}
Let us compute the pointwise limit of $\{ f_n \}$.
Fix an $x \in (0,1]$.
For $n \geq \nicefrac{1}{x}$
we have $x \geq \nicefrac{1}{n}$ and so $f_n(x) = 0$.
Therefore
\begin{equation*}
\lim_{n \to \infty} f_n(x) = 0.
\end{equation*}
We also have $f_n(0) = 0$ for all $n$.
Therefore the pointwise
limit of $\{ f_n \}$ is the zero function.
Thus
\begin{equation*}
\nicefrac{1}{2} =
\lim_{n\to\infty}
\int_0^1 f_n (x)~dx
\not=
\int_0^1
\left(
\lim_{n\to\infty}
f_n(x)\right)~dx
=
\int_0^1 0~dx = 0 .
\end{equation*}
\end{example}

But
if we again require the convergence to be uniform, the limits can
be interchanged.

\begin{thm} \label{integralinterchange:thm}
Let $\{ f_n \}$ be a sequence of Riemann integrable
functions
$f_n \colon [a,b] \to \R$
converging uniformly to $f \colon [a,b]
\to \R$.
Then $f$ is Riemann integrable and
\begin{equation*}
\int_a^b f = \lim_{n\to\infty} \int_a^b f_n .
\end{equation*}
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given.
As $f_n$ goes to $f$ uniformly, we find an $M \in \N$ such that
for all $n \geq M$ we have 
$\abs{f_n(x)-f(x)} < \frac{\epsilon}{2(b-a)}$ for all $x \in [a,b]$.
In particular, by reverse triangle inequality
$\abs{f(x)} < \frac{\epsilon}{2(b-a)} + \abs{f_n(x)}$ for all $x$,
hence $f$ is bounded
as $f_n$ is bounded.
Note that $f_n$ is integrable and compute
\begin{equation*}
\begin{split}
\overline{\int_a^b} f
-
\underline{\int_a^b} f
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) + f_n(x) \bigr)~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) + f_n(x) \bigr)~dx
\\
& \leq
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx +  \overline{\int_a^b} f_n(x) ~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx -  \underline{\int_a^b} f_n(x) ~dx
\\
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx +  \int_a^b f_n(x) ~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx -  \int_a^b f_n(x) ~dx
\\
& =
\overline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx
-
\underline{\int_a^b} \bigl( f(x) - f_n(x) \bigr)~dx
\\
& \leq
\frac{\epsilon}{2(b-a)} (b-a) + 
\frac{\epsilon}{2(b-a)} (b-a) = \epsilon .
\end{split}
\end{equation*}
The first inequality is \exerciseref{exercise:upperlowerlinineq}
(it follows as supremum of a sum is less than or equal to the sum of
suprema and similarly for infima, see \exerciseref{exercise:sumofsup}).
The second inequality follows from \propref{intulbound:prop} and 
the fact that for all $x \in [a,b]$ we have
$\frac{-\epsilon}{2(b-a)} < f(x)-f_n(x) < \frac{\epsilon}{2(b-a)}$.
As $\epsilon > 0$ was arbitrary, $f$ is Riemann integrable.

Finally we compute $\int_a^b f$.
We apply \propref{intbound:prop}
in the calculation.
Again, for $n \geq M$ (where $M$ is the same as above) we have
\begin{equation*}
\begin{split}
\abs{\int_a^b f - \int_a^b f_n} & = 
\abs{ \int_a^b \bigl(f(x) - f_n(x)\bigr)~dx}
\\
& \leq
\frac{\epsilon}{2(b-a)} (b-a) = \frac{\epsilon}{2} < \epsilon .
\end{split}
\end{equation*}
Therefore $\{ \int_a^b f_n \}$ converges to $\int_a^b f$.
\end{proof}

\begin{example}
Suppose we wish to compute
\begin{equation*}
\lim_{n\to\infty} \int_0^1 \frac{nx+ \sin(nx^2)}{n} ~dx .
\end{equation*}
It is impossible to compute the integrals for any particular $n$ using 
calculus as $\sin(nx^2)$ has no closed-form antiderivative.
However,
we can compute the limit.
We have shown before that $\frac{nx+ \sin(nx^2)}{n}$ converges uniformly
on $[0,1]$ to $x$.
By \thmref{integralinterchange:thm}, the limit exists and
\begin{equation*}
\lim_{n\to\infty} \int_0^1 \frac{nx+ \sin(nx^2)}{n} ~dx
=
\int_0^1
x ~dx = \nicefrac{1}{2} .
\end{equation*}
\end{example}

\begin{example}
If convergence is only pointwise, the limit need not even be Riemann
integrable.
On $[0,1]$ define
\begin{equation*}
f_n(x) :=
\begin{cases}
1 & \text{if $x = \nicefrac{p}{q}$ in lowest terms and $q \leq n$,} \\
0 & \text{otherwise.}
\end{cases}
\end{equation*}
The function $f_n$ differs from the zero function at finitely many points;
there are only finitely many fractions in $[0,1]$ with denominator less than
or equal to $n$.
 So $f_n$ is integrable and $\int_0^1 f_n = \int_0^1 0 =
0$.
It is an easy exercise to show that $\{ f_n \}$ converges pointwise to the
Dirichlet function
\begin{equation*}
f(x) :=
\begin{cases}
1 & \text{if $x \in \Q$,} \\
0 & \text{otherwise,}
\end{cases}
\end{equation*}
which is not Riemann integrable.
\end{example}

\begin{example}
In fact, if the convergence is only pointwise, the limit of bounded
functions is not even necessarily bounded.
Define $f_n \colon [0,1] \to \R$ by
\begin{equation*}
f_n(x) :=
\begin{cases}
0 & \text{ if $x < \nicefrac{1}{n}$,}\\
\nicefrac{1}{x} & \text{ else.}
\end{cases}
\end{equation*}
For every $n$ we get that $\abs{f_n(x)} \leq n$ for all $x \in [0,1]$ so the
functions are bounded.
However $f_n$ converge pointwise to
\begin{equation*}
f(x) :=
\begin{cases}
0 & \text{ if $x = 0$,}\\
\nicefrac{1}{x} & \text{ else,}
\end{cases}
\end{equation*}
which is unbounded.
\end{example}

Let us remark that
while uniform convergence is enough to swap limits with integrals, it is not,
however, enough to swap limits with derivatives, unless you also have
uniform convergence of the derivatives themselves.
See the exercises below.

\subsection*{Exercises}

\begin{exercise}
While uniform convergence preserves continuity, it does not preserve
differentiability.
Find an explicit example of a sequence of
differentiable functions on $[-1,1]$ that converge uniformly to
a function $f$ such that $f$ is not differentiable.
Hint: Consider
$\abs{x}^{1+1/n}$, show that these functions are differentiable,
converge uniformly, and then show that the limit is not differentiable.
\end{exercise}

\begin{exercise}
Let $f_n(x) = \frac{x^n}{n}$.
Show that $\{ f_n \}$ converges uniformly to
a differentiable function $f$ on $[0,1]$ (find $f$).
However, show that
$f'(1) \not= \lim\limits_{n\to\infty} f_n'(1)$.
\end{exercise}

\begin{exnote}
Note: The previous two exercises show that
we cannot simply swap limits with derivatives, even if the convergence is
uniform.
See also \exerciseref{c1uniflim:exercise} below.
\end{exnote}

\begin{exercise}
Let $f \colon [0,1] \to \R$ be a Riemann integrable (hence bounded)
function.
Find
$\displaystyle \lim_{n\to\infty} \int_0^1 \frac{f(x)}{n} ~dx$.
\end{exercise}

\begin{exercise}
Show
$\displaystyle \lim_{n\to\infty} \int_1^2 e^{-nx^2} ~dx = 0$.
Feel free to
use
what you know about the exponential function from calculus.
\end{exercise}

\begin{exercise}
Find an example of a sequence of continuous functions on $(0,1)$ that converges 
pointwise to a continuous function on $(0,1)$, but the convergence is not
uniform.
\end{exercise}

\begin{exnote}
Note: In the previous exercise, $(0,1)$ was picked for simplicity.
For a
more challenging exercise, replace $(0,1)$ with $[0,1]$.
\end{exnote}

\begin{exercise}
True/False; prove or find a counterexample to the following statement:
If $\{ f_n \}$ is a sequence of everywhere discontinuous functions on $[0,1]$
that converge uniformly to a function $f$, then $f$ is everywhere
discontinuous.
\end{exercise}

\begin{exercise} \label{c1uniflim:exercise}
For a continuously differentiable function $f \colon [a,b] \to \R$, define
\begin{equation*}
\norm{f}_{C^1} := \norm{f}_u + \norm{f'}_u .
\end{equation*}
Suppose $\{ f_n \}$ is a sequence of continuously differentiable
functions such that for every $\epsilon >0$, there exists an $M$
such that for all $n,k \geq M$ we have
\begin{equation*}
\norm{f_n-f_k}_{C^1} < \epsilon .
\end{equation*}
Show that $\{ f_n \}$ converges uniformly to some continuously differentiable
function $f \colon [a,b] \to \R$.
\end{exercise}

\begin{exnote}
For the following two exercises let us define for a Riemann integrable
function $f \colon [0,1] \to
\R$ the following number
\begin{equation*}
\norm{f}_{L^1} := 
\int_0^1 \abs{f(x)}~dx .
\end{equation*}
It is true that $\abs{f}$ is integrable whenever $f$ is, see
\exerciseref{exercise:hardabsint}.
This norm defines another very common type of
convergence called the $L^1$-convergence, that is however a bit more
subtle.
\end{exnote}

\begin{exercise}
Suppose $\{ f_n \}$ is a sequence of Riemann integrable functions on $[0,1]$
that converges uniformly
to $0$.
Show that
\begin{equation*}
\lim_{n\to\infty} \norm{f_n}_{L^1} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Find a sequence of Riemann integrable functions 
$\{ f_n \}$ on $[0,1]$ that converges pointwise to $0$, but
\begin{equation*}
\lim_{n\to\infty} \norm{f_n}_{L^1} \text{ does not exist (is $\infty$).}
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Prove \emph{\myindex{Dini's theorem}}:
Let $f_n \colon [a,b] \to \R$ be a sequence of continuous functions such that
\begin{equation*}
0 \leq f_{n+1}(x) \leq f_n(x) \leq \cdots \leq f_1(x) 
\qquad \text{for all $n \in \N$.}
\end{equation*}
Suppose $\{ f_n \}$ converges pointwise to $0$.
Show that $\{ f_n \}$ converges to zero uniformly.
\end{exercise}

\begin{exercise}
Suppose $f_n \colon [a,b] \to \R$ is a sequence of continuous
functions that
converges pointwise
to a continuous $f \colon [a,b] \to \R$.
Suppose that
for any $x \in [a,b]$ the sequence $\{ \abs{f_n(x)-f(x)} \}$ is monotone.
Show that the sequence $\{f_n\}$ converges uniformly.
\end{exercise}

\begin{exercise}
Find a sequence of Riemann integrable functions $f_n \colon [0,1] \to \R$ such
that $\{ f_n \}$ converges to zero pointwise, and such that
\begin{enumerate}[a)]
 \item $\bigl\{ \int_0^1 f_n \bigr\}_{n=1}^\infty$ increases without bound,
  \item $\bigl\{ \int_0^1 f_n \bigr\}_{n=1}^\infty$ is the sequence $-1,1,-1,1,-1,1, \ldots$.
\end{enumerate}
\end{exercise}

\begin{exnote}
It is possible to define a 
\emph{\myindex{joint limit}} of a double sequence $\{ x_{n,m} \}$ of real
numbers (that is a function from $\N \times \N$ to $\R$).
We say $L$ is the joint limit of $\{ x_{n,m} \}$ and write
\begin{equation*}
\lim_{\substack{n\to\infty\\m\to\infty}}
x_{n,m} = L ,
\qquad
\text{or}
\qquad
\lim_{(n,m) \to \infty}
x_{n,m} = L ,
\end{equation*}
if for every $\epsilon > 0$, there
exists an $M$ such that if $n \geq M$ and $m \geq M$, then
$\abs{x_{n,m} - L} < \epsilon$.
\end{exnote}

\begin{exercise}
Suppose the joint limit of $\{ x_{n,m} \}$ is $L$, and suppose
that for all $n$, $\lim\limits_{m \to \infty} x_{n,m}$ exists,
and for all $m$, $\lim\limits_{n \to \infty} x_{n,m}$ exists.
Then show
$\lim\limits_{n\to\infty}\lim\limits_{m \to \infty} x_{n,m}
=
\lim\limits_{m\to\infty}\lim\limits_{n \to \infty} x_{n,m} = L$.
\end{exercise}

\begin{exercise}
A joint limit does not mean the iterated limits even exist.
Consider $x_{n,m} := \frac{{(-1)}^{n+m}}{\min \{n,m \}}$.
\begin{enumerate}[a)]
 \item Show that for no $n$ does
$\lim\limits_{m \to \infty} x_{n,m}$ exist, and for no $m$
does 
$\lim\limits_{n \to \infty} x_{n,m}$ exist.
So neither
$\lim\limits_{n\to\infty}\lim\limits_{m \to \infty} x_{n,m}$ nor
$\lim\limits_{m\to\infty}\lim\limits_{n \to \infty} x_{n,m}$ makes any sense
at all.
 \item Show that the joint limit of $\{ x_{n,m} \}$ exists and is 0.
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Picard's theorem}
\label{sec:picard}

\sectionnotes{1--2 lectures (can be safely skipped)}

A first semester course in analysis should have
a \emph{pi\`ece de r\'esistance} caliber
theorem.
We pick a theorem whose proof combines everything we have
learned.
It is more sophisticated than the fundamental theorem of calculus,
the first highlight theorem of this course.
The
theorem we are talking about is Picard's
theorem%
\footnote{Named for the French mathematician
\href{http://en.wikipedia.org/wiki/\%C3\%89mile_Picard}{Charles \'Emile Picard}
(1856--1941).}
on existence and uniqueness of a solution to an ordinary differential equation.
Both the statement and the proof are beautiful examples of what one can do
with all we have learned.
It is also a good example of how analysis is
applied as differential equations are indispensable in science.

\subsection*{First order ordinary differential equation}

Modern science is described in the language of
\emph{differential equations}\index{differential equation}.
That is, equations involving not only the unknown, but also its
derivatives.
The simplest nontrivial form of a differential equation is
the so-called \emph{\myindex{first order ordinary differential equation}}
\begin{equation*}
y' = F(x,y) .
\end{equation*}
Generally we also specify $y(x_0)=y_0$.
The solution of
the equation is a function $y(x)$ such that 
$y(x_0)=y_0$ and $y'(x) = F\bigl(x,y(x)\bigr)$.

When $F$ involves only the $x$ variable, the solution is given by the
fundamental theorem of calculus.
On the other hand, when $F$ depends
on both $x$ and $y$ we need far more firepower.
It is not always
true that a solution exists, and if it does, that it is the unique solution.
Picard's theorem gives us certain sufficient conditions for existence
and uniqueness.

\subsection*{The theorem}

We need a definition of continuity in two variables.
First, a point in the
plane $\R^2 = \R \times \R$ is denoted by an ordered pair $(x,y)$.
To make matters simple, let
us give the following sequential definition of continuity.

\begin{defn}
Let $U \subset \R^2$ be a set and $F \colon U \to \R$ be a function.
Let $(x,y) \in U$ be a point.
The function $F$ is \emph{continuous}
\index{continuous function of two variables}
at $(x,y)$ if 
for every sequence
$\{ (x_n,y_n) \}_{n=1}^\infty$ of points in $U$ such that
$\lim\, x_n = x$ and 
$\lim\, y_n = y$, we have 
\begin{equation*}
\lim_{n \to \infty} F(x_n,y_n) = 
F(x,y) .
\end{equation*}
We say $F$ is continuous if it is continuous at all points in $U$.
\end{defn}

\begin{thm}[Picard's theorem on existence and uniqueness]%
\index{existence and uniqueness theorem}\index{Picard's theorem}
Let $I, J \subset \R$ be closed bounded intervals, 
let $I_0$ and $J_0$ be their interiors, and
let $(x_0,y_0) \in I_0 \times J_0$.
Suppose $F \colon I \times J \to \R$ is continuous
and Lipschitz in the second variable, that is, there exists a number $L$
such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Then there exists an $h > 0$ and a unique differentiable
function $f \colon [x_0 - h, x_0 + h] \to J \subset \R$, such that
\begin{equation} \label{picard:diffeq}
f'(x) = F\bigl(x,f(x)\bigr) \qquad \text{and} \qquad f(x_0) = y_0.
\end{equation}
\end{thm}

\begin{proof}
Suppose we could find a solution $f$.
Using the fundamental
theorem of calculus we integrate the equation 
$f'(x) = F\bigl(x,f(x)\bigr)$, $f(x_0) = y_0$, and write \eqref{picard:diffeq}
as the integral equation
\begin{equation} \label{picard:inteq}
f(x) = y_0 + \int_{x_0}^x F\bigl(t,f(t)\bigr)~dt .
\end{equation}
The idea of our proof is that we try to plug in approximations to
a solution to the right-hand side of \eqref{picard:inteq} to get better approximations on the left
hand side of  \eqref{picard:inteq}.
We hope that in the end the sequence 
converges and solves
\eqref{picard:inteq} and hence \eqref{picard:diffeq}.
The technique below is called \emph{\myindex{Picard iteration}},
and the individual functions $f_k$ are called the 
\emph{Picard iterates}\index{Picard iterate}.

Without loss of generality, suppose $x_0 = 0$ (exercise below).
Another
exercise tells us that $F$ is bounded as it is continuous.
Therefore pick some $M > 0$ so that 
%Let $M := \sup \{  \abs{F(x,y)} : (x,y) \in I\times J \}$.
$\abs{F(x,y)} \leq M$ for all $(x,y) \in I\times J$.
%Without loss of generality, we can assume $M > 0$ (why?).
Pick $\alpha > 0$ such that
$[-\alpha,\alpha] \subset I$ and $[y_0-\alpha, y_0 + \alpha] \subset J$.
Define
\begin{equation*}
h := \min \left\{ \alpha, \frac{\alpha}{M+L\alpha} \right\} .
\end{equation*}
Observe
$[-h,h] \subset I$.

Set $f_0(x) := y_0$.
We define $f_k$ inductively.
Assuming $f_{k-1}([-h,h]) \subset [y_0-\alpha,y_0+\alpha]$,
we see 
$F\bigl(t,f_{k-1}(t)\bigr)$ is
a well defined function of $t$ for $t \in [-h,h]$.
Further if $f_{k-1}$ is continuous
on $[-h,h]$, then
$F\bigl(t,f_{k-1}(t)\bigr)$ is
continuous as
a function of $t$ on $[-h,h]$ (left as an exercise).
Define
\begin{equation*}
f_k(x) := y_0+ \int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt ,
\end{equation*}
and $f_k$ is continuous on $[-h,h]$ by the fundamental theorem of calculus.
To see that $f_k$ maps $[-h,h]$ to $[y_0-\alpha,y_0+\alpha]$, we compute for
$x \in [-h,h]$
\begin{equation*}
\abs{f_k(x) - y_0} = 
\abs{\int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt }
\leq
M\abs{x}
\leq
Mh
\leq
M
\frac{\alpha}{M+L\alpha}
\leq \alpha .
\end{equation*}
We now define $f_{k+1}$ and so on, and
we have defined a sequence $\{ f_k \}$ of functions.
We need
to show that it converges to a function $f$ that solves
the equation \eqref{picard:inteq} and therefore \eqref{picard:diffeq}.

We wish to show that the sequence $\{ f_k \}$ converges uniformly
to some function on $[-h,h]$.
First, for $t \in [-h,h]$
we have the following
useful bound
\begin{equation*}
\abs{F\bigl(t,f_{n}(t)\bigr) - 
F\bigl(t,f_{k}(t)\bigr)}
\leq
L \abs{f_n(t)-f_k(t)}
\leq
L \norm{f_n-f_k}_u ,
\end{equation*}
where $\norm{f_n-f_k}_u$ is the uniform norm, that
is the supremum of $\abs{f_n(t)-f_k(t)}$ for $t \in [-h,h]$.
Now note that $\abs{x} \leq h \leq \frac{\alpha}{M+L\alpha}$.
Therefore
\begin{equation*}
\begin{split}
\abs{f_n(x) - f_k(x)}
& =
\abs{\int_{0}^x F\bigl(t,f_{n-1}(t)\bigr)~dt 
-
\int_{0}^x F\bigl(t,f_{k-1}(t)\bigr)~dt}
\\
& =
\abs{\int_{0}^x F\bigl(t,f_{n-1}(t)\bigr)-
F\bigl(t,f_{k-1}(t)\bigr)~dt}
\\
& \leq
L\norm{f_{n-1}-f_{k-1}}_u
\abs{x}
\\
& \leq
\frac{L\alpha}{M+L\alpha}
\norm{f_{n-1}-f_{k-1}}_u .
\end{split}
\end{equation*}
Let $C := \frac{L\alpha}{M+L\alpha}$ and note that $C < 1$.
Taking supremum on the left-hand side we get
\begin{equation*}
\norm{f_n-f_k}_u \leq C \norm{f_{n-1}-f_{k-1}}_u .
\end{equation*}
Without loss of generality,
suppose $n \geq k$.
Then by \hyperref[induction:thm]{induction} we can show 
\begin{equation*}
\norm{f_n-f_k}_u \leq C^{k} \norm{f_{n-k}-f_{0}}_u .
\end{equation*}
For $x \in [-h,h]$ we have
\begin{equation*}
\abs{f_{n-k}(x)-f_{0}(x)}
=
\abs{f_{n-k}(x)-y_0}
\leq \alpha .
\end{equation*}
Therefore,
\begin{equation*}
\norm{f_n-f_k}_u \leq C^{k} \norm{f_{n-k}-f_{0}}_u \leq C^{k} \alpha .
\end{equation*}
As $C < 1$, $\{f_n\}$ is uniformly Cauchy and by
\propref{prop:uniformcauchy} we obtain that $\{ f_n \}$
converges uniformly on $[-h,h]$ to some function $f \colon [-h,h] \to \R$.
The function $f$ is the uniform limit of continuous functions and therefore
continuous.
Furthremore since all the $f_n([-h,h]) \subset
[y_0-\alpha,y_0+\alpha]$, then $f([-h,h]) \subset [y_0-\alpha,y_0+\alpha]$
(why?).


We now need to show that $f$ solves \eqref{picard:inteq}.
First, as before we notice
\begin{equation*}
\abs{F\bigl(t,f_{n}(t)\bigr) - 
F\bigl(t,f(t)\bigr)}
\leq
L \abs{f_n(t)-f(t)}
\leq
L \norm{f_n-f}_u .
\end{equation*}
As $\norm{f_n-f}_u$ converges to 0, then
$F\bigl(t,f_n(t)\bigr)$ converges uniformly to $F\bigl(t,f(t)\bigr)$
for $t \in [-h,h]$.
Hence, for $x \in [-h,h]$
the convergence is uniform %(why?)
for $t \in [0,x]$ (or $[x,0]$ if $x < 0$).
Therefore,
\begin{align*}
y_0
+
\int_0^{x}
F(t,f(t)\bigr)~dt
& =
y_0
+
\int_0^{x}
F\bigl(t,\lim_{n\to\infty} f_n(t)\bigr)~dt
& &
\\
& =
y_0
+
\int_0^{x}
\lim_{n\to\infty} F\bigl(t,f_n(t)\bigr)~dt
& & \text{(by continuity of $F$)}
\\
& =
\lim_{n\to\infty} 
\left(
y_0
+
\int_0^{x}
F\bigl(t,f_n(t)\bigr)~dt
\right)
& & \text{(by uniform convergence)}
\\
& =
\lim_{n\to\infty} 
f_{n+1}(x)
=
f(x) .
& &
\end{align*}
We apply the fundamental theorem of calculus to show that
$f$ is differentiable and its derivative is $F\bigl(x,f(x)\bigr)$.
It is obvious
that $f(0) = y_0$.

Finally, what is left to do is to show uniqueness.
Suppose $g \colon [-h,h]
\to J \subset \R$ is another solution.
As before we use the fact that
$\abs{F\bigl(t,f(t)\bigr) - F\bigl(t,g(t)\bigr)} \leq L \norm{f-g}_u$.
Then
\begin{equation*}
\begin{split}
\abs{f(x)-g(x)}
& =
\abs{
y_0
+
\int_0^{x}
F\bigl(t,f(t)\bigr)~dt
-
\left(
y_0
+
\int_0^{x}
F\bigl(t,g(t)\bigr)~dt
\right)
}
\\
& =
\abs{
\int_0^{x}
F\bigl(t,f(t)\bigr)
-
F\bigl(t,g(t)\bigr)~dt
}
\\
& \leq
L\norm{f-g}_u\abs{x}
\leq
Lh\norm{f-g}_u
\leq
\frac{L\alpha}{M+L\alpha}\norm{f-g}_u .
\end{split}
\end{equation*}
As 
before, $C = \frac{L\alpha}{M+L\alpha} < 1$.
By taking supremum over $x \in
[-h,h]$ on the left
hand side we obtain
\begin{equation*}
\norm{f-g}_u \leq C \norm{f-g}_u .
\end{equation*}
This is only possible if $\norm{f-g}_u = 0$.
Therefore, $f=g$, and the
solution is unique.
\end{proof}

\subsection*{Examples}

Let us look at some examples.
The proof of the theorem 
gives us an explicit way to find an $h$ that works.
It does not, however, give
us the best $h$.
It is often possible to find a much larger $h$ for
which the conclusion of the theorem holds.

The proof also gives us the Picard iterates as approximations to the
solution.
So the proof actually tells us how to obtain
the solution, not just that the solution exists.

\begin{example}
Consider
\begin{equation*}
f'(x) = f(x), \qquad f(0) = 1 .
\end{equation*}
That is, we let $F(x,y) = y$, and we are looking for a function 
$f$ such that $f'(x) = f(x)$.
We pick any $I$ that contains 0
in the interior.
We pick an arbitrary $J$ that contains 1 in its interior.
We can
use $L = 1$.
The theorem guarantees an $h > 0$ such that
there exists a unique solution $f \colon [-h,h] \to \R$.
This solution
is usually denoted by
\begin{equation*}
e^x := f(x) .
\end{equation*}
We leave it to the reader to verify that by picking $I$ and $J$
large enough the proof of the theorem guarantees that
we are able to pick $\alpha$ such that we get any
$h$ we want as long as $h < \nicefrac{1}{2}$.
We omit the calculation.

Of course, we know %(though we have not proved)
this function exists
as a function for all $x$, so an arbitrary $h$ ought to work.
By same reasoning as above,
no matter what $x_0$ and $y_0$ are,
the proof guarantees an arbitrary $h$ as long as $h < \nicefrac{1}{2}$.
Fix such an $h$.
We get a unique function defined on $[x_0-h,x_0+h]$.
After defining the
function on $[-h,h]$ we find a solution on the interval $[0,2h]$
and notice that the two functions must coincide on $[0,h]$ by uniqueness.
We thus iteratively construct the exponential for all $x \in \R$.
Therefore Picard's theorem could be used to prove the existence and uniqueness
of the exponential.

Let us compute the Picard iterates.
We start with the constant function $f_0(x) := 1$.
Then
\begin{align*}
f_1(x) & = 1 + \int_0^x f_0(s)~ds =
1+x, \\
f_2(x) & = 1 + \int_0^x f_1(s)~ds =
1 + \int_0^x (1+s)~ds = 1 + x + \frac{x^2}{2}, \\
f_3(x) & = 1 + \int_0^x f_2(s)~ds =
1 + \int_0^x \left(1+ s + \frac{s^2}{2} \right)~ds =
1 + x + \frac{x^2}{2} + \frac{x^3}{6} .
\end{align*}
We recognize the beginning of the Taylor series for the exponential.
\end{example}

\begin{example}
Suppose we have the equation
\begin{equation*}
f'(x) = {\bigl(f(x)\bigr)}^2 \qquad \text{and} \qquad f(0)=1.
\end{equation*}
From elementary differential equations we know 
\begin{equation*}
f(x) = \frac{1}{1-x}
\end{equation*}
is the solution.
The solution is only defined on $(-\infty,1)$.
That is,
we are able to use $h < 1$, but never a larger $h$.
The function that takes $y$ to $y^2$ is
not Lipschitz as a function on all of $\R$.
As we approach $x=1$ from the left, the solution becomes larger
and larger.
The derivative of the solution grows as $y^2$, and therefore
the $L$ required will have to be larger and larger as $y_0$ grows.
Thus if we apply the
theorem with $x_0$ close to 1 and $y_0 = \frac{1}{1-x_0}$ we find
that the $h$ that the proof guarantees will be smaller and smaller as $x_0$
approaches 1.

By picking $\alpha$ correctly, the proof of the theorem guarantees
$h=1-\nicefrac{\sqrt{3}}{2} \approx 0.134$ (we omit the
calculation) for $x_0=0$ and $y_0=1$, even though
we saw above that any $h < 1$ should work.
\end{example}

\begin{example}
Consider the equation
\begin{equation*}
f'(x) = 2 \sqrt{\abs{f(x)}}, \qquad f(0) = 0 .
\end{equation*}
The function $F(x,y) = 2 \sqrt{\abs{y}}$ is continuous,
but not Lipschitz in $y$ (why?). 
The equation does not satisfy the hypotheses of the theorem.
The function
\begin{equation*}
f(x) =
\begin{cases}
x^2 & \text{ if $x \geq 0$,}\\
-x^2 & \text{ if $x < 0$,}
\end{cases}
\end{equation*}
is a solution, but $f(x) = 0$ is also a solution.
A solution exists, but is not unique.
\end{example}

\begin{example}
Consider $y' = \varphi(x)$ where $\varphi(x) := 0$ if $x \in \Q$ and
$\varphi(x):=1$ if $x
\not\in \Q$.
The equation has no solution regardless of the initial
conditions.
A solution would have
derivative $\varphi$, but $\varphi$ does not have the intermediate value property
at any point (why?).
No solution exists by
\hyperref[thm:darboux]{Darboux's theorem}.
Therefore to obtain existence of a solution, some continuity hypothesis on
$F$ is necessary.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Let $I, J \subset \R$ be intervals.
Let $F \colon I \times J \to \R$ be a continuous function
of two variables
and suppose $f \colon I \to J$ be a continuous function.
Show that $F\bigl(x,f(x)\bigr)$ is a continuous function on $I$.
\end{exercise}

\begin{exercise}
Let $I, J \subset \R$ be closed bounded intervals.
Show that if $F \colon I \times J \to \R$ is continuous,
then $F$ is bounded.
\end{exercise}

\begin{exercise}
We proved Picard's theorem under the assumption that $x_0 = 0$.
Prove the full statement of Picard's theorem for an arbitrary $x_0$.
\end{exercise}

\begin{exercise}
Let $f'(x)=x f(x)$ be our equation.
Start with the initial condition
$f(0)=2$ and find the Picard iterates $f_0,f_1,f_2,f_3,f_4$.
\end{exercise}

\begin{exercise}
Suppose $F \colon I \times J \to \R$
is a function that is continuous in the first variable,
that is, for any fixed $y$ the function that takes $x$ to $F(x,y)$ is
continuous.
Further, suppose $F$ is Lipschitz in the second variable,
that is, there exists a number $L$ such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Show that $F$ is continuous as a function of two variables.
Therefore, the
hypotheses in the theorem could be made even weaker.
\end{exercise}

\begin{exercise}
A common type of equation one encounters are
\emph{\myindex{linear first order differential equations}}, that is
equations of the form
\begin{equation*}
y' + p(x) y = q(x) , \qquad y(x_0) = y_0 .
\end{equation*}
Prove Picard's theorem for linear equations.
Suppose $I$ is an
interval, $x_0 \in I$, and $p \colon I \to \R$ and $q \colon I \to \R$ are
continuous.
Show that there exists a unique differentiable $f \colon I \to \R$,
such that $y = f(x)$
satisfies the equation and the initial condition.
Hint: Assume existence of the exponential function and use the integrating
factor formula for existence of $f$ (prove that it works):
\begin{equation*}
f(x) := e^{-\int_{x_0}^x p(s)\, ds} \left( \int_{x_0}^x e^{\int_{x_0}^t p(s)\, ds}
q(t) ~dt + y_0 \right).
\end{equation*}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metric Spaces} \label{ms:chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Metric spaces}
\label{sec:metric}

\sectionnotes{1.5 lectures}

As mentioned in the introduction, the main idea in analysis is to take
limits.
In \chapterref{seq:chapter} we learned to take limits of sequences of
real numbers.
And in \chapterref{lim:chapter} we learned to take limits
of functions as a real number approached some other real number.

We want to take limits in more complicated contexts.
For
example, we want to have sequences of points in 3-dimensional space.
We wish to define continuous functions of several variables.
We even want to define functions on spaces that are a little harder to
describe, such as the surface of the earth.
We still want to talk about
limits there.

Finally, we have seen the limit of a sequence of
functions in \chapterref{fs:chapter}.
We wish to unify all these notions so that we do not have to
reprove theorems over and over again in each context.
The concept of a
metric space is an elementary yet powerful tool in analysis.
And while it
is not sufficient to describe every type of limit we find in modern
analysis, it gets us very far indeed.

\begin{defn}
Let $X$ be a set, and let
$d \colon X \times X \to \R$
be a function such that
\begin{enumerate}[(i)]
\item \label{metric:pos} $d(x,y) \geq 0$ for all $x, y$ in $X$,
\item \label{metric:zero} $d(x,y) = 0$ if and only if $x = y$,
\item \label{metric:com} $d(x,y) = d(y,x)$, 
\item \label{metric:triang} $d(x,z) \leq d(x,y)+ d(y,z)$ \qquad (\emph{\myindex{triangle inequality}}).
\end{enumerate}
Then the pair $(X,d)$ is called a \emph{\myindex{metric space}}.
The
function $d$ is called the \emph{\myindex{metric}} or sometimes the
\emph{\myindex{distance function}}.
Sometimes we just say $X$ is a metric space if the metric is clear from
context.
\end{defn}

The geometric idea is that $d$ is the distance between two points. 
Items \ref{metric:pos}--\ref{metric:com} have obvious geometric
interpretation: distance is always nonnegative, the only point that is
distance 0 away from $x$ is $x$ itself, and finally that the distance from
$x$ to $y$ is the same as the distance from $y$ to $x$.
The triangle
inequality \ref{metric:triang} has the interpretation given in
\figureref{fig:mstriang}.
\begin{figure}[h!t]
\begin{center}
\input ms-triang.pdf_t
\caption{Diagram of the triangle inequality in metric spaces.\label{fig:mstriang}}
\end{center}
\end{figure}

For the purposes of drawing, it is convenient to draw figures and
diagrams in the plane and have the metric be the standard distance.
However, that is only one particular metric space.
Just because a
certain fact seems to be clear from drawing a picture does not mean it is
true.
You might be getting sidetracked by intuition from euclidean
geometry,
whereas the concept of a metric space is a lot more general.

Let us give some examples of metric spaces.

\begin{example}
The set of real numbers $\R$ is a metric space with the metric
\begin{equation*}
d(x,y) := \abs{x-y} .
\end{equation*}
Items \ref{metric:pos}--\ref{metric:com} of the definition
are easy to verify.
The
triangle inequality \ref{metric:triang} follows immediately
from the standard triangle inequality for real numbers:
\begin{equation*}
d(x,z) = \abs{x-z} = 
\abs{x-y+y-z} \leq
\abs{x-y}+\abs{y-z} =
d(x,y)+ d(y,z) .
\end{equation*}
This metric is the \emph{\myindex{standard metric on $\R$}}.
If we talk
about $\R$ as a metric space without mentioning a specific metric, we 
mean this particular metric.
\end{example}

\begin{example}
We can also put a different metric on the set of real numbers.
For example, take the set of real numbers $\R$ together with the metric
\begin{equation*}
d(x,y) :=
\frac{\abs{x-y}}{\abs{x-y}+1} .
\end{equation*}
Items \ref{metric:pos}--\ref{metric:com} are again easy to verify.
The
triangle inequality \ref{metric:triang} is a little bit more difficult.
Note that $d(x,y) = \varphi(\abs{x-y})$ where $\varphi(t) =
\frac{t}{t+1}$ and $\varphi$ is an increasing function
(positive derivative).
Hence
\begin{equation*}
\begin{split}
d(x,z) & = \varphi(\abs{x-z}) = 
\varphi(\abs{x-y+y-z}) \leq
\varphi(\abs{x-y}+\abs{y-z})
\\
& =
\frac{\abs{x-y}+\abs{y-z}}{\abs{x-y}+\abs{y-z}+1} =
\frac{\abs{x-y}}{\abs{x-y}+\abs{y-z}+1} +
\frac{\abs{y-z}}{\abs{x-y}+\abs{y-z}+1}
\\
& \leq
\frac{\abs{x-y}}{\abs{x-y}+1} +
\frac{\abs{y-z}}{\abs{y-z}+1} =
d(x,y)+ d(y,z) .
\end{split}
\end{equation*}
Here we have an example of a nonstandard metric on $\R$.
With this metric
we see for example that $d(x,y) < 1$ for all $x,y \in \R$.
That is,
any two points are less than 1 unit apart.
\end{example}

An important metric space is the
$n$-dimensional \emph{\myindex{euclidean space}} $\R^n = \R
\times \R \times \cdots \times \R$.
 We use the following
notation for points: $x =(x_1,x_2,\ldots,x_n) \in \R^n$.
We also
simply write $0 \in \R^n$ to mean the vector $(0,0,\ldots,0)$.
Before
making $\R^n$ a metric space, let us prove an important inequality first discovered by Bunyakovsky but usually called Cauchy--Schwarz inequality.

\begin{lemma}[\myindex{Cauchy--Schwarz inequality}]
Take $x =(x_1,x_2,\ldots,x_n) \in \R^n$ and $y =(y_1,y_2,\ldots,y_n) \in
\R^n$.
Then
\begin{equation*}
{\biggl( \sum_{j=1}^n x_j y_j \biggr)}^2
\leq
\biggl(\sum_{j=1}^n x_j^2 \biggr)
\biggl(\sum_{j=1}^n y_j^2 \biggr) .
\end{equation*}
\end{lemma}

\begin{proof}
Any square of a real number is nonnegative.
Hence any sum of squares is
nonnegative:
\begin{equation*}
\begin{split}
0 & \leq 
\sum_{j=1}^n \sum_{k=1}^n {(x_j y_k - x_k y_j)}^2
\\
& =
\sum_{j=1}^n \sum_{k=1}^n \bigl( x_j^2 y_k^2 + x_k^2 y_j^2 - 2 x_j x_k y_j
y_k \bigr)
\\
& =
\biggl( \sum_{j=1}^n x_j^2 \biggr)
\biggl( \sum_{k=1}^n y_k^2 \biggr)
+
\biggl( \sum_{j=1}^n y_j^2 \biggr)
\biggl( \sum_{k=1}^n x_k^2 \biggr)
-
2
\biggl( \sum_{j=1}^n x_j y_j \biggr)
\biggl( \sum_{k=1}^n x_k y_k \biggr)
\end{split}
\end{equation*}
We relabel and divide by 2 to obtain
\begin{equation*}
0 \leq 
\biggl( \sum_{j=1}^n x_j^2 \biggr)
\biggl( \sum_{j=1}^n y_j^2 \biggr)
-
{\biggl( \sum_{j=1}^n x_j y_j \biggr)}^2 ,
\end{equation*}
which is precisely what we wanted.
\end{proof}

\begin{example}
Let us construct the
standard metric\index{standard metric on $\R^n$} for $\R^n$.
Define
\begin{equation*}
d(x,y) :=
\sqrt{
{(x_1-y_1)}^2 + 
{(x_2-y_2)}^2 + 
\cdots +
{(x_n-y_n)}^2
} =
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2 
} .
\end{equation*}
For $n=1$, the real line, this metric agrees with what we did above.
Again,
the only tricky part of the definition to check is the triangle inequality.
It is less messy to work with the square of the metric.
In the
following, note the use of the Cauchy--Schwarz inequality.
\begin{equation*}
\begin{split}
{\bigl(d(x,z)\bigr)}^2 & =
\sum_{j=1}^n
{(x_j-z_j)}^2 
\\
& =
\sum_{j=1}^n
{(x_j-y_j+y_j-z_j)}^2 
\\
& =
\sum_{j=1}^n
\Bigl(
{(x_j-y_j)}^2+{(y_j-z_j)}^2 + 2(x_j-y_j)(y_j-z_j)
\Bigr)
\\
& =
\sum_{j=1}^n
{(x_j-y_j)}^2
+
\sum_{j=1}^n
{(y_j-z_j)}^2 
+
\sum_{j=1}^n
 2(x_j-y_j)(y_j-z_j)
\\
& \leq
\sum_{j=1}^n
{(x_j-y_j)}^2
+
\sum_{j=1}^n
{(y_j-z_j)}^2 
+
2
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2
\sum_{j=1}^n
{(y_j-z_j)}^2
}
\\
& =
{\left(
\sqrt{
\sum_{j=1}^n
{(x_j-y_j)}^2
}
+
\sqrt{
\sum_{j=1}^n
{(y_j-z_j)}^2 
}
\right)}^2
=
{\bigl( d(x,y) + d(y,z) \bigr)}^2 .
\end{split}
\end{equation*}
Taking the square root of both sides we obtain the correct inequality,
because the square root is an increasing function.
\end{example}

\begin{example}
An example to keep in mind is the so-called \emph{\myindex{discrete
metric}}.
Let $X$ be any set and define
\begin{equation*}
d(x,y) :=
\begin{cases}
1 & \text{if $x \not= y$}, \\
0 & \text{if $x = y$}.
\end{cases}
\end{equation*}
That is, all points are equally distant from each other.
When $X$ is a
finite set, we can draw a diagram, see for example
\figureref{fig:msdiscmetric}.
Things become subtle when $X$ is an infinite set such
as the real numbers.
\begin{figure}[h!t]
\begin{center}
\input msdiscmetric.pdf_t
\caption{Sample discrete metric space $\{ a,b,c,d,e \}$, the distance
between any two points is $1$.\label{fig:msdiscmetric}}
\end{center}
\end{figure}

While this particular
example seldom comes up in practice, it gives a useful 
``smell test.''  If you make a statement about metric spaces,
try it with the discrete metric.
To show that $(X,d)$ is indeed a metric space is left as an exercise.
\end{example}

\begin{example} \label{example:msC01}
Let $C([a,b],\R)$ be the set of continuous real-valued functions on the
interval $[a,b]$.
Define the metric on $C([a,b],\R)$ as
\begin{equation*}
d(f,g) := \sup_{x \in [a,b]} \abs{f(x)-g(x)} .
\end{equation*}
Let us check the properties.
First, $d(f,g)$ is finite as
$\abs{f(x)-g(x)}$ is a continuous function on a closed bounded interval
$[a,b]$, and so is bounded.
It is clear that $d(f,g) \geq 0$, 
it is the supremum of nonnegative numbers.
If $f = g$
then $\abs{f(x)-g(x)} = 0$ for all $x$ and hence $d(f,g) = 0$.
Conversely
if $d(f,g) = 0$, then for any $x$ we have $\abs{f(x)-g(x)} \leq d(f,g) = 0$
and hence $f(x) = g(x)$ for all $x$ and $f=g$.
That $d(f,g) = d(g,f)$
is equally trivial.
To show the triangle inequality we use the standard
triangle inequality.
\begin{equation*}
\begin{split}
d(f,g) & =
\sup_{x \in [a,b]} \abs{f(x)-g(x)} =
\sup_{x \in [a,b]} \abs{f(x)-h(x)+h(x)-g(x)}
\\
& \leq
\sup_{x \in [a,b]} ( \abs{f(x)-h(x)}+\abs{h(x)-g(x)} )
\\
& \leq
\sup_{x \in [a,b]} \abs{f(x)-h(x)}+
\sup_{x \in [a,b]} \abs{h(x)-g(x)} = d(f,h) + d(h,g) .
\end{split}
\end{equation*}
When treating $C([a,b],\R)$ as a metric space without mentioning a metric, we mean this
particular metric.
Notice that $d(f,g) = \norm{f-g}_u$, the uniform norm of \defnref{def:unifnorm}.

This example may seem esoteric at first, but it turns out that working with
spaces such as $C([a,b],\R)$ is really the meat of a large part of modern 
analysis.
Treating sets of functions as metric spaces allows us to
abstract away a lot of the grubby detail and prove powerful results such as
Picard's theorem with less work.
\end{example}

Oftentimes it is useful to consider a subset of a larger metric space
as a metric space itself.
We obtain the following proposition, which has
a trivial proof.

\begin{prop}
Let $(X,d)$ be a metric space and $Y \subset X$, then the restriction
$d|_{Y \times Y}$ is a metric on $Y$.
\end{prop}

\begin{defn}
If $(X,d)$ is a metric space, $Y \subset X$, and $d' := d|_{Y \times Y}$,
then $(Y,d')$ is said to be a \emph{\myindex{subspace}} of $(X,d)$.
\end{defn}

It is common to simply write $d$ for the metric on $Y$, as it is 
the restriction of the metric on $X$.
Sometimes we say $d'$ is
the \emph{\myindex{subspace metric}} and $Y$ has the
\emph{\myindex{subspace topology}}.

\medskip

A subset of the real
numbers is bounded whenever all its elements are at most some fixed distance
from 0.
We also define bounded sets in a metric space.
When dealing with an arbitrary metric space there may not be some
natural fixed point 0.
For the purposes of boundedness it does not matter.

\begin{defn}
Let $(X,d)$ be a metric space.
A subset $S \subset X$ is said to be
\emph{bounded}\index{bounded set} if there exists a $p \in X$ and a
$B \in \R$ such that
\begin{equation*}
d(p,x) \leq B \quad \text{for all $x \in S$}.
\end{equation*}
We say $(X,d)$ is bounded if $X$ itself is a bounded subset.
\end{defn}

For example, the set of real numbers with the standard metric is not a
bounded metric space.
It is not hard to see that a
subset of the real numbers is bounded in the
sense of \chapterref{rn:chapter} if and only if it is bounded as a subset of the
metric space of real numbers with the standard metric.

On the other hand, if we take the real numbers with the discrete metric,
then we obtain a bounded metric space.
In fact, any set with the
discrete metric is bounded.

\subsection*{Exercises}

\begin{exercise}
Show that for any set $X$, the discrete metric ($d(x,y) = 1$ if $x\not=y$ and
$d(x,x) = 0$) does give a metric space $(X,d)$.
\end{exercise}

\begin{exercise}
Let $X := \{ 0 \}$ be a set.
Can you make it into a metric space?
\end{exercise}

\begin{exercise}
Let $X := \{ a, b \}$ be a set.
Can you make it into two distinct metric
spaces?  (define two distinct metrics on it)
\end{exercise}

\begin{exercise}
Let the set $X := \{ A, B, C \}$ represent 3 buildings on campus.
Suppose we
wish our distance to be the time it takes to walk from one building to
the other.
It takes 5 minutes either way between buildings $A$ and $B$.
However,
building $C$ is on a hill and it takes 10 minutes from $A$ and 15 minutes
from $B$ to get to $C$.
On the other hand it takes 5 minutes to go
from $C$ to $A$ and 7 minutes to go from $C$ to $B$, as we are going
downhill.
Do these distances define a metric?
  If so, prove it, if not, say
why not.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is a metric space and
$\varphi \colon [0,\infty) \to \R$ is
%an increasing
a function such that 
$\varphi(t) \geq 0$ for all $t$ and $\varphi(t) = 0$ if and only if
$t=0$.
Also suppose $\varphi$ is \emph{\myindex{subadditive}},
that is, $\varphi(s+t) \leq \varphi(s)+\varphi(t)$.
Show that with $d'(x,y) := \varphi\bigl(d(x,y)\bigr)$, we obtain a new
metric space $(X,d')$.
\end{exercise}

\begin{exercise} \label{exercise:mscross}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
\begin{enumerate}[a)]
 \item Show that $(X \times Y,d)$ with
$d\bigl( (x_1,y_1), (x_2,y_2) \bigr) := d_X(x_1,x_2) + d_Y(y_1,y_2)$ is a metric space.
\item Show that $(X \times Y,d)$ with
$d\bigl( (x_1,y_1), (x_2,y_2) \bigr) := \max \{ d_X(x_1,x_2) , d_Y(y_1,y_2) \}$ is
a metric space.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $X$ be the set of continuous functions on $[0,1]$.
Let $\varphi \colon
[0,1] \to (0,\infty)$ be continuous.
Define
\begin{equation*}
d(f,g) := \int_0^1 \abs{f(x)-g(x)}\varphi(x)~dx .
\end{equation*}
Show that $(X,d)$ is a metric space.
\end{exercise}

\pagebreak[2]

\begin{exercise} \label{exercise:mshausdorffpseudo}
Let $(X,d)$ be a metric space.
For nonempty bounded subsets $A$ and $B$ let
\begin{equation*}
d(x,B) := \inf \{ d(x,b) : b \in B \}
\qquad \text{and} \qquad
d(A,B) := \sup \{ d(a,B) : a \in A \} .
\end{equation*}
Now define the \emph{\myindex{Hausdorff metric}} as
\begin{equation*}
d_H(A,B) := \max \{ d(A,B) , d(B,A) \} .
\end{equation*}
Note: $d_H$ can be defined for arbitrary nonempty subsets if we allow the
extended reals.
\begin{enumerate}[a)]
 \item Let $Y \subset \sP(X)$ be the set of bounded nonempty subsets.
Prove that
$(Y,d_H)$ is a so-called \emph{\myindex{pseudometric space}}:
$d_H$ satisfies the metric properties
\ref{metric:pos},
\ref{metric:com}, 
\ref{metric:triang}, and further
$d_H(A,A) = 0$ for all $A \in Y$. 
  \item Show by example that $d$ itself is not symmetric, that is $d(A,B) \not=
d(B,A)$.
   \item Find a metric space $X$ and two different
nonempty bounded subsets $A$ and $B$ such that $d_H(A,B) = 0$.
\end{enumerate}
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Open and closed sets}
\label{sec:mettop}

\sectionnotes{2 lectures}

\subsection*{Topology}

It is useful to define a so-called \emph{\myindex{topology}}.
That is
we define closed and open sets in a metric space.
Before doing so,
let us define two special sets.

\begin{defn}
Let $(X,d)$ be a metric space, $x \in X$ and $\delta > 0$.
Then define
the \emph{\myindex{open ball}} or simply \emph{\myindex{ball}} of radius $\delta$
around $x$ as
\begin{equation*}
B(x,\delta) := \{ y \in X : d(x,y) < \delta \} .
\end{equation*}
Similarly we define the \emph{\myindex{closed ball}} as
\begin{equation*}
C(x,\delta) := \{ y \in X : d(x,y) \leq \delta \} .
\end{equation*}
\end{defn}

When we are dealing with different metric spaces, it is sometimes 
convenient to emphasize which metric space the ball is in.
We do this by
writing $B_X(x,\delta) := B(x,\delta)$ or $C_X(x,\delta) := C(x,\delta)$.

\begin{example}
Take the metric space $\R$ with the standard metric.
For
$x \in \R$, and $\delta > 0$ we get 
\begin{equation*}
B(x,\delta) = (x-\delta,x+\delta) \qquad \text{and} \qquad
C(x,\delta) = [x-\delta,x+\delta] .
\end{equation*}
\end{example}

\begin{example}
Be careful when working on a subspace.
Suppose we take the
metric space $[0,1]$ as a subspace of $\R$.
Then in $[0,1]$
we get
\begin{equation*}
B(0,\nicefrac{1}{2}) = B_{[0,1]}(0,\nicefrac{1}{2}) = [0,\nicefrac{1}{2}) .
\end{equation*}
This is different from $B_{\R}(0,\nicefrac{1}{2}) =
(-\nicefrac{1}{2},\nicefrac{1}{2})$.
The important thing to keep in mind is which metric space we are working
in.
\end{example}

\begin{defn}
Let $(X,d)$ be a metric space.
A set $V \subset X$
is \emph{open}\index{open set}
if for every $x \in V$, there exists a $\delta > 0$ such that
$B(x,\delta) \subset V$.
See \figureref{fig:msopenset}.
A set $E \subset X$ is 
\emph{closed}\index{closed set} if the complement $E^c = X \setminus E$ is open.
When the ambient space $X$ is not clear from context we say
$V$ is open in $X$ and $E$ is closed in $X$.

If $x \in V$ and $V$ is open, then we say 
$V$ is an \emph{\myindex{open neighborhood}} of $x$ (or
sometimes just \emph{\myindex{neighborhood}}).
\end{defn}

\begin{figure}[h!t]
\begin{center}
\input msopenset.pdf_t
\caption{Open set in a metric space.
Note that $\delta$ depends on $x$.\label{fig:msopenset}}
\end{center}
\end{figure}

Intuitively, an open set is a set that does not include its ``boundary,''
wherever we are at in the set, we are allowed to ``wiggle'' a little bit and
stay in the set.
Note that not every set is either open or closed, in fact generally
most subsets are neither.

\begin{example}
The set $[0,1) \subset \R$ is neither open nor closed.
First,
every ball in $\R$ around $0$, $(-\delta,\delta)$, contains negative
numbers and hence is not contained in $[0,1)$ and so $[0,1)$ is not open.
Second, every ball in $\R$ around $1$, $(1-\delta,1+\delta)$ contains
numbers strictly less than 1 and greater than 0
(e.g.\ $1-\nicefrac{\delta}{2}$ as long as $\delta < 2$).
Thus $\R \setminus
[0,1)$ is not open, and so $[0,1)$ is not closed.
\end{example}

\begin{prop} \label{prop:topology:open}
Let $(X,d)$ be a metric space.
\begin{enumerate}[(i)]
\item \label{topology:openi} $\emptyset$ and $X$ are open in $X$.
\item \label{topology:openii} If $V_1, V_2, \ldots, V_k$ are open then
\begin{equation*}
\bigcap_{j=1}^k V_j
\end{equation*}
is also open.
That is, finite intersection of open sets is open.
\item \label{topology:openiii} If $\{ V_\lambda \}_{\lambda \in I}$ is
an arbitrary collection of open sets, then
\begin{equation*}
\bigcup_{\lambda \in I} V_\lambda
\end{equation*}
is also open.
That is, union of open sets is open.
\end{enumerate}
\end{prop}

Note that the index set in \ref{topology:openiii} is arbitrarily large.
By $\bigcup_{\lambda \in I} V_\lambda$ we simply mean the set of
all $x$ such that $x \in V_\lambda$ for at least one $\lambda \in I$.

\begin{proof}
The sets $X$ and $\emptyset$ are obviously open in $X$.

Let us prove \ref{topology:openii}.
If $x \in \bigcap_{j=1}^k V_j$, then $x \in V_j$ for all $j$.
As $V_j$ are all open, for every $j$ there exists a $\delta_j > 0$ 
such that $B(x,\delta_j) \subset V_j$.
Take $\delta := \min \{
\delta_1,\delta_2,\ldots,\delta_k \}$ and notice $\delta > 0$.
We have
$B(x,\delta) \subset B(x,\delta_j) \subset V_j$ for every $j$ and so
$B(x,\delta) \subset \bigcap_{j=1}^k V_j$.
Consequently the intersection is open.

Let us prove \ref{topology:openiii}.
If $x \in \bigcup_{\lambda \in I} V_\lambda$, then $x \in V_\lambda$ for some
$\lambda \in I$.
As $V_\lambda$ is open, there exists a $\delta > 0$
such that $B(x,\delta) \subset V_\lambda$.
But then
$B(x,\delta) \subset \bigcup_{\lambda \in I} V_\lambda$
and so the union is open.
\end{proof}

\begin{example}
The main thing to notice is the difference between
items
\ref{topology:openii} and \ref{topology:openiii}.
Item \ref{topology:openii} is not true for an arbitrary intersection,
for example $\bigcap_{n=1}^\infty (-\nicefrac{1}{n},\nicefrac{1}{n}) = \{ 0
\}$, which is not open.
\end{example}


The proof of the following analogous proposition for closed sets
is left as an exercise.

\begin{prop} \label{prop:topology:closed}
Let $(X,d)$ be a metric space.
\begin{enumerate}[(i)]
\item \label{topology:closedi} $\emptyset$ and $X$ are closed in $X$.
\item \label{topology:closedii} If $\{ E_\lambda \}_{\lambda \in I}$ is
an arbitrary collection of closed sets, then
\begin{equation*}
\bigcap_{\lambda \in I} E_\lambda
\end{equation*}
is also closed.
That is, intersection of closed sets is closed.
\item \label{topology:closediii} If $E_1, E_2, \ldots, E_k$ are closed then
\begin{equation*}
\bigcup_{j=1}^k E_j
\end{equation*}
is also closed.
That is, finite union of closed sets is closed.
\end{enumerate}
\end{prop}

We have not yet shown that the open ball is open and the closed ball is
closed.
Let us show this fact now to justify the terminology.

\begin{prop} \label{prop:topology:ballsopenclosed}
Let $(X,d)$ be a metric space, $x \in X$, and $\delta > 0$.
Then
$B(x,\delta)$ is open and 
$C(x,\delta)$ is closed.
\end{prop}

\begin{proof}
Let $y \in B(x,\delta)$.
Let $\alpha := \delta-d(x,y)$.
Of course $\alpha
> 0$.
Now let $z \in B(y,\alpha)$.
Then
\begin{equation*}
d(x,z) \leq d(x,y) + d(y,z) < d(x,y) + \alpha = d(x,y) + \delta-d(x,y) =
\delta .
\end{equation*}
Therefore $z \in B(x,\delta)$ for every $z \in B(y,\alpha)$.
So $B(y,\alpha) \subset B(x,\delta)$ and
$B(x,\delta)$ is open.

The proof that $C(x,\delta)$ is closed is left as an exercise.
\end{proof}

Again be careful about what is the ambient metric space.
As $[0,\nicefrac{1}{2})$ is
an open ball in $[0,1]$, this means that $[0,\nicefrac{1}{2})$ is
an open set in $[0,1]$.
On the other hand $[0,\nicefrac{1}{2})$
is neither open nor closed in $\R$.

A useful way to think about an open set is as a union of open balls.
If $U$ is
open, then for each $x \in U$, there is a $\delta_x > 0$ (depending on $x$) such that
$B(x,\delta_x) \subset U$.
Then $U = \bigcup_{x\in U} B(x,\delta_x)$.

The proof of the following proposition is left as an exercise.
Note that
there are many other open and
closed sets in $\R$.

\begin{prop} \label{prop:topology:intervals:openclosed}
Let $a < b$ be two real numbers.
Then $(a,b)$, $(a,\infty)$,
and $(-\infty,b)$ are open in $\R$.
Also $[a,b]$, $[a,\infty)$,
and $(-\infty,b]$ are closed in $\R$.
\end{prop}


\subsection*{Connected sets}

\begin{defn}
A nonempty
metric space $(X,d)$ is \emph{\myindex{connected}} if the
only subsets of $X$ that are both open and closed are $\emptyset$ and $X$ itself.
If $(X,d)$ is not connected we say it is
\emph{\myindex{disconnected}}.

When we apply the term \emph{connected} to a nonempty subset $A \subset X$, we simply
mean that $A$ with the subspace topology is connected.
\end{defn}

In other words, a nonempty $X$ is connected if whenever we write
$X = X_1 \cup X_2$ where $X_1 \cap X_2 = \emptyset$ and $X_1$ and $X_2$ are
open, then either $X_1 = \emptyset$ or $X_2 = \emptyset$.
So to show $X$ is disconnected, we need to find nonempty
disjoint open sets $X_1$ and
$X_2$ whose union is $X$.
For subsets, we state this idea as a proposition.

\begin{prop}
Let $(X,d)$ be a metric space.
A nonempty set $S \subset X$ is not connected if and only if
there exist open sets $U_1$ and
$U_2$ in $X$, such that $U_1 \cap U_2 \cap S = \emptyset$,
$U_1 \cap S \not= \emptyset$,
$U_2 \cap S \not= \emptyset$, and
\begin{equation*}
S = 
\bigl( U_1 \cap S \bigr)
\cup
\bigl( U_2 \cap S \bigr) .
\end{equation*}
\end{prop}

\begin{proof}
If $U_j$ is open in $X$,
then $U_j \cap S$ is open in $S$ in the subspace topology (with subspace
metric).
To see this,
note that if $B_X(x,\delta) \subset U_j$, then as
$B_S(x,\delta) = S \cap B_X(x,\delta)$,
we have $B_S(x,\delta) \subset U_j \cap S$.
So if $U_1$ and $U_2$ as above exist, then
$X$ is disconnected based on the discussion above.

The proof of the other direction follows by using
\exerciseref{exercise:mssubspace} to find $U_1$ and $U_2$ from two
open disjoint subsets of $S$.
\end{proof}

\begin{example}
Let $S \subset \R$ be such that $x < z < y$ with $x,y \in S$
and $z \notin S$.
Claim: $S$ is not connected.
\begin{proof}  Notice
\begin{equation*}
\bigl( (-\infty,z) \cap S \bigr)
\cup
\bigl( (z,\infty) \cap S \bigr)
= S .
\end{equation*}
\end{proof}
\end{example}

\begin{prop}
A nonempty set $S \subset \R$ is connected if and only if it is
an interval or a single point.
\end{prop}

\begin{proof}
Suppose $S$ is connected.
If $S$ is a single point
then we are done.
So suppose $x < y$ and $x,y \in S$.
If $z$ is such
that $x < z < y$, then $(-\infty,z) \cap S$ is nonempty and $(z,\infty) \cap
S$ is nonempty.
The two sets are disjoint.
As
$S$ is connected, we must have they their union is not $S$, so $z \in S$.

Suppose $S$ is bounded, connected, but not a single point.
Let $\alpha := \inf \, S$ and
$\beta := \sup \, S$ and note that $\alpha < \beta$.
Suppose $\alpha < z < \beta$.
As $\alpha$ is the
infimum, then there is an $x \in S$ such that $\alpha \leq x < z$.
Similarly
there is a $y \in S$ such that $\beta \geq y > z$. 
We have shown above that $z \in S$, so $(\alpha,\beta) \subset S$.
If $w < \alpha$, then $w \notin S$
as $\alpha$ was the infimum,
similarly if $w > \beta$ then $w \notin S$.
Therefore the only
possibilities for $S$ are
$(\alpha,\beta)$,
$[\alpha,\beta)$,
$(\alpha,\beta]$,
$[\alpha,\beta]$.

The proof that an unbounded connected $S$ is an interval is left as an exercise.

On the other hand suppose $S$ is an interval.
Suppose $U_1$ and $U_2$ are open subsets of $\R$,
$U_1 \cap S$ and $U_2 \cap S$ are nonempty, and
$S = 
\bigl( U_1 \cap S \bigr)
\cup
\bigl( U_2 \cap S \bigr)$.
We will show that $U_1 \cap S$
and $U_2 \cap S$ contain a common point, so they are not disjoint,
and hence $S$ must be connected.
Suppose there is $x \in U_1 \cap S$
and $y \in U_2 \cap S$.
Without loss of generality, assume $x < y$.
As $S$ is an interval
$[x,y] \subset S$.
Let $z := \inf (U_2 \cap [x,y])$.
If $z = x$, then $z
\in U_1$.
If $z > x$,
then for any $\delta > 0$ the 
ball $B(z,\delta) =
(z-\delta,z+\delta)$ contains points of $S$ that are not
in $U_2$, and so $z \notin U_2$ as $U_2$ is open.
Therefore, $z \in U_1$.
As $U_1$ is open, $B(z,\delta) \subset U_1$ for a small enough $\delta >
0$.
As $z$ is the infimum of $U_2 \cap [x,y]$, 
there must exist some $w \in U_2 \cap [x,y]$
such that $w \in [z,z+\delta) \subset B(z,\delta) \subset U_1$.
Therefore $w \in U_1 \cap U_2 \cap [x,y]$.
So $U_1 \cap S$ and $U_2 \cap S$ are not disjoint and hence $S$ is connected.
\end{proof}

\begin{example}
In many cases a ball $B(x,\delta)$ is connected.
But this is not
necessarily true in every metric space.
For a simplest example, take a two point space $\{ a,
b\}$ with the discrete metric.
Then $B(a,2) = \{ a , b \}$, which is not
connected as $B(a,1) = \{ a \}$ and 
$B(b,1) = \{ b \}$ are open and disjoint.
\end{example}

\subsection*{Closure and boundary}

Sometimes we wish to take a set and throw in everything that we can approach
from the set.
This concept is called the closure.

\begin{defn}
Let $(X,d)$ be a metric space and $A \subset X$.
Then
the \emph{\myindex{closure}} of $A$ is the set
\begin{equation*}
\overline{A} := \bigcap \{ E \subset X : \text{$E$ is closed and $A \subset
E$} \} .
\end{equation*}
That is, $\overline{A}$ is the intersection of all closed sets that contain
$A$.
\end{defn}

\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.
The closure $\overline{A}$
is closed.
Furthermore if $A$ is closed then $\overline{A} = A$.
\end{prop}

\begin{proof}
First, the closure is the intersection of closed sets, so it is closed.
Second, if $A$ is closed, then take $E = A$, hence the intersection of all
closed sets $E$ containing $A$ must be equal to $A$.
\end{proof}

\begin{example}
The closure of $(0,1)$ in $\R$ is $[0,1]$.
\begin{proof}  Simply notice that if
$E$ is closed and contains $(0,1)$, then $E$ must contain $0$ and $1$ (why?).
Thus $[0,1] \subset E$.
But $[0,1]$ is also closed.
Therefore the closure $\overline{(0,1)} = [0,1]$.
\end{proof}
\end{example}

\begin{example}
Be careful to notice what ambient metric space you are working with.
If $X = (0,\infty)$, then
the closure of $(0,1)$ in $(0,\infty)$ is $(0,1]$.
\begin{proof}  Similarly as
above $(0,1]$ is closed in $(0,\infty)$ (why?).
Any closed set $E$
that contains $(0,1)$ must contain 1 (why?).
Therefore $(0,1] \subset E$,
and hence $\overline{(0,1)} = (0,1]$ when working in $(0,\infty)$.
\end{proof}
\end{example}

Let us justify the statement that the closure is everything that we can
``approach'' from the set.

\begin{prop} \label{prop:msclosureappr}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $x \in \overline{A}$
if and only if for every $\delta > 0$, $B(x,\delta) \cap A \not=\emptyset$.
\end{prop}

\begin{proof}
Let us prove the two contrapositives.
Let us show that $x \notin \overline{A}$ if and only if there exists
a $\delta > 0$ such that $B(x,\delta) \cap A = \emptyset$.

First suppose $x \notin \overline{A}$.
We know $\overline{A}$ is
closed.
Thus there is a $\delta > 0$ such that
$B(x,\delta) \subset \overline{A}^c$.
As $A \subset \overline{A}$ we
see that $B(x,\delta) \subset A^c$ and hence
$B(x,\delta) \cap A = \emptyset$.

On the other hand suppose there is a $\delta > 0$ such that
$B(x,\delta) \cap A = \emptyset$.
Then ${B(x,\delta)}^c$ is a closed set and we have
that $A \subset {B(x,\delta)}^c$, but
$x \notin {B(x,\delta)}^c$.
Thus as $\overline{A}$ is the intersection
of closed sets containing $A$, we have $x \notin \overline{A}$.
\end{proof}

We can also talk about what is in the interior of a set and what is on the
boundary.

\begin{defn}
Let $(X,d)$ be a metric space and $A \subset X$, then
the \emph{\myindex{interior}} of $A$ is the set
\begin{equation*}
A^\circ := \{ x \in A : \text{there exists a $\delta > 0$ such that
$B(x,\delta) \subset A$} \} .
\end{equation*}
The \emph{\myindex{boundary}} of $A$ is the set
\begin{equation*}
\partial A := \overline{A}\setminus A^\circ.
\end{equation*}
\end{defn}

\begin{example}
Suppose $A=(0,1]$ and $X = \R$.
Then it is not hard
to see that $\overline{A}=[0,1]$, $A^\circ = (0,1)$,
and $\partial A = \{ 0, 1 \}$.
\end{example}

\begin{example}
Suppose $X = \{ a, b \}$ with the discrete metric.
Let $A = \{ a \}$, then $\overline{A} = A^\circ = A$ and $\partial A =
\emptyset$.
\end{example}


\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $A^\circ$ is open
and $\partial A$ is closed.
\end{prop}

\begin{proof}
Given $x \in A^\circ$ we have $\delta > 0$ such that $B(x,\delta)
\subset A$.
If $z \in B(x,\delta)$, then as open balls are open,
there is an $\epsilon > 0$ such that $B(z,\epsilon) \subset B(x,\delta)
\subset A$, so $z$ is in $A^\circ$.
Therefore $B(x,\delta) \subset
A^\circ$ and so $A^\circ$ is open.

As $A^\circ$ is open, then
$\partial A = \overline{A} \setminus A^\circ = \overline{A} \cap
{(A^\circ)}^c$ is closed.
\end{proof}

The boundary is the set of points that are close to both the set and its
complement.

\begin{prop}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $x \in \partial A$
if and only if for every $\delta > 0$,
$B(x,\delta) \cap A$ and
$B(x,\delta) \cap A^c$ are both nonempty.
\end{prop}

\begin{proof}
Suppose $x \in \partial A =  \overline{A} \setminus A^\circ$ and
let $\delta > 0$ be arbitrary.
By \propref{prop:msclosureappr}, $B(x,\delta)$ contains
a point from $A$.
If $B(x,\delta)$ contained no points of $A^c$,
then $x$ would be in $A^\circ$.
Hence $B(x,\delta)$ contains a point of
$A^c$ as well.

Let us prove the other direction by contrapositive.
If $x \notin \overline{A}$, then there is some $\delta > 0$ such that
$B(x,\delta) \subset \overline{A}^c$ as $\overline{A}$ is closed.
So $B(x,\delta)$ contains no points of $A$, because $\overline{A}^c \subset
A^c$.

Now suppose $x \in A^\circ$, then there exists a $\delta > 0$
such that $B(x,\delta) \subset A$, but that means $B(x,\delta)$
contains no points of $A^c$.
\end{proof}

We obtain the following immediate corollary about closures of $A$ and $A^c$.
We
simply apply \propref{prop:msclosureappr}.

\begin{cor}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $\partial A = \overline{A} \cap \overline{A^c}$.
\end{cor}

\subsection*{Exercises}

\begin{exercise}
Prove \propref{prop:topology:closed}.
Hint: consider the complements of the
sets and apply \propref{prop:topology:open}.
\end{exercise}

\begin{exercise}
Finish the proof of \propref{prop:topology:ballsopenclosed} by
proving that $C(x,\delta)$ is closed.
\end{exercise}

\begin{exercise}
Prove \propref{prop:topology:intervals:openclosed}.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is a nonempty metric space with the discrete topology.
Show
that $X$ is connected if and only if it contains exactly one element.
\end{exercise}

\begin{exercise}
Show that if $S \subset \R$ is a connected unbounded set, then it is an
(unbounded) interval.
\end{exercise}

\begin{exercise}
Show that every open set can be written as a union of closed sets.
\end{exercise}

\begin{exercise}
a) Show that $E$ is closed if and only if $\partial E \subset E$.
b) Show that $U$ is open if and only if $\partial U \cap U = \emptyset$.
\end{exercise}

\begin{exercise}
a) Show that $A$ is open if and only if $A^\circ = A$.
b) Suppose that $U$ is an open set and $U \subset A$.
Show
that $U \subset A^\circ$.
\end{exercise}

\begin{exercise}
Let $X$ be a set and $d$, $d'$ be two metrics on $X$.
Suppose there exists an $\alpha > 0$ and $\beta > 0$
such that $\alpha d(x,y) \leq d'(x,y) \leq \beta d(x,y)$ for all $x,y \in X$.
Show that $U$ is open in $(X,d)$ if and only if $U$ is open in $(X,d')$.
That is, the topologies of $(X,d)$ and $(X,d')$ are the same.
\end{exercise}


\begin{exercise}
Suppose $\{ S_i \}$, $i \in \N$
is a collection of connected subsets of a metric space $(X,d)$.
Suppose
there exists an $x \in X$ such that $x \in S_i$ for all $i \in \N$.
Show that $\bigcup_{i=1}^\infty S_i$ is connected.
\end{exercise}

\begin{exercise}
Let $A$ be a connected set.
a) \nolinebreak Is $\overline{A}$ connected?
  Prove or find a counterexample.
b) \nolinebreak Is $A^\circ$ connected?
  Prove or find a counterexample.
Hint: Think of sets in $\R^2$.
\end{exercise}

\begin{exnote}
The definition of open sets in the following exercise is usually called the
\emph{\myindex{subspace topology}}.
You are asked to show that
we obtain the same topology by considering the subspace metric.
\end{exnote}

\begin{exercise} \label{exercise:mssubspace}
Suppose $(X,d)$ is a metric space and $Y \subset X$.
Show that
with the subspace metric on $Y$, a set $U \subset Y$
is open (in $Y$) whenever there exists an open set $V \subset X$ such
that $U = V \cap Y$.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.
a) For any $x \in X$ and $\delta > 0$, show
$\overline{B(x,\delta)} \subset C(x,\delta)$.
b) Is it always true that
$\overline{B(x,\delta)} = C(x,\delta)$?
  Prove or find a counterexample.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space and $A \subset X$.
Show that
$A^\circ = \bigcup \{ V : V \subset A \text{ is open} \}$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Sequences and convergence}
\label{sec:metseqs}

\sectionnotes{1 lecture}

\subsection*{Sequences}

The notion of a sequence in a metric space is very similar to a sequence of
real numbers.
The definitions are essentially the same as those for real numbers
in the sense of \chapterref{seq:chapter} where $\R$ with
the standard metric $d(x,y)=\abs{x-y}$ is replaced
by an arbitrary metric space $(X,d)$.


\begin{defn}
A \emph{\myindex{sequence}} in a metric space $(X,d)$ is a function
$x \colon \N \to X$.
As before we write $x_n$ for the $n$th element in
the sequence and use the notation $\{ x_n \}$, or more precisely
\begin{equation*}
\{ x_n \}_{n=1}^\infty .
\end{equation*}

A sequence $\{ x_n \}$ is \emph{bounded}\index{bounded sequence} if
there exists a point $p \in X$ and $B \in \R$ such that
\begin{equation*}
d(p,x_n) \leq B \qquad \text{for all $n \in \N$.}
\end{equation*}
In other words, the sequence $\{x_n\}$ is bounded whenever
the set $\{ x_n : n \in \N \}$
is bounded.

If $\{ n_j \}_{j=1}^\infty$ is a sequence of natural numbers
such that $n_{j+1} > n_j$ for all $j$, then
the sequence $\{ x_{n_j} \}_{j=1}^\infty$ is said to be
a \emph{\myindex{subsequence}} of $\{x_n \}$.
\end{defn}

Similarly we also define convergence.
Again, we will be cheating a little
bit and we will use the definite article in front of the word \emph{limit}
before we prove that the limit is unique.

\begin{defn}
A sequence $\{ x_n \}$ in a metric space $(X,d)$ is said
to \emph{\myindex{converge}} to a point
$p \in X$, if for every $\epsilon > 0$, there exists an $M \in \N$ such
that $d(x_n,p) < \epsilon$ for all $n \geq M$.
The point $p$
is said to be the \emph{limit}\index{limit of a sequence}
of $\{ x_n \}$.
We write
\begin{equation*}
\lim_{n\to \infty} x_n := p .
\end{equation*}

A sequence
that converges is said to be \emph{convergent}\index{convergent sequence}.
Otherwise, the sequence is said to be
\emph{divergent}\index{divergent sequence}.
\end{defn}

Let us prove that the limit is unique.
Note that the proof is almost
identical to the proof of the same fact for sequences of real numbers.
Many results we know for sequences of real numbers can be proved in
the more general settings of metric spaces.
We must replace $\abs{x-y}$
with $d(x,y)$ in the proofs and apply the triangle inequality correctly.

\begin{prop} \label{prop:mslimisunique}
A convergent sequence in a metric space has a unique limit.
\end{prop}

\begin{proof}
Suppose the sequence $\{ x_n \}$ has the limit $x$ and the limit $y$.
Take an arbitrary $\epsilon > 0$.
From the definition find an $M_1$ such that for all $n \geq M_1$,
$d(x_n,x) < \nicefrac{\epsilon}{2}$.
Similarly find an $M_2$
such that for all $n \geq M_2$ we have
$d(x_n,y) < \nicefrac{\epsilon}{2}$.
Now take an $n$ such that
$n \geq M_1$ and also $n \geq M_2$
\begin{equation*}
\begin{split}
d(y,x)
& \leq
d(y,x_n) + d(x_n,x) \\
& <
\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon .
\end{split}
\end{equation*}
As $d(y,x) < \epsilon$ for all $\epsilon > 0$, then $d(x,y) = 0$
and $y=x$.
Hence the limit (if it exists) is unique.
\end{proof}

The proofs of the following propositions are left as exercises.

\begin{prop} \label{prop:msconvbound}
A convergent sequence in a metric space is bounded.
\end{prop}

\begin{prop} \label{prop:msconvifa}
A sequence $\{ x_n \}$ in a metric space $(X,d)$ converges to $p \in X$
if and only
if there exists a sequence $\{ a_n \}$ of real numbers such that
\begin{equation*}
d(x_n,p) \leq a_n \quad \text{for all $n \in \N$},
\end{equation*}
and
\begin{equation*}
\lim_{n\to\infty} a_n = 0.
\end{equation*}
\end{prop}

\begin{prop} \label{prop:mssubseq}
Let $\{ x_n \}$ be a sequence in a metric space $(X,d)$.
\begin{enumerate}[(i)]
\item If $\{ x_n \}$ converges to $p \in X$, then every subsequence $\{ x_{n_k} \}$
converges to $p$.
\item If for some $K \in \N$ the $K$-tail $\{ x_n \}_{n=K+1}^\infty$
converges to $p \in X$, then
 $\{ x_n \}$ converges to $p$.
\end{enumerate}
\end{prop}

\subsection*{Convergence in euclidean space}

It is useful to note what convergence means in the euclidean space
$\R^n$.

\begin{prop} \label{prop:msconveuc}
Let $\{ x_j \}_{j=1}^\infty$ be a sequence in $\R^n$,
where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
Then $\{ x_j \}_{j=1}^\infty$ converges if and only if
$\{ x_{j,k} \}_{j=1}^\infty$ converges for every $k$, in which case
\begin{equation*}
\lim_{j\to\infty}
x_j =
\Bigl(
\lim_{j\to\infty} x_{j,1},
\lim_{j\to\infty} x_{j,2},
\ldots,
\lim_{j\to\infty} x_{j,n}
\Bigr) .
\end{equation*}
\end{prop}

%For $\R = \R^1$ the result is immediate.
%So let $n > 1$.

\begin{proof}
Let $\{ x_j \}_{j=1}^\infty$ be a convergent sequence
in $\R^n$, where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
Let $y = (y_1,y_2,\ldots,y_n) \in \R^n$ be the limit.
Given $\epsilon > 0$, there exists an $M$ such that for all
$j \geq M$ we have
\begin{equation*}
d(y,x_j) < \epsilon.
\end{equation*}
Fix some $k=1,2,\ldots,n$.
For $j \geq M$ we have
\begin{equation*}
\bigl\lvert y_k - x_{j,k} \bigr\rvert
=
\sqrt{{\bigl(y_k - x_{j,k} \bigr)}^2}
\leq
\sqrt{\sum_{\ell=1}^n {\bigl(y_\ell-x_{j,\ell}\bigr)}^2}
= d(y,x_j) < \epsilon .
\end{equation*}
Hence the sequence $\{ x_{j,k} \}_{j=1}^\infty$ converges to $y_k$.

For the other direction suppose 
$\{ x_{j,k} \}_{j=1}^\infty$ converges to $y_k$ for every $k=1,2,\ldots,n$.
Hence, given $\epsilon > 0$, pick an $M$, such that if $j \geq M$ then 
$\bigl\lvert y_k-x_{j,k} \bigr\rvert < \nicefrac{\epsilon}{\sqrt{n}}$ for all
$k=1,2,\ldots,n$.
Then
\begin{equation*}
d(y,x_j)
=
\sqrt{\sum_{k=1}^n {\bigl(y_k-x_{j,k}\bigr)}^2}
<
\sqrt{\sum_{k=1}^n {\left(\frac{\epsilon}{\sqrt{n}}\right)}^2}
=
\sqrt{\sum_{k=1}^n \frac{{\epsilon^2}}{n}}
= \epsilon .
\end{equation*}
The sequence $\{ x_j \}$ converges to $y \in \R^n$ and we are done.
\end{proof}

\subsection*{Convergence and topology}

The topology, that is, the set of open sets of a space encodes which
sequences converge.

\begin{prop} \label{prop:msconvtopo}
Let $(X,d)$ be a metric space and $\{x_n\}$ a sequence in $X$.
Then
$\{ x_n \}$ converges to $x \in X$ if and only if for every open neighborhood
$U$ of $x$, there exists an $M \in \N$ such that for all $n \geq M$
we have $x_n \in U$.
\end{prop}

\begin{proof}
First suppose $\{ x_n \}$ converges.
Let $U$ be an open neighborhood
of $x$, then there exists an $\epsilon > 0$ such that $B(x,\epsilon) \subset
U$.
As the sequence converges, find an $M \in \N$ such that for all $n \geq
M$ we have $d(x,x_n) < \epsilon$ or in other words $x_n \in B(x,\epsilon)
\subset U$.

Let us prove the other direction.
Given $\epsilon > 0$ let $U :=
B(x,\epsilon)$ be the neighborhood of $x$.
Then there is an $M \in \N$
such that for $n \geq M$ we have $x_n \in U = B(x,\epsilon)$ or in other
words, $d(x,x_n) < \epsilon$.
\end{proof}

A set is closed when it contains the limits of its convergent sequences.

\begin{prop} \label{prop:msclosedlim}
Let $(X,d)$ be a metric space, $E \subset X$ a closed set
and $\{ x_n \}$ a sequence in $E$ that converges to some $x \in X$.
Then $x \in E$.
\end{prop}

\begin{proof}
Let us prove the contrapositive.
Suppose $\{ x_n \}$ is a sequence in $X$ that converges to $x \in E^c$.
As $E^c$ is open, \propref{prop:msconvtopo} says there is
an $M$ such that for all $n \geq M$,
$x_n \in E^c$.
So $\{ x_n \}$  is not a sequence in $E$.
%Let $\{ x_n \}$ be a sequence in $E$ that converges to some $x \in X$.
%Take $y \in E^c$, as $E^c$ is open then there exists a $\delta > 0$
%such that $B(y,\delta) \subset E^c$, or in particular $d(y,x_n) \geq \delta$
%for all $n$ as $x_n \in E$.  In particular, $\lim\, x_n = x \not= y$, and so
%$x \in E$.
\end{proof}

When we take a closure of a set $A$, we really throw in precisely 
those points that are limits of sequences in $A$.

\begin{prop} \label{prop:msclosureapprseq}
Let $(X,d)$ be a metric space and $A \subset X$.
Then $x \in \overline{A}$ if and only if there exists a sequence $\{ x_n \}$ of
elements in $A$ such that $\lim\, x_n = x$.
\end{prop}

\begin{proof}
Let $x \in \overline{A}$.
We know by
\propref{prop:msclosureappr} that given $\nicefrac{1}{n}$, there
exists a point $x_n \in B(x,\nicefrac{1}{n}) \cap A$.
As $d(x,x_n) < \nicefrac{1}{n}$, we have $\lim\, x_n = x$.

For the other direction, see \exerciseref{exercise:reverseclosedseq}.
%Exercise
%Now suppose
%$\{x_n\}$ a sequence in $A$ convergent to some $x \in X$.
%Take any closed $E$ such that $A \subset E$.  The sequence is
%in $E$ and so by
%\propref{prop:msclosedlim}, $x \in E$.  As $E$ was arbitrary, $x \in
%\overline{A}$.
\end{proof}

\subsection*{Exercises}

\begin{exercise} \label{exercise:reverseclosedseq}
Let $(X,d)$ be a metric space and
let $A \subset X$.
Let $E$ be the set of all $x \in X$ such that there
exists a sequence $\{ x_n \}$ in $A$ that converges to $x$.
Show 
$E = \overline{A}$.
\end{exercise}

\begin{exercise}
a) Show that $d(x,y) := \min \{ 1, \abs{x-y} \}$ defines a metric on $\R$.
b) Show that a sequence converges in $(\R,d)$ if and only if it converges
in the standard metric.
c) Find a bounded sequence in $(\R,d)$ that
contains no convergent subsequence.
\end{exercise}

\begin{exercise}
Prove \propref{prop:msconvbound}.
\end{exercise}

\begin{exercise}
Prove \propref{prop:msconvifa}.
\end{exercise}

\begin{exercise}
Suppose $\{x_n\}_{n=1}^\infty$ converges to $x$.
Suppose $f \colon \N
\to \N$ is a one-to-one function.
Show that
$\{ x_{f(n)} \}_{n=1}^\infty$ converges to $x$.
\end{exercise}

\begin{exercise}
If $(X,d)$ is a metric space where $d$ is the discrete metric.
Suppose 
$\{ x_n \}$ is a convergent sequence in $X$.
Show that there exists
a $K \in \N$ such that for all $n \geq K$ we have $x_n = x_K$.
\end{exercise}

\begin{exercise}
A set $S \subset X$ is said to be dense in $X$ if for every $x \in X$,
there exists a sequence $\{ x_n \}$ in $S$ that converges to $x$.
Prove
that $\R^n$ contains a countable dense subset.
\end{exercise}

\begin{exercise}[Tricky]
Suppose $\{ U_n \}_{n=1}^\infty$ be a decreasing ($U_{n+1} \subset U_n$ for
all $n$) sequence of open sets in a metric space $(X,d)$ such that
$\bigcap_{n=1}^\infty U_n = \{ p \}$ for some $p \in X$.
Suppose 
$\{ x_n \}$ is a sequence of points in $X$ such that $x_n \in U_n$.
Does
$\{ x_n \}$ necessarily converge to $p$?
  Prove or construct a counterexample.
\end{exercise}

\begin{exercise}
Let $E \subset X$ be closed and
let $\{ x_n \}$ be a sequence in $X$ converging to $p \in X$.
Suppose
$x_n \in E$ for infinitely many $n \in \N$.
Show $p \in E$.
\end{exercise}

\begin{exercise}
Take $\R^* = \{ -\infty \} \cup \R \cup \{ \infty \}$ be the extended reals.
Define $d(x,y) := \bigl\lvert \frac{x}{1+\abs{x}} - \frac{y}{1+\abs{y}}
\bigr\rvert$
if $x, y \in \R$,
define $d(\infty,x) := \bigl\lvert 1 - \frac{x}{1+\abs{x}} \bigr\rvert$,
$d(-\infty,x) := \bigl\lvert 1 + \frac{x}{1+\abs{x}} \bigr\rvert$
for all $x \in \R$, and
let $d(\infty,-\infty) := 2$.
a)~Show that $(\R^*,d)$ is a metric space.
b)~Suppose $\{ x_n \}$ is a sequence of real numbers such that
for every $M \in \R$, there exists an $N$ such that
$x_n \geq M$ for all $n \geq N$.
Show that $\lim\, x_n = \infty$ in
$(\R^*,d)$.
c)~Show that a sequence of real numbers converges to a real number
in $(\R^*,d)$ if and
only if it converges in $\R$ with the standard metric.
\end{exercise}

\begin{exercise}
Suppose $\{ V_n \}_{n=1}^\infty$ is a collection of open sets
in $(X,d)$
such that $V_{n+1} \supset V_n$.
Let $\{ x_n \}$ be a sequence
such that $x_n \in V_{n+1} \setminus V_n$ and suppose 
$\{ x_n \}$ converges to $p \in X$.
Show that $p \in \partial V$
where $V = \bigcup_{n=1}^\infty V_n$.
\end{exercise}

\begin{exercise}
Prove \propref{prop:mssubseq}.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Completeness and compactness}
\label{sec:metcompact}

\sectionnotes{2 lectures}

\subsection*{Cauchy sequences and completeness}

Just like with sequences of real numbers we define Cauchy sequences.

\begin{defn}
Let $(X,d)$ be a metric space.
A sequence $\{ x_n \}$ in $X$ is a \emph{\myindex{Cauchy sequence}} if
for every $\epsilon > 0$ there exists an $M \in \N$ such that
for all $n \geq M$ and all $k \geq M$ we have
\begin{equation*}
d(x_n, x_k) < \epsilon .
\end{equation*}
\end{defn}

The definition is again simply a translation of the concept
from the real numbers to metric spaces.
So a sequence of real
numbers is Cauchy in the sense of \chapterref{seq:chapter} if and only if
it is Cauchy in the sense above, provided we equip the real numbers with
the standard metric $d(x,y) = \abs{x-y}$.

\begin{prop}
A convergent sequence in a metric space is Cauchy.
\end{prop}

\begin{proof}
Suppose $\{ x_n \}$ converges to $x$.
Given $\epsilon > 0$ there is an $M$ such that for $n \geq M$
we have $d(x,x_n) < \nicefrac{\epsilon}{2}$.
Hence
for all $n,k \geq M$ we have
$d(x_n,x_k) \leq d(x_n,x) + d(x,x_k) < \nicefrac{\epsilon}{2} +
\nicefrac{\epsilon}{2} = \epsilon$.
\end{proof}

\begin{defn}
Let $(X,d)$ be a metric space.
We say $X$ is
\emph{\myindex{complete}} or \emph{\myindex{Cauchy-complete}}
if every Cauchy sequence $\{ x_n \}$ in $X$
converges to an $x \in X$.
\end{defn}

\begin{prop}
The space $\R^n$ with the standard metric is a complete metric space.
\end{prop}

For $\R = \R^1$ completeness was proved in \chapterref{seq:chapter}.
The proof of
the above proposition is a reduction to the one dimensional case.

\begin{proof}
%Take $n > 1$.
Let $\{ x_j \}_{j=1}^\infty$ be a Cauchy sequence
in $\R^n$, where we write $x_j = \bigl(x_{j,1},x_{j,2},\ldots,x_{j,n}\bigr) \in \R^n$.
As the sequence is Cauchy, given $\epsilon > 0$, there exists an $M$ such that for all
$i,j \geq M$ we have
\begin{equation*}
d(x_i,x_j) < \epsilon.
\end{equation*}

Fix some $k=1,2,\ldots,n$, for $i,j \geq M$ we have
\begin{equation*}
\bigl\lvert x_{i,k} - x_{j,k} \bigr\rvert
=
\sqrt{{\bigl(x_{i,k} - x_{j,k}\bigr)}^2}
\leq
\sqrt{\sum_{\ell=1}^n {\bigl(x_{i,\ell}-x_{j,\ell}\bigr)}^2}
= d(x_i,x_j) < \epsilon .
\end{equation*}
Hence the sequence $\{ x_{j,k} \}_{j=1}^\infty$ is Cauchy.
As $\R$ is
complete the sequence converges; there exists an $y_k \in \R$ such that
$y_k = \lim_{j\to\infty} x_{j,k}$.

Write $y = (y_1,y_2,\ldots,y_n) \in \R^n$.
By \propref{prop:msconveuc} we have that $\{ x_j \}$ converges
to $y \in \R^n$ and hence $\R^n$ is complete.
\end{proof}

Note that a subset of $\R^n$ with the subspace metric need not be
complete.
For example, $(0,1]$ with the subspace metric is not
complete as $\{ \nicefrac{1}{n} \}$ is a Cauchy sequence in $(0,1]$
with no limit in $(0,1]$.
But see also
\exerciseref{exercise:closedcomplete}.

\subsection*{Compactness}

\begin{defn}
Let $(X,d)$ be a metric space and $K \subset X$. 
The set $K$ is said to be \emph{\myindex{compact}}
if for any collection
of open sets $\{ U_{\lambda} \}_{\lambda \in I}$ such that
\begin{equation*}
K \subset \bigcup_{\lambda \in I} U_\lambda ,
\end{equation*}
there exists a finite subset
$\{ \lambda_1, \lambda_2,\ldots,\lambda_k \} \subset I$
such that
\begin{equation*}
K \subset \bigcup_{j=1}^k U_{\lambda_j} .
\end{equation*}
\end{defn}

A collection of open sets $\{ U_{\lambda} \}_{\lambda \in I}$ as above is
said to be a \emph{\myindex{open cover}} of $K$.
So a way to say that
$K$ is compact is to say that \emph{every open cover of $K$ has a finite
\myindex{subcover}}.

\begin{prop}
Let $(X,d)$ be a metric space.
A compact set $K \subset X$ is closed and
bounded.
\end{prop}

\begin{proof}
First, we prove that a compact set is bounded.
Fix $p \in X$.
We have the open cover
\begin{equation*}
K \subset \bigcup_{n=1}^\infty B(p,n) = X .
\end{equation*}
If $K$ is compact, then there exists some set of indices
$n_1 < n_2 < \ldots < n_k$ such that
\begin{equation*}
K \subset \bigcup_{j=1}^k B(p,n_j) = B(p,n_k) .
\end{equation*}
As $K$ is contained in a ball, $K$ is bounded.

Next, we show a set that is not closed is not compact.
Suppose 
$\overline{K} \not= K$, that is, there is a point $x \in \overline{K}
\setminus K$.
If $y \not= x$, then for $n$
with $\nicefrac{1}{n} < d(x,y)$ we have
$y \notin C(x,\nicefrac{1}{n})$. Furthermore $x \notin K$, so
\begin{equation*}
K \subset \bigcup_{n=1}^\infty {C(x,\nicefrac{1}{n})}^c .
\end{equation*}
As a closed ball is closed, ${C(x,\nicefrac{1}{n})}^c$ is open, and
so we have an open cover.
If we take any
finite collection of indices $n_1 < n_2 < \ldots < n_k$, then 
\begin{equation*}
\bigcup_{j=1}^k {C(x,\nicefrac{1}{n_j})}^c 
=
{C(x,\nicefrac{1}{n_k})}^c 
\end{equation*}
As $x$ is in the closure,
$C(x,\nicefrac{1}{n_k}) \cap K \not= \emptyset$.
So there is no
finite subcover and $K$ is not compact.
\end{proof}

We prove below that 
in finite dimensional euclidean space
every closed bounded set is compact.
So closed bounded sets
of $\R^n$ are examples of compact sets.
It is not true that in every metric space, closed and bounded is equivalent
to compact.
A simple example would be an incomplete metric space such as
$(0,1)$ with the subspace metric.
But there are many complete and very useful metric spaces where closed and bounded is not
enough to give compactness, see
\exerciseref{exercise:msclbounnotcompt}: $C([a,b],\R)$ is a complete metric
space, but the closed unit ball $C(0,1)$ is not compact.
However, see
\exerciseref{exercise:mstotbound}.

A useful property of compact sets in a metric space is that every
sequence has a convergent subsequence.
Such sets are sometimes called
\emph{\myindex{sequentially compact}}.
Let us prove that in the
context of metric spaces, a set is compact if and only if it is sequentially
compact.
First we prove a lemma.

\begin{lemma}[Lebesgue covering lemma%
\footnote{Named after the French mathematician
\href{http://en.wikipedia.org/wiki/Lebesgue}{Henri L\'eon Lebesgue}
(1875 -- 1941).
The number $\delta$ is sometimes called the \myindex{Lebesgue number} of the
cover.}]\label{ms:lebesgue}
\index{Lebesgue covering lemma}
Let $(X,d)$ be a metric space and $K \subset X$.
Suppose 
every sequence in $K$ has a subsequence convergent in $K$.
Given
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$, there exists a
$\delta > 0$ such that for every $x \in K$, there exists a $\lambda \in I$
with $B(x,\delta) \subset U_\lambda$.
\end{lemma}

It is important to recognize what the lemma says.
It says that given any
cover there is a single $\delta > 0$.
The $\delta$ can depend on the cover,
but of course it does not depend on $x$.

\begin{proof}
Let us prove the lemma by contrapositive.
If the conclusion is not true, then
there is
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$ with
the following property.
For every $n \in \N$ there exists an $x_n \in K$ such that
$B(x_n,\nicefrac{1}{n})$ is not a subset of any $U_\lambda$.
Given any $x \in K$, there is
a $\lambda \in I$ such that $x \in U_\lambda$.
Hence there
is an $\epsilon > 0$ 
such that $B(x,\epsilon) \subset U_\lambda$.
Take $M$ such that
$\nicefrac{1}{M} < \nicefrac{\epsilon}{2}$.
If $y \in 
B(x,\nicefrac{\epsilon}{2})$ and $n \geq M$, then
by triangle inequality
\begin{equation*}
B(y,\nicefrac{1}{n}) \subset
B(y,\nicefrac{1}{M}) \subset B(y,\nicefrac{\epsilon}{2}) \subset
B(x,\epsilon) \subset U_\lambda .
\end{equation*}
In other words, for all $n \geq M$, $x_n \notin B(x,\nicefrac{\epsilon}{2})$. 
Hence the sequence cannot have a subsequence converging to $x$.
As $x \in K$ was
arbitrary we are done.
\end{proof}

\begin{thm} \label{thm:mscompactisseqcpt}
Let $(X,d)$ be a metric space.
Then $K \subset X$ is a compact set if
and only if every sequence in $K$ has a subsequence converging to
a point in $K$.
\end{thm}

\begin{proof}
Let $K \subset X$ be a set and
$\{ x_n \}$ a sequence in $K$.
Suppose that for each $x \in K$,
there is a ball $B(x,\alpha_x)$ for some $\alpha_x > 0$ such that
$x_n \in B(x,\alpha_x)$ for only finitely many $n \in \N$.
Then
\begin{equation*}
K \subset \bigcup_{x \in K} B(x,\alpha_x) .
\end{equation*}
Any finite collection of these balls is going to contain only finitely many
$x_n$.
Thus for any finite collection of such balls there is an $x_n \in K$
that is not in the union.
Therefore, $K$ is not compact.

So if $K$ is compact,
then there exists an $x \in K$ such that
for any $\delta > 0$,
$B(x,\delta)$ contains $x_k$ for infinitely many $k \in \N$.
The ball $B(x,1)$ contains some $x_k$ so let $n_1 := k$.
If $n_{j-1}$ is defined, then there must
exist a $k > n_{j-1}$ such that $x_k \in B(x,\nicefrac{1}{j})$, so define
$n_j := k$.
Notice that
$d(x,x_{n_j}) < \nicefrac{1}{j}$.
By \propref{prop:msconvifa},
$\lim\, x_{n_j} = x$.

For the other direction, suppose every sequence in $K$
has a 
subsequence converging in $K$.
Take
an open cover $\{ U_\lambda \}_{\lambda \in I}$ of $K$.
Using the Lebesgue covering lemma above, we find a $\delta > 0$
such that for every $x$, there is a $\lambda \in I$ with
$B(x,\delta) \subset U_\lambda$.

Pick $x_1 \in K$ and find $\lambda_1 \in I$ such that $B(x_1,\delta) \subset
U_{\lambda_1}$.
If $K \subset U_{\lambda_1}$, we stop as we have found a
finite subcover.
Otherwise, there must be
a point $x_2 \in K \setminus U_{\lambda_1}$.
Note that $d(x_2,x_1) \geq \delta$.
There must exist some $\lambda_2 \in I$ such that
$B(x_2,\delta) \subset U_{\lambda_2}$.
We work inductively.
Suppose $\lambda_{n-1}$ is defined.
Either
$U_{\lambda_1} \cup
U_{\lambda_2} \cup \cdots \cup
U_{\lambda_{n-1}}$ is a finite cover of $K$, in which case we
stop, or
there must be 
a point $x_n \in K \setminus \bigl( U_{\lambda_1} \cup
U_{\lambda_2} \cup \cdots \cup
U_{\lambda_{n-1}}\bigr)$.
Note that $d(x_n,x_j) \geq \delta$ for all $j = 1,2,\ldots,n-1$.
Next, there must be some $\lambda_n \in I$
such that $B(x_n,\delta) \subset U_{\lambda_n}$.

Either at some point we obtain a finite subcover of $K$
or we obtain an
infinite
sequence $\{ x_n \}$ as above.
For contradiction suppose that
there is no finite subcover and we have the sequence $\{ x_n \}$.
For all $n$ and $k$, $n \not= k$, 
we have $d(x_n,x_k) \geq \delta$,
so no subsequence of $\{ x_n \}$ can be
Cauchy.
Hence no subsequence of $\{ x_n \}$ can be convergent,
which is a contradiction.
\end{proof}

\begin{example}
The Bolzano--Weierstrass theorem for sequences of real numbers
(\thmref{thm:bwseq})
says that any bounded sequence in $\R$ has a convergent
subsequence.
Therefore any sequence in a closed interval $[a,b] \subset \R$ has 
a convergent subsequence.
The limit must also be in $[a,b]$ as limits
preserve non-strict inequalities.
Hence a closed bounded interval $[a,b]
\subset \R$ is compact.
\end{example}

\begin{prop}
Let $(X,d)$ be a metric space and let $K \subset X$ be compact.
If
$E \subset K$ is a closed set, then $E$ is compact.
\end{prop}

\begin{proof}
Let $\{ x_n \}$ be a sequence in $E$.
It is also a sequence in $K$.
Therefore it has a convergent subsequence $\{ x_{n_j} \}$ that converges to
some $x \in K$.
As $E$ is closed the limit of a sequence in $E$ is also in $E$
and so $x \in E$.
Thus $E$ must be compact.
\end{proof}

\begin{thm}[Heine--Borel%
\footnote{Named after the German mathematician 
\href{http://en.wikipedia.org/wiki/Eduard_Heine}{Heinrich Eduard Heine}
(1821--1881),
and the French mathematician
\href{http://en.wikipedia.org/wiki/\%C3\%89mile_Borel}{F\'elix \'Edouard Justin \'Emile Borel}
(1871--1956).}]%
\index{Heine--Borel theorem}
\label{thm:msbw}
A closed bounded subset $K \subset \R^n$ is compact.
\end{thm}

So subsets of $\R^n$ are compact if and only if they are closed and bounded,
a condition that is much easier to check.
Let us reiterate that the Heine--Borel theorem only holds for $\R^n$ and not
for metric spaces in general.
In general, compact implies closed and
bounded, but not vice versa.

\begin{proof}
For $\R = \R^1$ if $K \subset \R$ is closed and bounded, then
any sequence $\{ x_k \}$ in $K$ is bounded, so it has a convergent
subsequence by
Bolzano--Weierstrass theorem (\thmref{thm:bwseq}).
As $K$ is closed, the limit of the subsequence must be an element of
$K$.
So $K$ is compact.

Let us carry out the proof for $n=2$ and leave arbitrary $n$ as an exercise.
As $K \subset \R^2$ is bounded, there exists a set
$B=[a,b]\times[c,d] \subset \R^2$ such that $K \subset B$.
We will show
that $B$ is compact.
Then $K$, being a closed subset of a compact $B$, is
also compact.


Let $\{ (x_k,y_k) \}_{k=1}^\infty$ be a sequence in $B$.
That is,
$a \leq x_k \leq b$ and
$c \leq y_k \leq d$ for all $k$.
A bounded sequence of real numbers
has a convergent
subsequence so there is a subsequence $\{ x_{k_j} \}_{j=1}^\infty$
that is convergent.
The subsequence 
$\{ y_{k_j} \}_{j=1}^\infty$ is also a bounded sequence so there exists
a subsequence
$\{ y_{k_{j_i}} \}_{i=1}^\infty$ that is convergent.
A subsequence of a
convergent sequence is still convergent, so 
$\{ x_{k_{j_i}} \}_{i=1}^\infty$ is convergent.
Let
\begin{equation*}
x := \lim_{i\to\infty} x_{k_{j_i}}
\qquad \text{and} \qquad
y := \lim_{i\to\infty} y_{k_{j_i}} .
\end{equation*}
By \propref{prop:msconveuc},
$\bigl\{ (x_{k_{j_i}},y_{k_{j_i}}) \bigr\}_{i=1}^\infty$ converges to $(x,y)$.
Furthermore, as $a \leq x_k \leq b$ and
$c \leq y_k \leq d$ for all $k$, we know that $(x,y) \in B$.
\end{proof}

\begin{example}
The discrete metric provides interesting counterexamples again.
Let $(X,d)$ be a metric space with the discrete metric, that is $d(x,y) = 1$
if $x \not= y$.
Suppose
$X$ is an infinite set.
Then:
\begin{enumerate}[(i)]
\item $(X,d)$ is a complete metric space.
\item Any subset $K \subset X$ is closed and bounded.
\item A subset $K \subset X$ is compact if and only if it is a finite set.
\item The conclusion of the Lebesgue covering lemma is always satisfied with
e.g. $\delta = \nicefrac{1}{2}$, even for noncompact $K \subset X$.
\end{enumerate}
The proofs
of these statements are either trivial or are relegated to the exercises
below.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Let $(X,d)$ be a metric space and $A$ a finite subset of $X$.
Show that $A$ is compact.
\end{exercise}

\begin{exercise}
Let $A = \{ \nicefrac{1}{n} : n \in \N \} \subset \R$.  
\begin{enumerate}[a)]
 \item Show that $A$ is
not compact directly using the definition.
 \item Show that $A \cup \{ 0 \}$ is
compact directly using the definition.
\end{enumerate}
\end{exercise}


\begin{exercise}
Let $(X,d)$ be a metric space with the discrete metric.  
\begin{enumerate}[a)]
\item Prove that $X$ is complete.
\item Prove that $X$ is compact if and only if $X$ is a finite
set.
\end{enumerate}
\end{exercise}

\begin{exercise}
a)~Show that the union of finitely many compact sets is a compact set.
b)~Find an example where the union of infinitely many compact sets is not
compact.
\end{exercise}

\begin{exercise}
Prove \thmref{thm:msbw} for arbitrary dimension.
Hint: The trick is to use the correct notation.
\end{exercise}

\begin{exercise}
Show that a compact set $K$ is a complete metric space (using the subspace
metric).
\end{exercise}

\begin{exercise} \label{exercise:CabRcomplete}
Let $C([a,b],\R)$ be the metric space as in \exampleref{example:msC01}.
Show that
$C([a,b],\R)$ is a complete metric space.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:msclbounnotcompt}
Let $C([0,1],\R)$ be the metric space of \exampleref{example:msC01}.
Let $0$
denote the zero function.
Then show that the closed ball
$C(0,1)$ is not compact (even though it is closed and bounded).
Hints: Construct a sequence of distinct continuous functions $\{ f_n \}$ such that
$d(f_n,0) = 1$ and $d(f_n,f_k) = 1$ for all $n \not= k$.
Show that
the set $\{ f_n : n \in \N \} \subset C(0,1)$ is closed but not compact.
See \chapterref{fs:chapter} for inspiration.
\end{exercise}

\begin{exercise}[Challenging]
Show that there exists a metric on $\R$ that makes $\R$ into a compact set.
\end{exercise}

\begin{exercise}
Suppose $(X,d)$ is complete and suppose we have a countably infinite
collection of nonempty compact sets $E_1 \supset E_2 \supset E_3 \supset
\cdots$ then prove $\bigcap_{j=1}^\infty E_j \not= \emptyset$.
\end{exercise}

\begin{exercise}[Challenging]
Let $C([0,1],\R)$ be the metric space of \exampleref{example:msC01}.
Let $K$ be the set of $f \in C([0,1],\R)$ such that
$f$ is equal to a quadratic polynomial, i.e.\ $f(x) = a+bx+cx^2$, and such that
$\abs{f(x)} \leq 1$ for all $x \in [0,1]$,
that is $f \in C(0,1)$.
Show that $K$ is compact.
\end{exercise}

\begin{exercise}[Challenging] \label{exercise:mstotbound}
Let $(X,d)$ be a complete metric space.
Show that $K \subset X$ is compact if and only if $K$ is closed
and such that for every $\epsilon > 0$
there exists a finite set of points $x_1,x_2,\ldots,x_n$ with
%\begin{equation*}
$K \subset \bigcup_{j=1}^n B(x_j,\epsilon)$.
%\end{equation*}
Note: Such a set $K$ is said to be \emph{\myindex{totally bounded}},
so in a complete metric space a set is compact if and only
if it is closed and totally bounded.
\end{exercise}

\begin{exercise}
Take $\N \subset \R$ using the standard metric.
Find an open cover of $\N$
such that the conclusion of the Lebesgue covering lemma does not hold.
\end{exercise}

\begin{exercise}
Prove the general \myindex{Bolzano--Weierstrass theorem}:
Any bounded sequence $\{ x_k
\}$ in $\R^n$ has a convergent subsequence.
\end{exercise}

\begin{exercise}
Let $X$ be a metric space and
$C \subset \sP(X)$ the set of nonempty compact subsets of $X$.
Using the Hausdorff metric from \exerciseref{exercise:mshausdorffpseudo},
show that $(C,d_H)$ is a metric space.
That is, show that
if $L$ and $K$ are nonempty compact subsets then $d_H(L,K) = 0$
if and only if $L=K$.
\end{exercise}

\begin{exercise} \label{exercise:closedcomplete}
Let $(X,d)$ be a complete metric space and $E \subset X$ a closed set.
Show that $E$ with the subspace metric is a complete metric space.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Continuous functions}
\label{sec:metcont}

\sectionnotes{1 lecture}

\subsection*{Continuity}

\begin{defn}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces and $c \in X$.
Then $f \colon X \to Y$ is
\emph{continuous at $c$}\index{continuous at $c$}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x \in X$ and $d_X(x,c) <
\delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.

\medskip

When $f \colon X \to Y$ is continuous at all $c \in X$, then we simply say
that $f$ is a \emph{\myindex{continuous function}}.
\end{defn}

The definition agrees with the definition from \chapterref{lim:chapter} when
$f$ is a real-valued function on the real line, if we take the standard
metric on $\R$.

\begin{prop} \label{prop:contiscont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Then $f \colon X \to Y$ is
continuous at $c \in X$
if and only if for every sequence $\{ x_n \}$ in $X$
converging to $c$, the sequence $\{ f(x_n) \}$ converges
to $f(c)$.
\end{prop}

\begin{proof}
Suppose $f$ is continuous at $c$.
Let $\{ x_n \}$ be a
sequence in $X$ converging to $c$.
Given $\epsilon > 0$,
there is a $\delta > 0$ such that $d_X(x,c) < \delta$ implies
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.
So take $M$ such that
for all $n \geq M$, we have $d_X(x_n,c) < \delta$, then
$d_Y\bigl(f(x_n),f(c)\bigr) < \epsilon$.
Hence $\{ f(x_n) \}$
converges to $f(c)$.

On the other hand suppose $f$ is not continuous at $c$.
Then there exists an $\epsilon > 0$,
such that for every $n \in \N$ there exists an $x_n \in X$,
with
$d_X(x_n,c) < \nicefrac{1}{n}$ such that $d_Y\bigl(f(x_n),f(c)\bigr) \geq
\epsilon$.
Then $\{ x_n \}$ converges to $c$, but $\{ f(x_n) \}$
does not converge to $f(c)$.
\end{proof}

\begin{example}
Suppose $f \colon \R^2 \to \R$ is a polynomial.
That is,
\begin{equation*}
f(x,y) =
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk}\,x^jy^k =
a_{0\,0} + a_{1\,0} \, x +
a_{0\,1} \, y+  
a_{2\,0} \, x^2+  
a_{1\,1} \, xy+  
a_{0\,2} \, y^2+ \cdots +
a_{0\,d} \, y^d ,
\end{equation*}
for some $d \in \N$ (the degree) and $a_{jk} \in \R$.
Then we claim 
$f$ is continuous.
Let $\{ (x_n,y_n) \}_{n=1}^\infty$ be a sequence
in $\R^2$ that converges to $(x,y) \in \R^2$.
We have proved that this
means that $\lim\, x_n = x$ and $\lim\, y_n = y$.
So
by \propref{prop:contalg} we have
\begin{equation*}
\lim_{n\to\infty}
f(x_n,y_n) =
\lim_{n\to\infty}
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk} \, x_n^jy_n^k 
=
\sum_{j=0}^d
\sum_{k=0}^{d-j}
a_{jk} \, x^jy^k
=
f(x,y) .
\end{equation*}
So $f$ is continuous at $(x,y)$, and as $(x,y)$ was arbitrary $f$ is
continuous everywhere.
Similarly, a
polynomial in $n$ variables is continuous.
\end{example}

\subsection*{Compactness and continuity}

Continuous maps do not map closed sets to closed sets.
For example,
$f \colon (0,1) \to \R$ defined by $f(x) := x$ takes the set $(0,1)$, which
is closed in $(0,1)$, to the set $(0,1)$, which is not closed in $\R$.
On the other hand continuous maps do preserve compact sets.

\begin{lemma} \label{lemma:continuouscompact}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces
and $f \colon X \to Y$ a continuous function.
If
$K \subset X$ is a compact set, then $f(K)$ is a compact set.
\end{lemma}

\begin{proof}
A sequence in $f(K)$ can be written as
$\{ f(x_n) \}_{n=1}^\infty$, where
$\{ x_n \}_{n=1}^\infty$ is a sequence in $K$.
The set $K$ is compact and
therefore there is a subsequence
$\{ x_{n_i} \}_{i=1}^\infty$ that converges to some $x \in K$.
By continuity,
\begin{equation*}
\lim_{i\to\infty} f(x_{n_i}) = f(x) \in f(K) .
\end{equation*}
So every sequence in $f(K)$ has a subsequence convergent to 
a point in $f(K)$, and $f(K)$ is compact by \thmref{thm:mscompactisseqcpt}.
\end{proof}

As before, $f \colon X \to \R$ achieves an
\emph{\myindex{absolute minimum}} at $c \in X$ if
\begin{equation*}
f(x) \geq f(c) \qquad \text{ for all $x \in X$.}
\end{equation*}
On the other hand, $f$ achieves an 
\emph{\myindex{absolute maximum}} at $c \in X$ if
\begin{equation*}
f(x) \leq f(c) \qquad \text{ for all $x \in X$.}
\end{equation*}

\begin{thm}
Let $(X,d)$ be a compact metric space
and $f \colon X \to \R$ a continuous function.
Then
$f$ is bounded and in fact
$f$ achieves an absolute minimum and an absolute maximum on $X$.
\end{thm}

\begin{proof}
As $X$ is compact and $f$ is continuous, we have
that $f(X) \subset \R$ is compact.
Hence $f(X)$ is closed
and bounded.
In particular,
$\sup f(X) \in f(X)$ and
$\inf f(X) \in f(X)$, because both the sup and inf
can be achieved by sequences in $f(X)$ and $f(X)$ is closed.
Therefore there is some $x \in X$ such that $f(x) = \sup f(X)$
and some $y \in X$ such that $f(y) = \inf f(X)$.
\end{proof}

\subsection*{Continuity and topology}

Let us see how to define continuity in terms of the topology, that is,
the open sets.
We have already seen that topology determines which 
sequences converge, and so it is no wonder that the topology also
determines continuity of functions.

\begin{lemma} \label{lemma:mstopocontloc}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
A function $f \colon X \to Y$ is continuous at $c \in X$
if and only if for every open neighborhood $U$ of $f(c)$ in $Y$, the set
$f^{-1}(U)$ contains an open neighborhood of $c$ in $X$.
\end{lemma}

\begin{proof}
First suppose that $f$ is continuous at $c$.
Let $U$ be an open neighborhood of $f(c)$
in $Y$, then $B_Y\bigl(f(c),\epsilon\bigr) \subset U$ for some $\epsilon >
0$.
By continuity of $f$, there exists a $\delta > 0$
such that whenever $x$ is such that $d_X(x,c) < \delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.
In other words,
\begin{equation*}
B_X(c,\delta) \subset f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr) \subset
f^{-1}(U) ,
\end{equation*}
and $B_X(c,\delta)$ is an open neighborhood of $c$.

For the other direction,
let $\epsilon > 0$ be given.
If
$f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr)$ contains an open
neighborhood $W$ of $c$, it contains a ball.
That is, there is some $\delta > 0$
such that
\begin{equation*}
B_X(c,\delta) \subset W \subset f^{-1}\bigl(B_Y\bigl(f(c),\epsilon\bigr)\bigr) .
\end{equation*}
That means precisely that if $d_X(x,c) < \delta$ then $d_Y\bigl(f(x),f(c)\bigr)
< \epsilon$, and so $f$ is continuous at $c$.
\end{proof}

\begin{thm} \label{thm:mstopocont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
A function $f \colon X \to Y$
is continuous if and only if
for every open $U \subset Y$, $f^{-1}(U)$ is open in $X$.
\end{thm}

The proof follows from \lemmaref{lemma:mstopocontloc} and is left as
an exercise.

\begin{example}
Let $f \colon X \to Y$ be a continuous function.
\thmref{thm:mstopocont} tells us that if $E \subset Y$ is closed, then 
$f^{-1}(E) = X \setminus f^{-1}(E^c)$ is also closed.
Therefore if
we have a continuous
function $f \colon X \to \R$, then the
\emph{\myindex{zero set}} of $f$, that is, 
$f^{-1}(0) = \{ x \in X :
f(x) = 0 \}$, is closed.
We have just proved the most basic result in
\emph{\myindex{algebraic geometry}}, the study of
zero sets of polynomials.

Similarly the set where $f$ is nonnegative, that is,
$f^{-1}\bigl( [0,\infty) \bigr) = \{ x \in X :
f(x) \geq 0 \}$ is closed.
On the other hand the
set where $f$ is positive,
$f^{-1}\bigl( (0,\infty) \bigr) = \{ x \in X :
f(x) > 0 \}$ is open.
\end{example}

\subsection*{Uniform continuity}

As for continuous
functions on the real line, in the definition of continuity
it is sometimes convenient to be able to pick
one $\delta$ for all points.

\begin{defn}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Then $f \colon X \to Y$ is
\emph{uniformly continuous}\index{uniformly continuous!metric spaces}
if for every $\epsilon > 0$
there is a $\delta > 0$ such that whenever $x,c \in X$ and $d_X(x,c) <
\delta$, then
$d_Y\bigl(f(x),f(c)\bigr) < \epsilon$.
\end{defn}

A uniformly continuous function is continuous, but not necessarily
vice-versa as we have seen.

\begin{thm} \label{thm:Xcompactfunifcont}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces.
Suppose $f \colon X \to Y$ is continuous and $X$ compact.
Then
$f$ is uniformly continuous.
\end{thm}

\begin{proof}
Let $\epsilon > 0$ be given.
For each $c \in X$, pick $\delta_c > 0$ such that
$d_Y\bigl(f(x),f(c)\bigr) < \nicefrac{\epsilon}{2}$
whenever
$d_X(x,c) < \delta_c$.
The balls
$B(c,\delta_c)$ cover $X$, and the space $X$ is compact.  
Apply the \hyperref[ms:lebesgue]{Lebesgue covering lemma} to obtain a 
$\delta > 0$ such that for every $x \in X$, there is a $c \in X$
for which $B(x,\delta) \subset B(c,\delta_c)$.

If $x_1, x_2 \in X$ where $d_X(x_1,x_2) < \delta$,
find a $c \in X$ such that $B(x_1,\delta) \subset B(c,\delta_c)$.
Then $x_2 \in B(c,\delta_c)$.
By the triangle inequality
and the definition of $\delta_c$ we have
\begin{equation*}
d_Y\bigl(f(x_1),f(x_2)\bigr)
\leq
d_Y\bigl(f(x_1),f(c)\bigr)
+
d_Y\bigl(f(c),f(x_2)\bigr)
<
\nicefrac{\epsilon}{2}+
\nicefrac{\epsilon}{2} = \epsilon .  \qedhere
\end{equation*}
\end{proof}

\begin{example}
Useful examples of uniformly continuous functions are again the so-called
\emph{\myindex{Lipschitz continuous}} functions.
That is if
$(X,d_X)$ and $(Y,d_Y)$ are metric spaces, then $f \colon X \to Y$
is called Lipschitz or $K$-Lipschitz if there exists a $K \in \R$ such that
\begin{equation*}
d_Y\bigl(f(x),f(c)\bigr) \leq K d_X(x,c)
\ \ \ \ \text{for all } x,c \in X.
\end{equation*}
It is not difficult to prove that Lipschitz implies uniformly continuous,
just take $\delta = \nicefrac{\epsilon}{K}$.
And we already saw in the case
of functions on the real line, a function can be uniformly continuous
but not Lipschitz.

It is worth mentioning that,
if a function is Lipschitz, it tends to be
easiest to simply show it is Lipschitz even if we are only
interested in knowing continuity.
\end{example}

\subsection*{Exercises}

\begin{exercise}
Consider $\N \subset \R$ with the standard metric.  
Let $(X,d)$ be a metric space and $f \colon X \to \N$ a continuous function.  
\begin{enumerate}[a)]
 \item Prove that
if $X$ is connected, then $f$ is constant (the range of $f$ is a single
value).
 \item Find an example where $X$ is disconnected and $f$ is not constant.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let $f \colon \R^2 \to \R$ be defined by $f(0,0) := 0$, and $f(x,y) := \frac{xy}{x^2+y^2}$ if $(x,y) \not= (0,0)$.  
\begin{enumerate}[a)]
 \item Show that for any fixed $x$,
the function that takes $y$ to $f(x,y)$ is continuous.
Similarly
for any fixed $y$, the function that takes $x$ to $f(x,y)$ is continuous.
 \item Show that $f$ is not continuous.
\end{enumerate}
\end{exercise}

\begin{exercise} 
Suppose that $f \colon X \to Y$ is continuous for metric spaces $(X,d_X)$ and $(Y,d_Y)$.  
Let $A \subset X$.
\begin{enumerate}[a)]
 \item Show that $f(\overline{A}) \subset
\overline{f(A)}$.
  \item Show that the subset can be proper.
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove \thmref{thm:mstopocont}.
Hint: Use \lemmaref{lemma:mstopocontloc}.
\end{exercise}

\begin{exercise} \label{exercise:msconnconn}
Suppose $f \colon X \to Y$ is continuous for metric spaces $(X,d_X)$
and $(Y,d_Y)$.
Show that if $X$ is connected, then $f(X)$ is connected.
\end{exercise}

\begin{exercise}
Prove the following version of the
\hyperref[IVT:thm]{intermediate value theorem}.
Let $(X,d)$ be a connected
metric space and $f \colon X \to \R$ a continuous function.
Suppose that
there exist $x_0,x_1 \in X$ and $y \in \R$ such that $f(x_0) < y < f(x_1)$.
Then prove that there exists a $z \in X$ such that $f(z) = y$.
Hint: See \exerciseref{exercise:msconnconn}.
\end{exercise}

\begin{exercise}
A continuous function $f \colon X \to Y$ for metric spaces $(X,d_X)$ and
$(Y,d_Y)$ is said to be \emph{\myindex{proper}}
if for every compact set $K \subset Y$, the set $f^{-1}(K)$ is compact.
Suppose a continuous $f \colon (0,1) \to (0,1)$ is proper and $\{ x_n
\}$ is a sequence in $(0,1)$ that converges to $0$.
Show that
$\{ f(x_n) \}$ has no subsequence that converges in $(0,1)$.
\end{exercise}

\begin{exercise}
Let $(X,d_X)$ and $(Y,d_Y)$ be metric space and
$f \colon X \to Y$ be a one-to-one and onto continuous function.
Suppose
$X$ is compact.
Prove that the inverse $f^{-1} \colon Y \to X$
is continuous.
\end{exercise}

\begin{exercise}
Take the metric space of continuous functions $C([0,1],\R)$.
Let
$k \colon [0,1] \times [0,1] \to \R$ be a continuous function.
Given $f \in C([0,1],\R)$ define
\begin{equation*}
\varphi_f(x) := \int_0^1 k(x,y) f(y) ~dy .
\end{equation*}
a) Show that $T(f) := \varphi_f$ defines a function $T \colon C([0,1],\R) \to
C([0,1],\R)$.
\\
b) Show that $T$ is continuous.
\end{exercise}

\begin{exercise}
Let $(X,d)$ be a metric space.\\
a) If $p \in X$,
show that $f \colon X \to \R$ defined
by $f(x) := d(x,p)$ is continuous.
\\
b) Define a metric on $X \times X$ as in \exerciseref{exercise:mscross} part
b, and show that $g \colon X \times X \to \R$ defined by
$g(x,y) := d(x,y)$ is continuous.
\\
c) Show that if $K_1$ and $K_2$ are compact subsets of $X$, then
there exists a $p \in K_1$ and $q \in K_2$ such that $d(p,q)$ is minimal,
that is, $d(p,q) = \inf \{ (x,y) \colon x \in K_1, y \in K_2 \}$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sectionnewpage
\section{Fixed point theorem and Picard's theorem again}
\label{sec:metpicard}

\sectionnotes{1 lecture (optional, does not require \sectionref{sec:picard})}

In this section we prove the fixed point theorem for contraction
mappings.
As an application we prove Picard's theorem, which we proved
without metric spaces in \sectionref{sec:picard}.
The proof we present here is similar, but the proof goes a lot
smoother with metric spaces and the fixed point theorem.

\subsection*{Fixed point theorem}

\begin{defn}
Let $(X,d)$ and $(X',d')$ be metric spaces.
$f \colon X \to X'$ is said to be a \emph{contraction}
(or a contractive map) if it is
a $k$-Lipschitz map for some $k < 1$, i.e.\ if there exists a $k < 1$ such that
\begin{equation*}
d'\bigl(f(x),f(y)\bigr) \leq k d(x,y)
\ \ \ \ \text{for all } x,y \in X.
\end{equation*}

\medskip

If $f \colon X \to X$ is a map, $x \in X$ is called a \emph{fixed point}
if $f(x)=x$.
\end{defn}

\begin{thm}%
[\myindex{Contraction mapping principle} or \myindex{Fixed point theorem}]
\label{thm:contr}
Let $(X,d)$ be a nonempty complete metric space and $f \colon X \to X$ a
contraction.
Then $f$ has a unique fixed point.
\end{thm}

The words \emph{complete} and \emph{contraction} are necessary.
See \exerciseref{exercise:nofixedpoint}.

\begin{proof}
Pick any $x_0 \in X$.
Define a sequence $\{ x_n \}$ by $x_{n+1} := f(x_n)$.
\begin{equation*}
d(x_{n+1},x_n) = d\bigl(f(x_n),f(x_{n-1})\bigr)
\leq k d(x_n,x_{n-1})
\leq \cdots
\leq k^n d(x_1,x_0) .
\end{equation*}
Suppose $m > n$, then
\begin{equation*}
\begin{split}
d(x_m,x_n)
& \leq \sum_{i=n}^{m-1} d(x_{i+1},x_i) \\
& \leq \sum_{i=n}^{m-1} k^i d(x_1,x_0) \\
& = k^n d(x_1,x_0) \sum_{i=0}^{m-n-1} k^i \\
& \leq k^n d(x_1,x_0) \sum_{i=0}^{\infty} k^i
= k^n d(x_1,x_0) \frac{1}{1-k} .
\end{split}
\end{equation*}
In particular the sequence is Cauchy (why?).
Since $X$ is complete
we let $x := \lim\, x_n$, and we claim that $x$
is our unique fixed point.

Fixed point?
  The function $f$ is continuous as it is a contraction, so
Lipschitz continuous.
Hence
\begin{equation*}
f(x) = f( \lim \, x_n) = \lim\, f(x_n) = \lim\, x_{n+1} = x .
\end{equation*}

Unique?
  Let $x$ and $y$ both be fixed points.
\begin{equation*}
d(x,y) = d\bigl(f(x),f(y)\bigr) \leq k d(x,y) .
\end{equation*}
As $k < 1$ this means that $d(x,y) = 0$ and hence $x=y$.
The theorem is
proved.
\end{proof}

The proof is constructive.
Not only do we know 
a unique fixed point exists.
We also know how to find it.
We start with
any point $x_0 \in X$ and simply iterate $f(x_0)$,
$f(f(x_0))$,
$f(f(f(x_0)))$, etc\ldots  In fact, you can even find how far away
from the fixed point you are, see the exercises.
The idea of the proof is
therefore used in real world applications.

\subsection*{Picard's theorem}

Before we get to Picard, let us mention what metric space we will be
applying the fixed point theorem to.
We will use the metric space
$C([a,b],\R)$ of \exampleref{example:msC01}.
That is, $C([a,b],\R)$
is the space of continuous functions $f \colon [a,b] \to \R$ with the metric
\begin{equation*}
d(f,g) = \sup_{x \in [a,b]} \abs{f(x)-g(x)} .
\end{equation*}
Convergence in this metric is convergence in uniform norm, or in other
words, uniform convergence.
Therefore, see
\exerciseref{exercise:CabRcomplete}, $C([a,b],\R)$ is a complete metric space.

\medskip

Let us use the
fixed point theorem
to prove the classical Picard theorem on the existence and uniqueness of
ordinary differential equations.
Consider the equation
\begin{equation*}
\frac{dy}{dx} = F(x,y) .
\end{equation*}
Given some $x_0, y_0$ we are looking for a function $y=f(x)$ such that
$f(x_0) = y_0$ and such that
\begin{equation*}
f'(x) = F\bigl(x,f(x)\bigr) .
\end{equation*}
To avoid having to come up with many names we often simply write $y' = F(x,y)$
and $y(x)$ for the solution.

The simplest example is for example the equation $y' = y$, $y(0) = 1$.
The solution is the exponential $y(x) = e^x$.
A somewhat more complicated
example is $y' = -2xy$, $y(0) = 1$, whose solution is the Gaussian
$y(x) = e^{-x^2}$.

There are some subtle issues, for example how long does the
solution exist.
Look at the equation $y' = y^2$, $y(0)=1$.
Then $y(x) = \frac{1}{1-x}$ is a
solution.
While $F$ is a reasonably ``nice'' function and in particular
exists for all $x$ and $y$, the solution ``blows up'' at $x=1$.
For more examples related to Picard's theorem see \sectionref{sec:picard}.

\begin{thm}[Picard's theorem on existence and uniqueness]%
\index{existence and uniqueness theorem}\index{Picard's theorem}
Let $I, J \subset \R$ be compact intervals, let $I_0$ and $J_0$
be their interiors, and 
let $(x_0,y_0) \in I_0 \times J_0$.
Suppose $F \colon I \times J \to \R$ is continuous
and Lipschitz in the second variable, that is, there exists
an $L \in \R$ such that
\begin{equation*}
\abs{F(x,y) - F(x,z)} \leq L \abs{y-z}
\ \ \ \text{ for all $y,z \in J$, $x \in I$} .
\end{equation*}
Then there exists an $h > 0$ and a unique differentiable
function
$f \colon [x_0 - h, x_0 + h] \to J \subset \R$, such that
\begin{equation*}
f'(x) = F\bigl(x,f(x)\bigr) \qquad \text{and} \qquad f(x_0) = y_0.
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality assume $x_0 =0$.
As $I \times J$ is compact and
$F(x,y)$ is continuous, it is bounded.
So find an $M > 0$, such that
$\abs{F(x,y)} \leq M$ for all $(x,y) \in I\times J$.
%Let $M := \sup \{ \abs{F(x,y)} : (x,y) \in I\times J \}$, nah because we
%want $M > 0$.
Pick $\alpha > 0$ such that
$[-\alpha,\alpha] \subset I$ and $[y_0-\alpha, y_0 + \alpha] \subset J$.
Let
\begin{equation*}
h := \min \left\{ \alpha, \frac{\alpha}{M+L\alpha} \right\} .
\end{equation*}
Note $[-h,h] \subset I$.
Define the set
\begin{equation*}
Y := \{ f \in C([-h,h],\R) : f([-h,h]) \subset J \} . % [y_0-\alpha,y_0+\alpha] \} .
\end{equation*}
That is, $Y$ is the space of continuous functions on $[-h,h]$ with values in
$J$, in other words,
exactly those functions where $F\bigl(x,f(x)\bigr)$ makes sense.
The metric used is the standard metric given above.
%Here $C([-h,h],\R)$ is equipped with the standard metric $d(f,g) := 
%\sup \{ \abs{f(x)-g(x)} : x \in [-h,h] \}$.  With this metric
%you have shown in \exerciseref{exercise:CabRcomplete} that $C([-h,h],\R)$ is a complete metric space.
%Here $C([-h,h],\R)$ is equipped with the standard metric $d(f,g) := 
%\sup \{ \abs{f(x)-g(x)} : x \in [-h,h] \}$.  With this metric
%you have shown in \exerciseref{exercise:CabRcomplete} that $C([-h,h],\R)$ is a complete metric space.

\begin{exercise}
Show that $Y \subset C([-h,h],\R)$ is closed.
Hint: $J$ is closed.
\end{exercise}

The space $C([-h,h],\R)$ is complete, and
a closed subset of a complete metric space is a complete metric space with
the subspace metric, see \exerciseref{exercise:closedcomplete}.
So $Y$ with the subspace metric is
complete.

Define a mapping
$T \colon Y \to C([-h,h],\R)$ by
\begin{equation*}
T(f)(x)
:=
y_0 + \int_0^x F\bigl(t,f(t)\bigr)~dt .
\end{equation*}

\begin{exercise}
Show that if $f \colon [-h,h] \to J$ is continuous then $F\bigl(t,f(t)\bigr)$
is continuous on $[-h,h]$ as a function of $t$.
Use this to show that
$T$ is well defined and that $T(f) \in C([-h,h],\R)$.
\end{exercise}

Let $f \in Y$ and $\abs{x} \leq h$.
As $F$ is bounded by $M$ we have
\begin{equation*}
\begin{split}
\abs{T(f)(x) - y_0}
&= \abs{\int_0^x F\bigl(t,f(t)\bigr)~dt} \\
& \leq 
\abs{x}M \leq hM \leq \frac{\alpha M}{M+ L\alpha} \leq \alpha .
\end{split}
\end{equation*}
So $T(f)([-h,h]) \subset [y_0-\alpha,y_0+\alpha] \subset J$, and
$T(f) \in Y$.
In other words, $T(Y) \subset Y$.
We thus consider
$T$ as a mapping of $Y$ to $Y$.

We claim $T \colon Y \to Y$ is a contraction.
First, for $x \in [-h,h]$
and $f,g \in Y$ we have
\begin{equation*}
\abs{F\bigl(x,f(x)\bigr) - F\bigl(x,g(x)\bigr)} \leq
L\abs{f(x)- g(x)} \leq L \, d(f,g) .
\end{equation*}
Therefore,
\begin{equation*}
\begin{split}
\abs{T(f)(x) - T(g)(x)}
&= \abs{\int_0^x F\bigl(t,f(t)\bigr) - F\bigl(t,g(t)\bigr)~dt} \\
& \leq \abs{x} L \, d(f,g)
 \leq h L\, d(f,g)
 \leq \frac{L\alpha}{M+L\alpha} \, d(f,g) .
\end{split}
\end{equation*}
We chose $M > 0$ and so
$\frac{L\alpha}{M+L\alpha} < 1$.
The claim is proved by
taking supremum over $x \in [-h,h]$ of the left hand side above to obtain
$d\bigl(T(f),T(g)\bigr) \leq \frac{L\alpha}{M+L\alpha} \, d(f,g)$.

We apply the fixed point theorem (\thmref{thm:contr})
to find a unique $f \in Y$ such that $T(f) = f$, that is,
\begin{equation*} %\label{equation:msinteqpicard}
f(x) = y_0 + \int_0^x F\bigl(t,f(t)\bigr)~dt .
\end{equation*}
By the fundamental theorem of calculus, $T(f)$ is the unique differentiable 
function whose derivative is
$F\bigl(x,f(x)\bigr)$ and $T(f)(0) = y_0$.
Therefore $f$ is the unique
solution of $f'(x) = F\bigl(x,f(x)\bigr)$ and $f(0) = y_0$.
\end{proof}

%\begin{exercise}
%We have shown that $f$ is the unique function in $Y$.  Why is it the unique
%continuous function $f \colon [-h,h] \to J$ that solves
%\eqref{equation:msinteqpicard} above?
%\end{exercise}

\begin{exercise}
Prove that the statement ``Without loss of generality assume $x_0 = 0$'' is
justified.
That is, prove that if we know the theorem with $x_0 = 0$, the
theorem is true as stated.
\end{exercise}

\subsection*{Exercises}

\begin{exnote}
For more exercises related to Picard's theorem see \sectionref{sec:picard}.
\end{exnote}

\begin{exercise}
Let $F \colon \R \to \R$ be defined by
$F(x) := kx + b$ where $0 < k < 1$, $b \in \R$.\\
a) Show that $F$ is a contraction.\\
b) Find the fixed
point and show directly that it is unique.
\end{exercise}

\begin{exercise}
Let $f \colon [0,\nicefrac{1}{4}] \to [0,\nicefrac{1}{4}]$ be defined by
$f(x) := x^2$ is a contraction.\\
a) Show that $f$
is a contraction, and find the best (smallest) $k$ from the definition that works.\\
b) Find the fixed point and show directly that it is unique.
\end{exercise}

\begin{exercise} \label{exercise:nofixedpoint}
a) Find an example of a contraction $f \colon X \to X$
of non-complete metric space $X$ with no
fixed point.
b) Find a 1-Lipschitz map $f \colon X \to X$ of a complete metric space $X$ with no fixed point.
\end{exercise}

\begin{exercise}
Consider $y' =y^2$, $y(0)=1$.
Use the iteration scheme
from the proof of the contraction mapping principle.
Start with $f_0(x) = 1$.
Find a 
few iterates (at least up to $f_2$).
Prove that
the pointwise limit of $f_n$ is $\frac{1}{1-x}$, that is for every $x$
with $\abs{x} < h$ for some $h > 0$,
prove that $\lim\limits_{n\to\infty}f_n(x) = \frac{1}{1-x}$.
\end{exercise}

\begin{exercise}
Suppose $f \colon X \to X$ is a contraction for $k < 1$.
Suppose you use the iteration
procedure with $x_{n+1} := f(x_n)$ as in the proof of the fixed point theorem.
Suppose $x$ is the fixed
point of $f$.\\
a) Show that $d(x,x_n) \leq k^n d(x_1,x_0) \frac{1}{1-k}$ for all $n \in \N$.\\
b) Suppose $d(y_1,y_2) \leq 16$ for all $y_1,y_2 \in X$, and $k=
\nicefrac{1}{2}$.
Find an $N$ such that starting at any point $x_0 \in X$, 
$d(x,x_n) \leq 2^{-16}$ for all $n \geq N$.
\end{exercise}

\begin{exercise}
Let $f(x) := x-\frac{x^2-2}{2x}$. (You may recognize Newton's method for
$\sqrt{2}$)\\
a) Prove $f\bigl([1,\infty)\bigr) \subset [1,\infty)$.\\
b) Prove that $f \colon [1,\infty) \to [1,\infty)$ is a contraction.\\
c) Apply the fixed point theorem to find an $x \geq 1$ such that
$f(x) = x$, and show that $x = \sqrt{2}$.
\end{exercise}

\begin{exercise}
Suppose $f \colon X \to X$ is a contraction, and $(X,d)$ is a metric space
with the discrete metric, that is $d(x,y) = 1$ whenever $x \not= y$.
Show that $f$ is constant, that is,
there exists a $c \in X$ such that $f(x) = c$ for all $x \in X$.
\end{exercise}

\chapter{Additional topics} \label{additional topics:chapter}

\section{Cardinality}

In this subsection we generalize the statements Chapter~\ref{rn:counable} to abstract sets.

As we already saw, some infinite sets, as the set of reals and rational numbers, have different ``size'' .  
The concept of cardinality is made to formalize it.  



\begin{defn}
Let $A$ and $B$ be sets.
We say $A$ and $B$ have the same
\emph{\myindex{cardinality}}
when there exists a bijection $f \colon A \to B$.
In this case we write $\abs{A}=\abs{B}$.

More precisely,
the existence of bijection between the sets is 
an \emph{equivalence relation} and $\abs{A}$ denotes the \emph{equivalence class} of this relation. 
The class $\abs{A}$ is called the \emph{cardinality} of $A$.
%???EQUUVALENCE RELATION IS NOT DEFINED, AND MAYBE IT SHOULD BE...
\end{defn}

Note that $A$ has the same cardinality as the empty set if and only
if $A$ itself is the empty set.
We then write $\abs{A} := 0$.

\begin{defn}
Suppose $A$ has the same cardinality as $\{ 1,2,3,\ldots,n \}$
for some $n \in \N$.
We then write $\abs{A} := n$, and we say $A$ is \emph{\myindex{finite}}.
When $A$ is the empty set, we also call $A$ finite.

We say $A$ is \emph{\myindex{infinite}} or ``of infinite cardinality''
if $A$ is not finite.
\end{defn}

That the notation $\abs{A} = n$ is justified we leave as an exercise.
That
is, for each nonempty finite set $A$, there exists a unique natural number
$n$ such that there exists a bijection from $A$ to $\{ 1,2,3,\ldots,n \}$.

Now let us order sets by ``size''.

\begin{defn}\label{def:comparecards}
We write
$\abs{A} \leq \abs{B}$
if there exists an injection from $A$ to $B$.
We write $\abs{A} = \abs{B}$
if $A$ and $B$ have the same cardinality.
We write $\abs{A} < \abs{B}$
if $\abs{A} \leq \abs{B}$, but $A$ and $B$ do not have the same cardinality.
\end{defn}

The following proposition might look obvious,
but  
some mathematicians think it is right, 
some think it is not and most of mathematicians do not care.
This is an equivalent to so called \emph{axiom of choice};
it can not be proved or disproved based on standard foundations of mathematics, so it is kind of okey to assume it is true when needed and we will do so.

\begin{prop}\label{axiom of chice}
Let $A$ and $B$ be two sets.
Assume there is a surjection $f\colon B\to A$ then there is an injection $g\colon A\to B$
such that $f\circ g (a)=a$ for any $a$.

In particular if there is a surjection $B\to A$, then $\abs{B}\ge \abs{A}$.
\end{prop}

Let us state two result without giving a proof.
The second part of this statement is called Cantor--Bernstein--Schroeder theorem

\begin{thm}
For any two sets $A$ and $B$, we have 
$\abs{A} \leq \abs{B}$ or
$\abs{B} \leq \abs{A}$.
Moreover,
$\abs{A} = \abs{B}$ if and only if
$\abs{A} \leq \abs{B}$ and
$\abs{B} \leq \abs{A}$.

In other words, for any two sets $A$ and $B$
there is an injection $A\to B$ or $B\to A$
and if both of them exist then there is a bijection $A\to B$. 
\end{thm}

The issues surrounding the first statement are very subtle;
its proof requires Proposition \ref{axiom of chice}.

Using this terminology, we can say that the set $A$ is countable if
$\abs{A} \le \abs{\N}$.

\begin{example}
The set of even natural numbers has the same cardinality as $\N$.  

\begin{proof}
Given an even natural number, write it as $2n$ for some $n \in \N$.  
Then create a bijection taking $2n\mapsto n$.
\end{proof}
\end{example}

In fact, let us mention without proof the following characterization
of infinite sets: \emph{A set is infinite if and only if it is in one-to-one
correspondence with a proper subset of itself}.

For completeness we mention the following statement.
\emph{If $A \subset
B$ and $B$ is countable, then $A$ is countable.
Similarly if $A$ is
uncountable, then $B$ is uncountable}.
As we will not need this statement in
the sequel, and as the proof requires the
Cantor--Bernstein--Schroeder theorem mentioned above, we will not give it
here.

\begin{defn}
If $A$ is a set,
we define the \emph{\myindex{power set}} of $A$, denoted by $\sP(A)$, to be
the set of all subsets of $A$.
\end{defn}

For example, if $A := \{ 1,2\}$, then $\sP(A) = \{ \emptyset, \{ 1 \}, \{ 2 \},
\{ 1, 2 \} \}$.  
For a finite set $A$ of cardinality $n$, the cardinality of $\sP(A)$ is $2^n$. 
This fact can be easily proved by induction on $n$.  

In particular, 
$$\abs{A} < \abs{\sP(A)}$$
for any finite set $A$.  
It might look unexpected and striking fact is that this statement is still true for infinite sets.

\begin{thm}[Cantor%
\footnote{Named after the German mathematician
\href{http://en.wikipedia.org/wiki/Georg_Cantor}{Georg Ferdinand Ludwig
Philipp Cantor} (1845 -- 1918).}]
\index{Cantor's theorem}
For any set $A$, we have
$$\abs{A} < \abs{\sP(A)}.$$  
In particular, there exists no surjection $A\to\sP(A)$.
\end{thm}

In fact it is $\abs{\R}=\abs{\sP(\N)}$, see \exerciseref{ex:|P(N)|=|R|}.

At this point one may see that the statement does not hold if $A$ is set of all sets.
Indeed, $\sP(A)$ must be a subset of $A$ since it includes \emph{all} sets.
This question is not at all stupid and indeed according to the theorem there the set of all sets does not exist.

One particular consequence of this 
theorem is that there do exist uncountable sets,
as $\sP(\N)$ must be uncountable.
As you will see soon, the set of real numbers is uncountable.
The existence of uncountable sets may seem unintuitive, and the theorem
caused quite a controversy at the time
it was announced.
The theorem not only says that uncountable sets exist,
but that there in fact exist progressively larger
and larger infinite sets since 
$$\abs{\N}<\abs{\sP(\N)}<\abs{\sP(\sP(\N))}<\abs{\sP(\sP(\sP(\N)))}<\ldots$$


\begin{proof}
There exists an injection $f \colon A \to \sP(A)$.
For any $x \in A$, define $f(x) := \{ x \}$;
that is $f(x)$ is the set containing single element $x$. 
Therefore $\abs{A} \leq \abs{\sP(A)}$.

To finish the proof, we must show that
no function $f \colon A \to \sP(A)$ is a surjection.

Arguing by contradiction, suppose
$f \colon A \to \sP(A)$ is a surjection.  
So for $x \in A$, $f(x)$ is a subset of $A$.  
Consider the following subset of $A$
\begin{equation*}
B := \{ x \in A : x \notin f(x) \} .
\end{equation*}
We claim that $B$ is not in the range of $f$ and hence $f$ is not a
surjection.  
Suppose there exists an $x_0$ such that $f(x_0) = B$.
Either $x_0 \in B$ or $x_0 \notin B$.  
If $x_0 \in B$, then $x_0 \notin f(x_0) = B$, which is a contradiction.  
If $x_0 \notin B$, then $x_0 \in f(x_0) = B$, which is again a contradiction.
\end{proof}

Lets us use the idea in the above proof together with decimal representation or reals to prove Theorem~\ref{thm:Cantor}.
This is Cantor's second proof, and is probably more well known.
While this proof may seem shorter, it is because we have already done the hard part above and we are left with a slick trick to prove that $\R$ is uncountable.  
This trick is called \emph{\myindex{Cantor diagonalization}}\index{diagonalization} and finds use in other proofs as well.

\begin{proof}[Alternative proof of Theorem~\ref{thm:Cantor}]
We will show that the set $(0,1]$ is uncountable;
it is stronger than required.

Let $X := \{ x_1,x_2,x_3,\ldots \}$ be any countable subset of real numbers in $(0,1]$.
We will construct a real number not in $X$.
Let
\begin{equation*}
x_n = 0.d_1^nd_2^nd_3^n\ldots
\end{equation*}
be the unique representation from the proposition, that is $d_j^n$ is the
$j$th digit of the $n$th number.
Let
$e_n := 1$ if $d_n^n \not= 1$, and 
let $e_n := 2$ if $d_n^n = 1$.
Let $E_n$ be the $n$-digit truncation of $y = 0.e_1e_2e_3\ldots$.
Because
all the digits are nonzero we get that $E_n < E_{n+1} \leq y$.
Therefore
\begin{equation*}
E_n < y \leq E_n + {10}^{-n} 
\end{equation*}
for all $n$, and the representation is the unique one for $y$ from 
the proposition.
But for every $n$, the $n$th digit
of $y$ is different from the $n$th digit of $x_n$, so $y \not= x_n$.
Therefore $y \notin X$, and as $X$ was an arbitrary countable subset,
$(0,1]$ must be uncountable.
\end{proof}

\subsection*{Exercises}

\begin{exercise}
Prove that the cardinality of $[0,1]$ is the same as the cardinality of
$(0,1)$ by showing that
$\abs{[0,1]} \leq \abs{(0,1)}$ and
$\abs{(0,1)} \leq \abs{[0,1]}$.
See 
\defnref{def:comparecards}.
Note that this requires the Cantor--Bernstein--Schroeder theorem we
stated without proof.
Also note that this proof does not give you an
explicit bijection.
\end{exercise}

\begin{exercise}
Determine $\sP(S)$ (the power set) for each of the following:
\begin{enumerate}[a)]
\item $S = \emptyset$,
\item $S = \{1\}$,
\item $S = \{1,2\}$,
\item $S = \{1,2,3,4\}$.
\end{enumerate}
\end{exercise}

\begin{exercise}\label{ex:|P(N)|=|R|}
Using \exerciseref{ex:binary} with $b=2$ (binary), 
show that cardinality of $\R$ is the same as the cardinality of $\sP(\N)$,
obtaining yet another (though related) proof that $\R$ is uncountable.
Hint: Consider the map $\sP(\N)\to[0,1]$ which sends
set $A \subset \N$, to the real number with the $n$th binary digit 1 if $n\in A$ and $0$ otherwise and modify it to make it bijection.
\end{exercise}

\begin{exercise}[Challenging]
Assume $A\cup B=[0,1]$. 
Show that $\abs{A}=\abs{[0,1]}$ or $\abs{B}=\abs{[0,1]}$.
Hint: Use \exerciseref{exercise:RxR} and Proposition~\ref{axiom of chice}.
\end{exercise}

\begin{exercise}
For $a < b$, construct an explicit bijection from $(a,b]$ to $(0,1]$.
\end{exercise}

\begin{exercise}
Suppose a bijection $f \colon [0,1] \to (0,1)$ is given.
Use $f$ to construct a
bijection from $[-1,1]$ to $\R$.
\end{exercise}

\begin{exercise}
Construct an explicit bijection from $(0,\infty)$ to $[0,\infty)$.
Hint: Take $x\mapsto x-1$ for $x\in\N$ and $x\mapsto x$ otherwise.
\end{exercise}

\begin{exercise}[Hard]
Construct an explicit bijection from $[0,1]$ to $(0,1)$.
\end{exercise}

\begin{exercise}
Denote by $\mathcal{F}$ the set of all functions $\R\to\R$. 
Show that $\abs{\mathcal{F}}>\abs{\R}$.
\end{exercise}

\begin{exercise}
Denote by $\mathcal{C}$ the set of all continuous functions $\R\to\R$. 
Show that $\abs{\mathcal{C}}=\abs{\R}$.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{I do not think it is needed}

\subsection*{Range of functions}

The last example leads us to the concept of bounded functions.

\begin{defn}
Suppose $f \colon D \to \R$ is a function.
We say $f$ is
\emph{bounded}\index{bounded function}
if there exists a number $M$
such that $\abs{f(x)} \leq M$ for all $x \in D$.
\end{defn}

In the example we proved $x^2-9x+1$ is bounded when considered as a
function on $D = \{ x : -1 \leq x \leq 5 \}$.
 On the other hand,
if we consider the same polynomial as a function on the whole real line $\R$,
then it is not bounded.

For a function $f \colon D \to \R$ we write
\begin{align*}
& \sup_{x \in D} f(x) := \sup\, f(D) , \\
& \inf_{x \in D} f(x) := \inf\, f(D) .
\end{align*}
We also sometimes replace the ``$x \in D$'' with an expression.
For example if, as before, $f(x) = x^2-9x+1$, for $-1 \leq x \leq 5$, 
a little bit of calculus shows
\begin{equation*}
\sup_{x \in D} f(x) = 
\sup_{-1 \leq x \leq 5} ( x^2 -9x+1 ) = 11,
\qquad
\inf_{x \in D} f(x) = 
\inf_{-1 \leq x \leq 5} ( x^2 -9x+1 ) = \nicefrac{-77}{4} .
\end{equation*}



%To illustrate some common issues, let us prove the following proposition.

\begin{prop} \label{prop:funcsupinf}
If $f \colon D \to \R$ and $g \colon D \to \R$ ($D$ nonempty) are
bounded\footnote{The boundedness hypothesis is for simplicity,
it can be dropped if we allow for the extended real numbers.}
functions and
\begin{equation*}
f(x) \leq g(x) \qquad \text{for all $x \in D$},
\end{equation*}
then
\begin{equation} \label{prop:funcsupinf:eq}
\sup_{x \in D} f(x) \leq \sup_{x \in D} g(x)
\qquad \text{and} \qquad
\inf_{x \in D} f(x) \leq \inf_{x \in D} g(x) .
\end{equation}
\end{prop}

You should be careful with the variables.
The $x$ on the left side of
the inequality in \eqref{prop:funcsupinf:eq}
is different from the $x$ on the right.
You
should really think of the first inequality as
\begin{equation*}
\sup_{x \in D} f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
Let us prove this inequality.
If $b$ is an upper bound for $g(D)$, then
$f(x) \leq g(x) \leq b$ for all $x \in D$, and hence $b$ is an upper bound for $f(D)$.
Taking the least upper bound we get that for all $x \in D$
\begin{equation*}
f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
Therefore
$\sup_{y \in D} g(y)$ is an upper bound for $f(D)$ and thus greater than or
equal to the least upper bound of $f(D)$.
\begin{equation*}
\sup_{x \in D} f(x) \leq \sup_{y \in D} g(y) .
\end{equation*}
The second inequality (the statement about the inf) is left as an exercise.

\medskip

A common mistake is to conclude 
\begin{equation} \label{rn:av:ltnottrue}
\sup_{x \in D} f(x) \leq \inf_{y \in D} g(y) .
\end{equation}
The inequality \eqref{rn:av:ltnottrue} is not true given the hypothesis of the claim above.
For this stronger
inequality we need the stronger hypothesis
\begin{equation*}
f(x) \leq g(y) \qquad \text{for all $x \in D$ and $y \in D$.}
\end{equation*}
The proof as well as a counterexample is left as an exercise.

\subsection*{Exercises}

\begin{exercise}
Let $x, y \in \R$.  
Suppose $x^2 + y^2 = 0$.  
Prove that $x = 0$ and $y = 0$.
\end{exercise}

\begin{exercise}
Prove the \emph{\myindex{arithmetic-geometric mean inequality}}.  That is, 
for two positive real numbers $x,y$ we have
\begin{equation*}
\sqrt{xy} \leq \frac{x+y}{2} .
\end{equation*}
Furthermore, equality occurs if and only if $x=y$.
\end{exercise}

\begin{exercise}[Easy]
Prove \propref{prop:existsxepsfromsup}.
\end{exercise}

\begin{exercise} \label{exercise:bernoulliineq}
Prove the so-called \emph{\myindex{Bernoulli's inequality}}%
\footnote{%
Named after the Swiss mathematician
\href{http://en.wikipedia.org/wiki/Jacob_Bernoulli}{Jacob Bernoulli} (1655 --
1705).}%
: If $1+x > 0$ then
for all $n \in \N$ we have $(1+x)^n \geq 1+nx$.
\end{exercise}


\begin{exercise}
Finish the proof of \propref{prop:funcsupinf}.
That is, prove that
given any set $D$,
and two bounded functions
$f \colon D \to \R$ and $g \colon D \to \R$ 
such that $f(x) \leq g(x)$ for all $x \in D$, then 
\begin{equation*}
\inf_{x\in D} f(x) \leq \inf_{x\in D} g(x) .
\end{equation*}
\end{exercise}

\begin{exercise}
Let 
$f \colon D \to \R$ and $g \colon D \to \R$ be functions ($D$ nonempty).
\begin{enumerate}[a)]
%
\item
Suppose 
$f(x) \leq g(y)$ for all $x \in D$ and $y \in D$.
Show that
\begin{equation*}
\sup_{x\in D} f(x) \leq \inf_{x\in D} g(x) .
\end{equation*}
%
\item
Find a specific $D$, $f$, and $g$, such that
$f(x) \leq g(x)$ for all $x \in D$, but
\begin{equation*}
\sup_{x\in D} f(x) > \inf_{x\in D} g(x) .
\end{equation*}
\end{enumerate}
\end{exercise}

\begin{exercise}
Prove \propref{prop:funcsupinf} without the assumption that
the functions are bounded.
Hint: You need to use the extended real
numbers.
\end{exercise}

\begin{exercise} \label{exercise:sumofsup}
Let $D$ be a nonempty set.
Suppose $f \colon D \to \R$ and $g \colon D \to \R$ are bounded functions.
\begin{enumerate}[a)]
\item Show 
\begin{equation*}
\sup_{x\in D} \bigl(f(x) + g(x) \bigr) \leq
\sup_{x\in D} f(x)
+
\sup_{x\in D} g(x)
\qquad \text{and} \qquad
\inf_{x\in D} \bigl(f(x) + g(x) \bigr) \geq
\inf_{x\in D} f(x)
+
\inf_{x\in D} g(x) .
\end{equation*}
\item Find examples where we obtain strict inequalities.
\end{enumerate}
\end{exercise}

\subsection*{Recursively defined sequences}

Now that we know we can interchange limits and algebraic operations, we can
compute the limits of many sequences.
One such class are recursively defined sequences, that is, sequences where
the next number in the sequence computed using a formula from a fixed number
of preceding elements in the sequence.

\begin{example}
Let $\{ x_n \}$ be defined by $x_1 := 2$ and
\begin{equation*}
x_{n+1} := x_n - \frac{x_n^2-2}{2x_n} .
\end{equation*}
We must first find out if this sequence is well defined; we must show we never
divide by zero.
Then we must find out if the sequence converges.
Only then
can we attempt to find the limit.

First let us prove 
$x_n$ exists and $x_n > 0$ for all $n$ (so the sequence is well defined
and bounded below).
Let us show this by \hyperref[induction:thm]{induction}.
We know that
$x_1 = 2 > 0$.
For the induction step, suppose $x_n > 0$.
Then
\begin{equation*}
x_{n+1} = x_n - \frac{x_n^2-2}{2x_n} =
\frac{2x_n^2 - x_n^2+2}{2x_n} =
\frac{x_n^2+2}{2x_n} .
\end{equation*}
If $x_n > 0$, then $x_n^2+2 > 0$ and hence $x_{n+1} > 0$.

Next let us
show that the sequence is monotone decreasing.
If we show that
$x_n^2-2 \geq 0$ for all $n$, then $x_{n+1} \leq x_n$ for all $n$.
Obviously $x_1^2-2 = 4-2 = 2 > 0$.
For an arbitrary $n$ we have 
\begin{equation*}
x_{n+1}^2-2 =
{\left( \frac{x_n^2+2}{2x_n} \right)}^2 - 2
=
\frac{x_n^4+4x_n^2+4 - 8x_n^2}{4x_n^2}
=
\frac{x_n^4-4x_n^2+4}{4x_n^2}
=
\frac{{\left( x_n^2-2 \right)}^2}{4x_n^2} .
\end{equation*}
Since any number squared is nonnegative, we have
that $x_{n+1}^2-2 \geq 0$ for all $n$.
Therefore,
$\{ x_n \}$ is monotone decreasing and bounded ($x_n > 0$ for all $n$), and 
the limit exists.
It remains to find the limit.

Let us write
\begin{equation*}
2x_nx_{n+1} = x_n^2+2 .
\end{equation*}
Since $\{ x_{n+1} \}$ is the 1-tail of $\{ x_n \}$, it converges to the
same limit.
Let us define $x := \lim\, x_n$.
We take the limit of
both sides to obtain
\begin{equation*}
2x^2 = x^2+2 ,
\end{equation*}
or $x^2 = 2$.
As $x_n > 0$ for all $n$ we get $x \geq 0$, and therefore $x = \sqrt{2}$.
\end{example}

You may have seen the above sequence before.
It is the
\emph{Newton's method}%
\footnote{%
Named after the English physicist and mathematician
\href{http://en.wikipedia.org/wiki/Isaac_Newton}{Isaac Newton} (1642 --
1726/7).}
for finding the square root of 2.
This method comes up very often in
practice and converges very rapidly.
Notice that we have used the fact that
$x_1^2 -2 >0$, although it was not strictly needed to show convergence by
considering a tail of the sequence.
In fact the sequence converges as long as $x_1 \not= 0$, although with a negative $x_1$
we would arrive at $x=-\sqrt{2}$.
By replacing the 2 in the numerator we 
obtain the square root of any positive number.
These statements are left as
an exercise.

You should, however, be careful.
Before taking any limits, you must
make sure the sequence converges.
Let us see an example.

\begin{example}
Suppose $x_1 := 1$ and $x_{n+1} := x_n^2+x_n$.
If we blindly assumed that the limit exists (call it $x$), then we
would get the equation $x = x^2+x$, from which we might
conclude $x=0$.
However, it is not hard
to show that $\{ x_n \}$ is unbounded and therefore does not converge.

The thing to notice in this example is that the method still works, but
it depends on the initial value $x_1$.
If we set $x_1 := 0$,
then the sequence converges and the limit really is 0.
An entire branch of mathematics, called dynamics, deals precisely with these
issues.
\end{example}

\subsection*{Some convergence tests}

It is not always necessary to go back to the definition of convergence
to prove that a sequence is convergent.
We first give a simple convergence test.
The main idea is that 
$\{ x_n \}$ converges to $x$ if and only if 
$\{ \abs{ x_n - x } \}$ converges to zero.

\begin{prop} \label{convzero:prop}
Let $\{ x_n \}$ be a sequence. 
Suppose there is an $x \in \R$
and a convergent sequence $\{ a_n \}$
such that
\begin{equation*}
\lim_{n\to\infty} a_n = 0
\end{equation*}
and 
\begin{equation*}
\abs{x_n - x} \leq a_n
\end{equation*}
for all $n$.
Then $\{ x_n \}$ converges and $\lim\, x_n = x$.
\end{prop}

\begin{proof}
Let $\epsilon > 0$ be given.
Note that $a_n \geq 0$
for all $n$.
Find an $M \in \N$ such that for
all $n \geq M$ we have
$a_n = \abs{a_n - 0} < \epsilon$.
Then, for all $n \geq M$
we have
\begin{equation*}
\abs{x_n - x} \leq a_n < \epsilon . \qedhere
\end{equation*}
\end{proof}

As the proposition shows, to study when a sequence has a limit is 
the same as studying when another sequence goes to zero.
%However, it may not give us an easily applicable test for convergence.
In general it may be hard to decide if a sequence converges, but
for certain sequences there exist easy to apply tests that tell us
if the sequence converges or not.
Let us see one such test.
%For some special sequences we can test the convergence easily.
First let
us compute the limit of a very specific sequence.
\begin{prop}
Let $c > 0$.
\begin{enumerate}[(i)]
\item
If $c < 1$, then
\begin{equation*}
\lim_{n\to\infty} c^n = 0.
\end{equation*}
\item
If $c > 1$, then $\{ c^n \}$ is unbounded.
\end{enumerate}
\end{prop}

\begin{proof}
First let us suppose $c > 1$.
We write
$c = 1+r$ for some $r > 0$.
By \hyperref[induction:thm]{induction} (or using the binomial theorem
if you know it) we have Bernoulli's inequality (see also
\exerciseref{exercise:bernoulliineq}):
\begin{equation*}
c^n = {(1+r)}^n \geq 1+nr .
\end{equation*}
By the \hyperref[thm:arch:i]{Archimedean property}
of the real numbers, the sequence $\{ 1+nr \}$
is unbounded (for any number $B$, we find an $n \in \N$ such that $nr \geq
B-1$).
Therefore $c^n$ is unbounded.

Now let $c < 1$.
Write $c = \frac{1}{1+r}$, where $r > 0$.
Then
\begin{equation*}
c^n = \frac{1}{{(1+r)}^n} \leq
\frac{1}{1+nr} \leq \frac{1}{r} \frac{1}{n} .
\end{equation*}
As $\{ \frac{1}{n} \}$ converges to zero, so does
$\{ \frac{1}{r} \frac{1}{n} \}$.
Hence, $\{ c^n \}$ converges to zero.
\end{proof}

If we look at the above proposition, we note that the
ratio of the $(n+1)$th term and the $n$th term is $c$.
We 
generalize this simple result to a larger class of sequences.
The following lemma will come up again once we get to series.

\begin{lemma}[Ratio test for sequences]\index{ratio test for sequences}
\label{seq:ratiotest}
Let $\{ x_n \}$ be a sequence such that $x_n \not= 0$ for all $n$ and such that
the limit
\begin{equation*}
L := \lim_{n\to\infty} \frac{\abs{x_{n+1}}}{\abs{x_n}}
\end{equation*}
exists.
\begin{enumerate}[(i)]
\item
If $L < 1$, then $\{ x_n \}$ converges and $\lim\, x_n = 0$.
\item
If $L > 1$, then $\{ x_n \}$ is unbounded (hence diverges).
\end{enumerate}
\end{lemma}

If $L$ exists, but $L=1$, the lemma says nothing.
We cannot make any
conclusion based on that information alone.
For example,
the sequence $\{ \nicefrac{1}{n} \}$ converges to zero, but $L=1$.
The constant sequence $\{ 1 \}$ converges to 1, not zero, and also
$L=1$.
The sequence $\{ {(-1)}^n \}$ does not converge at all, and $L=1$.
Finally the sequence $\{ \ln n \}$ is unbounded, yet again $L=1$.

\begin{proof}
Suppose $L < 1$.
As
$\frac{\abs{x_{n+1}}}{\abs{x_n}} \geq 0$, we have that $L \geq 0$.
Pick $r$ such that $L < r < 1$.
We wish to compare the sequence to the sequence $r^n$.
The idea is that
while the sequence is not going to be less than $L$ eventually,
it will eventually be less than $r$, which is still less than 1.
The intuitive idea of the proof is illustrated in \figureref{figratseq}.
\begin{figure}[h!t]
\begin{center}
\input figratseq.pdf_t
\caption{Proof of ratio test in picture.
The short lines represent the
ratios 
$\frac{\abs{x_{n+1}}}{\abs{x_n}}$
approaching $L$.\label{figratseq}}
\end{center}
\end{figure}

As $r-L > 0$, there exists an $M \in \N$ such that for
all $n \geq M$ we have
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < r-L .
\end{equation*}
Therefore,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} < r .
\end{equation*}
For $n > M$ (that is for $n \geq M+1$)
we write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
<
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
The sequence $\{ r^n \}$ converges to zero and hence 
$\abs{x_M} r^{-M} r^n$ converges to zero.
By \propref{convzero:prop},
the $M$-tail of 
$\{x_n\}$ converges to zero and therefore $\{x_n\}$ converges to zero.

Now suppose $L > 1$.
Pick
$r$ such that $1 < r < L$.
As $L-r > 0$,
there exists an $M \in \N$ such that for
all $n \geq M$ we have
\begin{equation*}
\abs{\frac{\abs{x_{n+1}}}{\abs{x_n}} - L} < L-r .
\end{equation*}
Therefore,
\begin{equation*}
\frac{\abs{x_{n+1}}}{\abs{x_n}} > r .
\end{equation*}
Again for $n > M$
we write
\begin{equation*}
\abs{x_n} =
\abs{x_M}
\frac{\abs{x_{M+1}}}{\abs{x_{M}}}
\frac{\abs{x_{M+2}}}{\abs{x_{M+1}}}
\cdots
\frac{\abs{x_{n}}}{\abs{x_{n-1}}}
>
\abs{x_M}
r r \cdots r = \abs{x_M} r^{n-M} = (\abs{x_M} r^{-M}) r^n .
\end{equation*}
The sequence $\{ r^n \}$ is unbounded (since $r > 1$), and therefore
$\{x_n\}$ cannot be bounded (if $\abs{x_n} \leq B$ for all $n$, then
$r^n < \frac{B}{\abs{x_M}} r^{M}$ for all $n$, which is impossible).
Consequently, $\{ x_n \}$ cannot converge.
\end{proof}

\begin{example}
A simple application of the above lemma is to prove that
\begin{equation*}
\lim_{n\to\infty} \frac{2^n}{n!} = 0 .
\end{equation*}

\begin{proof}
We find that
\begin{equation*}
\frac{2^{n+1} / (n+1)!}{2^n/n!}
=
\frac{2^{n+1}}{2^n}\frac{n!}{(n+1)!}
=
\frac{2}{n+1} .
\end{equation*}
It is not hard to see that $\{ \frac{2}{n+1} \}$ converges to zero.
The conclusion follows by the lemma.
\end{proof}
\end{example}

\subsection*{Exercises}

\begin{exercise}
Prove \corref{limandineq:cor}.
Hint: Use constant sequences
and \lemmaref{limandineq:lemma}.
\end{exercise}

\begin{exercise}
Prove part \ref{prop:contalg:ii} of \propref{prop:contalg}.
\end{exercise}

\begin{exercise}
Prove that if $\{ x_n \}$ is a convergent sequence, $k \in \N$, then
\begin{equation*}
\lim_{n\to\infty} x_n^k = 
{\left( \lim_{n\to\infty} x_n \right)}^k .
\end{equation*}
Hint: Use \hyperref[induction:thm]{induction}.
\end{exercise}

\begin{exercise}
Suppose $x_1 := \frac{1}{2}$ and $x_{n+1} := x_n^2$.
Show that
$\{ x_n \}$ converges and find
$\lim\, x_n$.
Hint: You cannot divide by zero!
\end{exercise}

\begin{exercise}
Let $x_n := \frac{n-\cos(n)}{n}$.
Use the
\hyperref[squeeze:lemma]{squeeze lemma} to show that
$\{ x_n \}$ converges and find the limit.
\end{exercise}

\begin{exercise}
Let $x_n := \frac{1}{n^2}$ and $y_n := \frac{1}{n}$.
Define
$z_n := \frac{x_n}{y_n}$ and 
$w_n := \frac{y_n}{x_n}$.
Do $\{ z_n \}$ and $\{ w_n \}$
converge?
What are the limits?
Can you apply \propref{prop:contalg}?
Why or why not?
\end{exercise}

\begin{exercise}
True or false, prove or find a counterexample.
If $\{ x_n \}$ is a sequence
such that $\{ x_n^2 \}$ converges, then $\{ x_n \}$ converges.
\end{exercise}

\begin{exercise}
Show that
\begin{equation*}
\lim_{n\to\infty} \frac{n^2}{2^n} = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $\{ x_n \}$ is a sequence and suppose for
some $x \in \R$, the limit
\begin{equation*}
L := \lim_{n \to \infty} \frac{\abs{x_{n+1}-x}}{\abs{x_n-x}}
\end{equation*}
exists and $L < 1$.
Show that $\{ x_n \}$ converges to $x$.
\end{exercise}

\begin{exercise}[Challenging]
Let $\{ x_n \}$ be a convergent sequence such
that $x_n \geq 0$ and $k \in \N$.
Then
\begin{equation*}
\lim_{n\to\infty} x_n^{1/k} =
{\left( \lim_{n\to\infty} x_n \right)}^{1/k} .
\end{equation*}
Hint: Find an expression $q$ such that $\frac{x_n^{1/k}-x^{1/k}}{x_n-x} =
\frac{1}{q}$.
\end{exercise}

\begin{exercise}
Let $r > 0$.
Show that starting with any $x_1 \not= 0$, the sequence
defined by
\begin{equation*}
x_{n+1} := x_n - \frac{x_n^2-r}{2x_n}
\end{equation*}
converges to $\sqrt{r}$ if $x_1 > 0$ and $-\sqrt{r}$ if $x_1 < 0$.
\end{exercise}

\begin{exercise}
\begin{enumerate}[a)]
 \item Suppose $\{ a_n \}$ is a bounded sequence and $\{ b_n \}$ is a sequence
converging to 0. Show that $\{ a_n b_n \}$ converges to 0.
 \item Find an example where $\{ a_n \}$ is unbounded, $\{ b_n \}$ converges to
0, and $\{ a_n b_n \}$ is not convergent.
  \item Find an example where $\{ a_n \}$ is bounded, $\{ b_n \}$ converges to
some $x \not= 0$, and $\{ a_n b_n \}$ is not convergent.
\end{enumerate}
\end{exercise}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage  
\phantomsection
\addcontentsline{toc}{chapter}{Further Reading}
\markboth{FURTHER READING}{FURTHER READING}
\begin{bibchapter}[Further Reading]
\begin{biblist}[\normalsize]

\bib{BS}{book}{
   author={Bartle, Robert G.},
   author={Sherbert, Donald R.},
   title={Introduction to real analysis},
   edition={3},
   publisher={John Wiley \& Sons Inc.},
   place={New York},
   date={2000},
}

\bib{DW}{book}{
   author={D'Angelo, John P.},
   author={West, Douglas B.},
   title={Mathematical Thinking: Problem-Solving and Proofs},
   edition={2},
   publisher={Prentice Hall},
   date={1999},
}

\bib{GIAM}{misc}{
   author={Fields, Joseph E.},
   title={A Gentle Introduction to the Art of Mathematics},
   note={Available at \url{http://ares.southernct.edu/~fields/GIAM/}},
}

\bib{Hammack}{misc}{
   author={Hammack, Richard},
   title={Book of Proof},
   note={Available at \url{http://www.people.vcu.edu/~rhammack/BookOfProof/}},
}

\bib{Rosenlicht}{book}{
   author={Rosenlicht, Maxwell},
   title={Introduction to analysis},
   note={Reprint of the 1968 edition},
   publisher={Dover Publications Inc.},
   place={New York},
   date={1986},
   pages={viii+254},
   isbn={0-486-65038-3},
   %review={\MR{851984 (87g:26001)}},
}

\bib{Rudin:baby}{book}{
   author={Rudin, Walter},
   title={Principles of mathematical analysis},
   edition={3},
   note={International Series in Pure and Applied Mathematics},
   publisher={McGraw-Hill Book Co.},
   place={New York},
   date={1976},
   pages={x+342},
   %review={\MR{0385023 (52 \#5893)}},
}

\bib{Trench}{book}{
   author={Trench, William F.},
   title={Introduction to real analysis},
   year={2003},
   publisher={Pearson Education},
   note={\url{http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF}},
}



\end{biblist}
\end{bibchapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cleardoublepage  
\phantomsection
\addcontentsline{toc}{chapter}{\indexname}  
\printindex

\end{document}
